CHUNKS FROM PDF FILES
============================================================
Total chunks: 560

CHUNK 1
File: unknown
Size: 89 chars
Content:
----------------------------------------
INFO 5731 Computational Methods for Information Systems Section: 020 SYLLABUS Spring 2025
----------------------------------------

CHUNK 2
File: unknown
Size: 562 chars
Content:
----------------------------------------
Table of Contents COURSE INFORMATION 1 Instructor Contact Information 1 Teaching Assistant 1 Communicating with Your Instructor 1 Course Pre-requisites, Co-requisites, and/or Other Restrictions 2 Course Format 2 Course Description 2 Course Goals, Learning Objectives 2 Materials 2 Teaching Philosophy 3 TECHNICAL REQUIREMENTS/ASSISTANCE 3 Minimum Technical Skills Needed 4 Student Academic Support Services 4 ASSESSMENT & GRADING 4 Assessments 4 Grading 4 Grading Table 6 COURSE CALENDAR 6 Table 1. Lessons and Readings 7 Study Schedule and Due Dates 7 Table 2. 
----------------------------------------

CHUNK 3
File: unknown
Size: 329 chars
Content:
----------------------------------------
Study Schedule and Due Dates 7 COURSE EVALUATION 8 Student Evaluation Administration Dates 8 COURSE POLICIES 9 Assignment Policy 9 Examination Policy 9 Instructor Responsibilities and Feedback 9 Late Work and Missed Work 9 Course Incomplete Grade 9 Withdrawal 9 Attendance Policy 10 Students’ Responsibility for Their Learning 10
----------------------------------------

CHUNK 4
File: unknown
Size: 410 chars
Content:
----------------------------------------
UNT POLICIES 10 Academic Integrity Policy 10 ADA Policy 10 Emergency Notification & Procedures 10 Retention of Student Records 10 Acceptable Student Behavior 11 Access to Information - Eagle Connect 11 Sexual Assault Prevention 11 Important Notice for F-1 Students taking Distance Education Courses 11 Federal Regulation 11 University of North Texas Compliance 12 Student Verification 12 Use of Student Work 12
----------------------------------------

CHUNK 5
File: unknown
Size: 1915 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 COURSE INFORMATION • INFO 5731, Sections 020, 3 Credit Hours • Title: Computational Methods for Information Systems • Meeting Dates (Face-to-face): See Table 2 • Meeting Time: Wednesday 5:30PM - 8:20PM • Room: NTDP B185 Instructor Contact Information • Haihua Chen, Assistant Professor in Data Science, Anuradha and Vikas Sinha Department of Data Science, University of North Texas. • Office: DP E298A (By appointment) • Zoom Meeting ID: 247 728 2245 (By appointment) • Phone: (940) 268-8589 • Email address: haihua.chen@unt.edu Teaching Assistant • Fengjiao Tu, PhD student in Information Science, Department of Information Science, College of Information, Univeristy of North Texas • Office and office hour: Tuesday 1-5PM, E292L, other time by appointment • Zoom meeting ID: 884 2281 7391 (By appointment) • Email address: fengjiaotu@my.unt.edu • Huyen Thi Ngoc Nguyen, PhD candidate in Information Science, Teaching Fellow, Department of Information Science, College of Information, Univeristy of North Texas • Office and office hour: Wednesday 1-3PM, E292J, other time by appointment • Zoom meeting ID: 889 281 7606 (By appointment) • Email address: huyennguyen5@my.unt.edu • Fardeen Ali Mohammed, Master student in Information Science, Department of Information Science, College of Information, Univeristy of North Texas • Office and office hour: Wednesday 1:30pm to 3:30pm, E292L, other time by appointment • Zoom meeting ID: TBA • Email address: fardeenalimohammed@my.unt.edu Communicating with Your Instructor This course will have a website in UNT Canvas (https://unt.instructure.com/login/canvas) for online discussion, assignment submissions, and sharing of reading materials. Students are welcome to make an appointment with the instructor and/or the teaching assistant (TA) to discuss course-related questions (in person or online). 
----------------------------------------

CHUNK 6
File: unknown
Size: 85 chars
Content:
----------------------------------------
If you need to schedule an individual online meeting with the INFO 5731 1 Spring 2025
----------------------------------------

CHUNK 7
File: unknown
Size: 1732 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 instructor or the TA, please send her/him an email via the course website in Canvas Course Messages to make an appointment. Course Pre-requisites, Co-requisites, and/or Other Restrictions • Pre-requisite: Basic programming knowledge and experience (Python), or consent of instructor Course Format INFO 5731, Sections 020 hold face-to-face lectures by the instructor. The course uses Canvas, UNT's new learning management system. All course materials will be available at the course site on Canvas that is accessible to all students. And students will submit all assignments through the tools available on Canvas. Course Description Introduces computational methods that facilitate information analysis, management, and presentation in information systems. Students learn effective computer programming skills and analytical tools to process real-world data. Problem-oriented and project-based, allows students to explore interesting research ideas or implement useful information management applications. Course Goals, Learning Objectives • Master key concepts and components of NLP and linguistics. • Manipulate large corpora, explore linguistic models, and test empirical claims. • Design and implement applications that process, manage, and analyze text data. • Clean and preprocess raw text data using basic natural language processing techniques. • Demonstrate the ability to extract and analyze information from text data using Python Program. • Build robust systems to perform linguistic tasks with technological applications. • Document and report on information processing and applications. Materials Textbook information (required): 1. Downey, Allen B. 
----------------------------------------

CHUNK 8
File: unknown
Size: 223 chars
Content:
----------------------------------------
(2016). Think Python: How to Think Like a Computer Scientist, 2nd Edition. O’Reilly, ISBN-13: 978-1-491-93936-9. Free access link: https://greenteapress.com/thinkpython/thinkpython.html 2. Hapke, H., Howard, C., & Lane, H. 
----------------------------------------

CHUNK 9
File: unknown
Size: 695 chars
Content:
----------------------------------------
(2021). Natural Language Processing in Action: Understanding, analyzing, and generating text with Python (2nd Edition). Simon and Schuster. Link: https://www.manning.com/books/natural-language-processing-in-action-second-edition Free access link: https://www.nltk.org/book/ Exercises in the book: https://github.com/STRZGR/Natural-Language-Processing-with-Python- Analyzing-Text-with-the-Natural-Language-Toolkit?tab=readme-ov-file 3. Tunstall, L., Von Werra, L., & Wolf, T. (2022). Natural language processing with transformers (Revised Edition). " O'Reilly Media, Inc.". Link: https://transformersbook.com Free access link: https://books.google.ch/books?id=7hhyzgEACAAJ INFO 5731 2 Spring 2025
----------------------------------------

CHUNK 10
File: unknown
Size: 302 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 Code example of the book: https://github.com/nlp-with-transformers/notebooks Supplementary materials and/or readings (recommended): 4. Python Documentation: https://www.python.org/doc/. 5. Python Forums: https://python-forum.io/. 6. 
----------------------------------------

CHUNK 11
File: unknown
Size: 43 chars
Content:
----------------------------------------
Stackoverflow: https://stackoverflow.com/. 
----------------------------------------

CHUNK 12
File: unknown
Size: 1723 chars
Content:
----------------------------------------
7. NLTK Documentation: https://www.nltk.org/. 8. Google Colab: http://colab.research.google.com/. 9. Success Story of Sylvain Gugger: https://www.fast.ai/2019/01/02/one-year-of-deep-learning/. 10. Github link of the first textbook: https://github.com/AllenDowney/ThinkPython 11. Github link of the second textbook: https://github.com/totalgood/nlpia 12. Github link of the third textbook: https://github.com/nlp-with-transformers/notebooks 13. Jacob Eisenstein. (2019). Introduction to Natural Language Processing (Adaptive Computation and Machine Learning series). The MIT Press, ISBN-13: 978-0262042840. Teaching Philosophy The instructor will take a problem-solving approach and work together with students to understand Natural Language Processing. We will learn how to solve practical data collecting, text processing, information extraction, and text mining problems. He will monitor the progress of students and is open to suggestions from students. Students are expected to study 12-15 hours per week, and to submit their assignments on time to achieve satisfactory class performance. Interaction between the student and the instructor/TA is guaranteed and strongly encouraged. Students who don’t have knowledge and experience in python are expected to spend extra hours on this course. TECHNICAL REQUIREMENTS/ASSISTANCE UIT Help Desk: https://www.python.org/doc/.0 The University of North Texas provides student technical support in the use of Canvas and supported resources. The student help desk may be reached at: Email: helpdesk@unt.edu Phone: 940.565-2324 In-Person: Sage Hall, Room 330 Hours are: • Monday-Thursday 8am-midnight • Friday 8am-8pm • Saturday 9am-5p • Sunday 8am-midnight INFO 5731 3 Spring 2025
----------------------------------------

CHUNK 13
File: unknown
Size: 2428 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 • Canvas technical requirements: https://clear.unt.edu/supported- technologies/canvas/requirements • Other related hardware or software necessary for the course: such as headset/microphone for synchronous chats, word processor, etc. Minimum Technical Skills Needed Using the Internet and the learning management system Canvas, using email with attachments, creating and submitting files in commonly used word processing program formats, downloading and installing software, using python programs. Student Academic Support Services • Code of Student Conduct: provides Code of Student Conduct along with other useful links • Office of Disability Access: exists to prevent discrimination based on disability and to help students reach a higher level of independence • Counseling and Testing Services: provides counseling services to the UNT community, as well as testing services; such as admissions testing, computer-based testing, career testing, and other tests • UNT Libraries • UNT Learning Center: provides a variety of services, including tutoring, to enhance the student academic experience • UNT Writing Center: offers free writing tutoring to all UNT students, undergraduate, and graduate, including online tutoring • Succeed at UNT: information regarding how to be a successful student at UNT ASSESSMENT & GRADING Assessments Class Attendance and Participation will not be directly graded. However, students who have 3 absences (including excused and unexcused) will receive an F directly. A student’s grade is composed of the following: • Assignments (50%) • Term Project (40%) • Discussions (10%) • Extra Credits (10%) Grading Class Attendance and Participation. Class Attendance and Participation will not be directly graded. However, students who have 3 absences (including excused and unexcused) will receive an F directly. Being late once will be counted as 0.5 absence. Not being present during roll call will be considered as an absence. Arriving within 10 minutes after roll call will be considered late. Beyond 10 minutes, it will be considered as an absence. Students who are late should inform the instructor after class to ensure they are marked as late instead of absent. This semester, you must meet your project instructor at least four times. These meetings will also be counted towards your class attendance and participation grade. 
----------------------------------------

CHUNK 14
File: unknown
Size: 23 chars
Content:
----------------------------------------
INFO 5731 4 Spring 2025
----------------------------------------

CHUNK 15
File: unknown
Size: 2195 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 Assignments (50%). The class will have FIVE assignments. These assignments are designed to help students understand important concepts and gain hands-on experience in Python programming, data processing, and problem-solving. Assignments must be prepared and submitted using Overleaf in LaTeX format. Diagrams should be created using appropriate graphics software (e.g., PowerPoint, Excel, or similar tools). Code should be written on Google Colab, and students should submit a link to the Colab notebook with their assignment. Term Project (40%). The term project is structured according to a project-based learning framework. Throughout the semester, students will submit four reports and deliver a final presentation. The first three reports each account for 5% of the total grade (15% in total). The final project submission consists of a final version of the report (5%) and a presentation (10%), making the term project worth 40% of the total course grade. • First Report (5%): The first report should introduce the project by providing: Background and significance of the chosen topic, a preliminary literature review, and an initial research design, specifying the types of data to be collected and the methodology to be used. • Second Report (5%): The second report should detail the progress of data collection, including: Data sources and collection methods and challenges encountered and how they were addressed. • Third Report (5%): The third report should describe the selected models and evaluation metrics, including: the rationale for selecting these models and metrics, an explanation of the implementation process, and any preliminary results obtained during the project. • Final Submission (15%): The final report (5%) should be a comprehensive document summarizing the entire project, covering all aspects from the background and methodology to results, analysis, and conclusions. The final presentation (10%) should concisely and effectively communicate the key components of the project, including the problem definition, methodology, results, key insights, and potential future work. Discussions (10%). 
----------------------------------------

CHUNK 16
File: unknown
Size: 959 chars
Content:
----------------------------------------
Each week we will post a discussion question for the week in the discussion area. Each question is worth 1% of the grade. Please preview the class readings and prepare the discussion questions. Extra Credits (100 points). Extra credits are divided into three parts: Paper reading notes (50 points), in- class presentation (10 points), course evaluation (10 points), and attending research presentations (30 points). • Paper reading notes (30 points): Students can submit up to 10 reading notes, each worth 10 points. Only the first 5 submissions will be counted toward the total 50 points. Reading notes should be one page long and submitted before each lecture. A list of related papers will be provided for each lecture, and students should select papers from this list. • Peer review (20 points): Peer reviews are conducted as a team. Each team is required to provide at least N suggestions for another team's report, where N ≥ the number of team members. 
----------------------------------------

CHUNK 17
File: unknown
Size: 395 chars
Content:
----------------------------------------
The team receiving feedback must respond to each suggestion individually, specifying whether they acceping or declining the suggestion and a clear and reasonable explanation must be provided. • In-Class Presentation (10 points): Each student is required to give at least one in-class presentation on a selected paper during the semester. The presentation schedule will be INFO 5731 5 Spring 2025
----------------------------------------

CHUNK 18
File: unknown
Size: 1716 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 determined during the first class. Completing both the reading notes and the in-class presentation can earn students a total of 60 points. • Course Evaluation (10 points): At the end of the semester, students will receive a link to complete the course evaluation. Upon submitting a screenshot showing the completion of the evaluation, students will receive 10 extra points. • Attending Research Presentations (30 points): Throughout the semester, the instructor will announce relevant research presentations (e.g., online research talks). Students can attend up to 3 such presentations. Each attended presentation, with valid proof of attendance, will earn 10 points, up to a maximum of 30 points. Total Points Possible for Semester/Grading Scale = 1100 1100-900 = A 899-800 = B 799-700 = C 699-600 = D 599 and below = F Grading Table Assignment Points Possible Percentage of Final Grade Assignment Assignment 1 – 100 points 10% Assignment 2 – 100 points 10% Assignment 3 – 100 points 10% Assignment 4 – 100 points 10% Assignment 5 – 100 points 10% Term First Report 50 points 5% Project Second Report 50 points 5% Third Report 50 points 5% Final report 50 pointsb 5% Final presentation 100 points 10% Discussion Each discussion @ 10 points 100 points 10% Extra Readings and presentation @ 50 points 50 points 5% credits In-class presentation @ 10 points 100 points 10% Course evaluation @ 10 points 100 points 10% Attend research meetings @ 30 points 300 points 10% Total Points Possible 1100 points 110% COURSE CALENDAR The contents of the course are organized into 17 weeks. Please refer to Table 1 for lessons, topics, and readings materials. 
----------------------------------------

CHUNK 19
File: unknown
Size: 114 chars
Content:
----------------------------------------
Table 2 lists the suggested study schedule, assignments, quiz, and term project due dates. INFO 5731 6 Spring 2025
----------------------------------------

CHUNK 20
File: unknown
Size: 1344 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 Table 1. Lessons and Readings Lessons Topics Readings Lesson 1 Introduction to Python and NLP, Google Colab, GitHub. Downey: Chapter 1 Course Orientation and Overview Hobson: Chapter 1 Core Concepts Related to NLP Lesson 2 Python Basic (1): Integers, Floats, Booleans, Strings, Lists, List Downey: Chapter 2-3, 8, 10- Operations, Tuples, Dictionaries, Sets, List Comprehensions, Files, 14 Functions, I/O Lesson 3 Python Basics (2): Python Modules, Packages, Functions, Downey: Chapter 4-7, 9, 15- Conditionals, for Loops, Recursion, Selections, Exceptions, Classes 18 and Objects, Regular Expression Lesson 4 Accessing Text Copra and Lexical resources PPT Lesson 5 Raw Text Preprocessing and Cleaning: Removing Stop Words, Hobson: Chapter 2, 3 Stemming, Segmentation, and POS-Tagging Lesson 6 Analyzing Sentence Structure PPT Lesson 7 Extracting Information from Text Hobson: Chapter 6-10, PPT Lesson 8 Semantic Analysis of Sentences Hobson: Chapter 4 Lesson 9 Sentiment Analysis of Text PPT Lesson 10 Text Classification and Clustering PPT Lesson 11 Generative AI-Powered NLP applications (Optional) PPT Study Schedule and Due Dates Lectures 1, 2, 4, 6, 8, 10, and 11 will be delivered by Dr. Haihua Chen, Lectures 3, 5, 7, and 9 will be delivered by Huyen Thi Ngoc Nguyen. 
----------------------------------------

CHUNK 21
File: unknown
Size: 840 chars
Content:
----------------------------------------
(Assignments and the Project first submission will due on Sunday midnight of the specified week. Quizzes will be available online from 6:00 pm on Monday to 6:00 pm on Friday of the specified week. Term project final report will due on December 6 midnight). The time of the invited talk might be changed based on the speakers’ schedule. Table 2. Study Schedule and Due Dates Week Dates Meeting Study Focus Individual tasks Group tasks Date 1 Syllabus, Lesson 1 Discussion Student Grouping Jan 13 - Jan 19 Jan 15 Dr. Chen 2 Lesson 2 Assignment 1 Jan 20 - Jan 26 Jan 22 Dr. Chen Discussion 3 Lesson 3 Discussion Jan 27 - Feb 02 Jan 29 Huyen 4 Lesson 4 Discussion Feb 03 - Feb 09 Feb 05 Dr. Chen 5 Lesson 5 Assignment 2 Feb 10 - Feb 16 Feb 12 Huyen Discussion 6 Lesson 6-part 1 Discussion Feb 17 - Feb 23 Feb 19 Dr. Chen INFO 5731 7 Spring 2025
----------------------------------------

CHUNK 22
File: unknown
Size: 1990 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 7 Lesson 6-part2 Assignment 3 Choosing research Feb 24 - Mar 02 Feb 26 Dr. Chen topic 8 Work on First Report (Term Project Proposal). Meet with each group Mar 03 - Mar 09 Mar 05 separately to discuss the term project. 9 Mar 10 - Mar 16 No class No class 10 Project Topics Presentation Mar 17 - Mar 23 Mar 19 Discussion Peer Review 11 Lesson 7 Discussion Second Report (Meet Mar 24 - Mar 30 Mar 26 Huyen your Instructor) 12 Lesson 8 Assignment 4 Peer Review Mar 31 - Apr 06 Apr 02 Dr. Chen 13 Lesson 9 Discussion Apr 07 - Apr 13 Apr 09 Huyen Lesson 10 Assignment 5 Third Report (Meet Apr 14 - Apr 20 Apr 16 14 Dr. Chen your Instructor) 15 Lesson 11 (or Invited Talk Discussion Peer Review Apr 21 - Apr 27 Apr 23 from Industry) All the extra credit Dr. Chen submissions due Slides of the Project Presentation Due Apr 29 Midnight Class Summary Term Project Final 16 Apr 28 - May 04 Apr 30 Term Project Report Due at Presentation May 02 Midnight (Before your Presentation Meet your Instructor) Peer Review 17 Instructor will work on May 05 - May 11 May 07 the grading COURSE EVALUATION Student Evaluation Administration Dates Student feedback is important and an essential part of participation in this course. The student evaluation of instruction is a requirement for all organized classes at UNT. The survey will be made available during weeks 13, 14 and 15 of the long semesters to provide students with an opportunity to evaluate how this course is taught. Students will receive an email from "UNT SPOT Course Evaluations via IASystem Notification" (no-reply@iasystem.org) with the survey link. Students should look for the email in their UNT email inbox. Simply click on the link and complete the survey. Once students complete the survey they will receive a confirmation email that the survey has been submitted. For additional information, please visit the SPOT website at http://spot.unt.edu/ or email spot@unt.edu. 
----------------------------------------

CHUNK 23
File: unknown
Size: 23 chars
Content:
----------------------------------------
INFO 5731 8 Spring 2025
----------------------------------------

CHUNK 24
File: unknown
Size: 1063 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 COURSE POLICIES Assignment Policy Students should submit the assignments and term project reports via Dropbox at class site in canvas.unt.edu: PDF files with the code link on GitHub included in the file, also with the code uploaded on GitHub, details will be included in each assignment. Examination Policy There are no exams for this course. Instructor Responsibilities and Feedback • Helping students grow and learn • Providing clear instructions for projects and assessments • Answering questions about assignments • Identifying additional resources as necessary • Providing grading rubrics • Reviewing and updating course content • The instructor and TA will respond to students’ emails and questions posted to the discussion boards within two days except for the weekends • Assignments grades and feedback will be returned to the students within one week after the submission deadline. Late Work and Missed Work Students are expected to submit discussion assignments and projects on time. 
----------------------------------------

CHUNK 25
File: unknown
Size: 1487 chars
Content:
----------------------------------------
The due dates are Monday 11:59 pm of the week specified in Table 2. Study Schedule and Due Dates. If an extenuating circumstance such as a medically diagnosed illness or a family emergency arises, which prevents you from submitting your assignments, you should contact the instructor and the TA as soon as possible before the due date. Late work without the permission of the instructor will receive a grade with a 10% penalty (or 10 points out of 100) per day after the due date. A student who is having trouble with assignments is strongly encouraged to contact the instructor and the TA as early as possible for personal advising. Course Incomplete Grade The UNT Graduate Catalog (http://catalog.unt.edu/index.php?catoid=16) describes and explains grading policies. A grade of Incomplete (I) will be given only for a justifiable reason and only if the student is passing the course. The student is responsible for meeting with the instructor to request an incomplete and discuss requirements for completing the course. If an incomplete is not removed within the time frame agreed to by the instructor and student, the instructor may assign a grade of F. Withdrawal The UNT Graduate Catalog (http://catalog.unt.edu/index.php?catoid=16) describes and explains withdrawal policies and deadlines. The UNT semester course schedule lists specific deadlines regarding withdrawal. A grade of Withdraw (W) or Withdraw-Failing (WF) will be given depending on a student's INFO 5731 9 Spring 2025
----------------------------------------

CHUNK 26
File: unknown
Size: 2568 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 attendance record and grade earned. Please note that a student who simply stops attending class and does not file a withdrawal form may receive an F. Attendance Policy Attending the class meeting is required, students who miss more than 3 class meetings will receive an F directly. Prior to the meeting, please preview the readings for the class and prepare your questions for discussion. You will miss in-class exercises, or quizzes if you do not attend the class. Students’ Responsibility for Their Learning The students are required to follow course schedule and finish the classwork, assignments, quizzes, and term projects. Students are expected to study 12-15 hours per week to achieve satisfactory class performance. Students do not have programming experience are required to find extra materials to study. UNT POLICIES Academic Integrity Policy Academic Integrity Standards and Consequences. According to UNT Policy 06.003, Student Academic Integrity, academic dishonesty occurs when students engage in behaviors including, but not limited to cheating, fabrication, facilitating academic dishonesty, forgery, plagiarism, and sabotage. A finding of academic dishonesty may result in a range of academic penalties or sanctions ranging from admonition to expulsion from the University. ADA Policy UNT makes reasonable academic accommodation for students with disabilities. Students seeking accommodation must first register with the Office of Disability Accommodation (ODA) to verify their eligibility. If a disability is verified, the ODA will provide a student with an accommodation letter to be delivered to faculty to begin a private discussion regarding one’s specific course needs. Students may request accommodations at any time; however, ODA notices of accommodation should be provided as early as possible in the semester to avoid any delay in implementation. Note that students must obtain a new letter of accommodation for every semester and must meet with each faculty member prior to implementation in each class. For additional information see the ODA website at disability.unt.edu. Emergency Notification & Procedures UNT uses a system called Eagle Alert to quickly notify students with critical information in the event of an emergency (i.e., severe weather, campus closing, and health and public safety emergencies like chemical spills, fires, or violence). In the event of a university closure, please refer to Blackboard for contingency plans for covering course materials. 
----------------------------------------

CHUNK 27
File: unknown
Size: 571 chars
Content:
----------------------------------------
Retention of Student Records Student records pertaining to this course are maintained in a secure location by the instructor of record. All records such as exams, answer sheets (with keys), and written papers submitted during the duration of the course are kept for at least one calendar year after course completion. Course work completed via the Blackboard online system, including grading information and comments, is also stored in a safe electronic environment for one year. Students have the right to view their individual records; however, INFO 5731 10 Spring 2025
----------------------------------------

CHUNK 28
File: unknown
Size: 2979 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 information about student’s records will not be divulged to other individuals without proper written consent. Students are encouraged to review the Public Information Policy and the Family Educational Rights and Privacy Act (FERPA) laws and the University’s policy. See UNT Policy 10.10, Records Management and Retention for additional information. Acceptable Student Behavior Student behavior that interferes with an instructor’s ability to conduct a class or other students' opportunity to learn is unacceptable and disruptive and will not be tolerated in any instructional forum at UNT. Students engaging in unacceptable behavior will be directed to leave the classroom and the instructor may refer the student to the Dean of Students to consider whether the student's conduct violated the Code of Student Conduct. The University's expectations for student conduct apply to all instructional forums, including University and electronic classroom, labs, discussion groups, field trips, etc. The Code of Student Conduct can be found at deanofstudents.unt.edu/conduct. Access to Information - Eagle Connect Students’ access point for business and academic services at UNT is located at: my.unt.edu. All official communication from the University will be delivered to a student’s Eagle Connect account. For more information, please visit the website that explains Eagle Connect and how to forward e-mail: eagleconnect.unt.edu/. Sexual Assault Prevention UNT is committed to providing a safe learning environment free of all forms of sexual misconduct, including sexual harassment sexual assault, domestic violence, dating violence, and stalking. Federal laws (Title IX and the Violence Against Women Act) and UNT policies prohibit discrimination on the basis of sex, and therefore prohibit sexual misconduct. If you or someone you know is experiencing sexual harassment, relationship violence, stalking, and/or sexual assault, there are campus resources available to provide support and assistance. UNT’s Survivor Advocates can assist a student who has been impacted by violence by filing protective orders, completing crime victim’s compensation applications, contacting professors for absences related to an assault, working with housing to facilitate a room change where appropriate, and connecting students to other resources available both on and off campus. The Survivor Advocates can be reached at SurvivorAdvocate@unt.edu or by calling the Dean of Students Office at 940-565- 2648. Additionally, alleged sexual misconduct can be non-confidentially reported to the Title IX Coordinator at oeo@unt.edu or at (940) 565 2759. Important Notice for F-1 Students taking Distance Education Courses Federal Regulation To read detailed Immigration and Customs Enforcement regulations for F-1 students taking online courses, please go to the Electronic Code of Federal Regulations website at http://www.ecfr.gov/. 
----------------------------------------

CHUNK 29
File: unknown
Size: 570 chars
Content:
----------------------------------------
The specific portion concerning distance education courses is located at Title 8 CFR 214.2 Paragraph (f)(6)(i)(G). The paragraph reads: (G) For F-1 students enrolled in classes for credit or classroom hours, no more than the equivalent of one class or three credits per session, term, semester, trimester, or quarter may be counted toward the full course of study requirement if the class is taken on-line or through distance education and does not require the student's physical attendance for classes, examination or other purposes integral to INFO 5731 11 Spring 2025
----------------------------------------

CHUNK 30
File: unknown
Size: 2696 chars
Content:
----------------------------------------
INFO 5731— Computational Methods for Information Systems Spring 2025 completion of the class. An on-line or distance education course is a course that is offered principally through the use of television, audio, or computer transmission including open broadcast, closed circuit, cable, microwave, or satellite, audio conferencing, or computer conferencing. If the F-1 student's course of study is in a language study program, no on-line or distance education classes may be considered to count toward a student's full course of study requirement. University of North Texas Compliance To comply with immigration regulations, an F-1 visa holder within the United States may need to engage in an on-campus experiential component for this course. This component (which must be approved in advance by the instructor) can include activities such as taking an on-campus exam, participating in an on-campus lecture or lab activity, or other on-campus experience integral to the completion of this course. If such an on-campus activity is required, it is the student’s responsibility to do the following: (1) Submit a written request to the instructor for an on-campus experiential component within one week of the start of the course. (2) Ensure that the activity on campus takes place and the instructor documents it in writing with a notice sent to the International Student and Scholar Services Office. ISSS has a form available that you may use for this purpose. Because the decision may have serious immigration consequences, if an F-1 student is unsure about his or her need to participate in an on-campus experiential component for this course, s/he should contact the UNT International Student and Scholar Services Office (telephone 940-565-2195 or email internationaladvising@unt.edu) to get clarification before the one-week deadline. Student Verification UNT takes measures to protect the integrity of educational credentials awarded to students enrolled in distance education courses by verifying student identity, protecting student privacy, and notifying students of any special meeting times/locations or additional charges associated with student identity verification in distance education courses. See UNT Policy 07-002 Student Identity Verification, Privacy, and Notification and Distance Education Courses. Use of Student Work A student owns the copyright for all work (e.g., software, photographs, reports, presentations, and email postings) he or she creates within a class and the University is not entitled to use any student work without the student’s permission unless all of the following criteria are met: • The work is used only once. • The work is not used in its entirety. 
----------------------------------------

CHUNK 31
File: unknown
Size: 400 chars
Content:
----------------------------------------
• The use of the work does not affect any potential profits from the work. • The student is not identified. • The work is identified as student work. If the use of the work does not meet all of the above criteria, then the University office or department using the work must obtain the student’s written permission. Download the UNT System Permission, Waiver and Release Form INFO 5731 12 Spring 2025
----------------------------------------

CHUNK 32
File: unknown
Size: 58 chars
Content:
----------------------------------------
CS224C: NLP for CSS Topic Modeling Diyi Yang Stanford CS 1
----------------------------------------

CHUNK 33
File: unknown
Size: 169 chars
Content:
----------------------------------------
Overview What is topic modeling? LDA topic modeling Evaluation methods LDA variants SeededLDA Structural Topic Model LLM based topic modeling BERTopic, TopicGPT, LLooM 2
----------------------------------------

CHUNK 34
File: unknown
Size: 210 chars
Content:
----------------------------------------
Topic Modeling Organize the documents into a set of coherent topics Find relationships between these topics Understand how different documents talk about the same topic Track the evolution of topics over time 3
----------------------------------------

CHUNK 35
File: unknown
Size: 266 chars
Content:
----------------------------------------
Topic Modeling A method of (unsupervised) discovery of latent or hidden structure in a corpus ✦ Applied primarily to text corpora ✦ Provides a modeling toolbox ✦ Has prompted the exploration of a variety of new inference methods to accommodate large-scale datasets 4
----------------------------------------

CHUNK 36
File: unknown
Size: 1 chars
Content:
----------------------------------------
5
----------------------------------------

CHUNK 37
File: unknown
Size: 197 chars
Content:
----------------------------------------
Latent Dirichlet Allocation Generative Process Blei, David M., Andrew Y. Ng, and Michael I. Jordan. "Latent dirichlet allocation." Journal of machine Learning research 3, no. Jan (2003): 993-1022. 
----------------------------------------

CHUNK 38
File: unknown
Size: 1 chars
Content:
----------------------------------------
6
----------------------------------------

CHUNK 39
File: unknown
Size: 29 chars
Content:
----------------------------------------
Latent Dirichlet Allocation 7
----------------------------------------

CHUNK 40
File: unknown
Size: 204 chars
Content:
----------------------------------------
The generative story begins with only a Dirichlet prior over the topics Each topic is defined as a Multinomial distribution over the vocabulary, parameterized by φ k Example Credit to Matthew R. Gormley 8
----------------------------------------

CHUNK 41
File: unknown
Size: 104 chars
Content:
----------------------------------------
A topic is visualized as its high probability words. A pedagogical label is used to identify the topic. 
----------------------------------------

CHUNK 42
File: unknown
Size: 38 chars
Content:
----------------------------------------
Example Credit to Matthew R. Gormley 9
----------------------------------------

CHUNK 43
File: unknown
Size: 104 chars
Content:
----------------------------------------
A topic is visualized as its high probability words. A pedagogical label is used to identify the topic. 
----------------------------------------

CHUNK 44
File: unknown
Size: 39 chars
Content:
----------------------------------------
Example Credit to Matthew R. Gormley 10
----------------------------------------

CHUNK 45
File: unknown
Size: 39 chars
Content:
----------------------------------------
Example Credit to Matthew R. Gormley 11
----------------------------------------

CHUNK 46
File: unknown
Size: 39 chars
Content:
----------------------------------------
Example Credit to Matthew R. Gormley 12
----------------------------------------

CHUNK 47
File: unknown
Size: 39 chars
Content:
----------------------------------------
Example Credit to Matthew R. Gormley 13
----------------------------------------

CHUNK 48
File: unknown
Size: 39 chars
Content:
----------------------------------------
Example Credit to Matthew R. Gormley 14
----------------------------------------

CHUNK 49
File: unknown
Size: 39 chars
Content:
----------------------------------------
Example Credit to Matthew R. Gormley 15
----------------------------------------

CHUNK 50
File: unknown
Size: 39 chars
Content:
----------------------------------------
Example Credit to Matthew R. Gormley 16
----------------------------------------

CHUNK 51
File: unknown
Size: 67 chars
Content:
----------------------------------------
Distribution over words (topics) Distribution over topics (docs) 17
----------------------------------------

CHUNK 52
File: unknown
Size: 73 chars
Content:
----------------------------------------
Overview What is topic modeling? LDA topic modeling Evaluation methods 18
----------------------------------------

CHUNK 53
File: unknown
Size: 95 chars
Content:
----------------------------------------
Interpreting Topics Models What is the meaning of each topic? How to set the number of topics? 
----------------------------------------

CHUNK 54
File: unknown
Size: 40 chars
Content:
----------------------------------------
How to evaluate the resulting topics? 19
----------------------------------------

CHUNK 55
File: unknown
Size: 176 chars
Content:
----------------------------------------
Evaluating Topic Modeling Manual Inspection / Human judgement Top ranked words Intrinsic Evaluation Coherence score Intruder test Extrinsic Evaluation Downstream application 20
----------------------------------------

CHUNK 56
File: unknown
Size: 445 chars
Content:
----------------------------------------
Coherence Score Whether the words in a topic is coherent in terms of semantic similarity p(w , w ) i j UCI coherence measure log ∑ p(w )p(w ) i j i<j 1 + D(w , w ) i j UMass coherence measure log ∑ D(w ) i i<j Mimno, David, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. "Optimizing semantic coherence in topic models." In Proceedings of the 2011 conference on empirical methods in natural language processing, pp. 262-272. 
----------------------------------------

CHUNK 57
File: unknown
Size: 272 chars
Content:
----------------------------------------
2011. Newman, David, Jey Han Lau, Karl Grieser, and Timothy Baldwin. "Automatic evaluation of topic coherence." In Human language technologies: The 2010 annual conference of the North American chapter of the association for computational linguistics, pp. 100-108. 2010. 21
----------------------------------------

CHUNK 58
File: unknown
Size: 420 chars
Content:
----------------------------------------
Word Intrusion Task Given a few randomly ordered words, find the word which is out of place or does not belong with the others, i.e., the intruder Dog, cat, horse, apple, pig, cow Car, teacher, platypus, agile, blue, Zaire Chang, Jonathan, Sean Gerrish, Chong Wang, Jordan Boyd-Graber, and David Blei. "Reading tea leaves: How humans interpret topic models." Advances in neural information processing systems 22 (2009). 
----------------------------------------

CHUNK 59
File: unknown
Size: 2 chars
Content:
----------------------------------------
22
----------------------------------------

CHUNK 60
File: unknown
Size: 281 chars
Content:
----------------------------------------
Topic Intrusion Tests whether a topic model’s decomposition of documents into a mixture of topics agrees with human judgements of the document’s content Given a title and a snippet from a document, judge which topic out of the four given topics does not belong with the document 23
----------------------------------------

CHUNK 61
File: unknown
Size: 41 chars
Content:
----------------------------------------
Two Intrusion Tasks to Evaluate Topics 24
----------------------------------------

CHUNK 62
File: unknown
Size: 223 chars
Content:
----------------------------------------
Toolkits & Interactive topic model visualization • Gensim • https://github.com/bmabey/pyLDAvis • Jupiter Notebook demo Řehůřek, Radim, and Petr Sojka. "Software framework for topic modelling with large corpora." (2010). 
----------------------------------------

CHUNK 63
File: unknown
Size: 2 chars
Content:
----------------------------------------
25
----------------------------------------

CHUNK 64
File: unknown
Size: 119 chars
Content:
----------------------------------------
Overview What is topic modeling? LDA topic modeling Evaluation methods LDA variants SeededLDA Structural Topic Model 26
----------------------------------------

CHUNK 65
File: unknown
Size: 150 chars
Content:
----------------------------------------
What if the input text is “noisy”? Removing non-latin characters Filtering out stop words e.g., “the”, “is” and “and” Converting words to lower case? 
----------------------------------------

CHUNK 66
File: unknown
Size: 75 chars
Content:
----------------------------------------
Filtering out words with a frequency less than k Performing stemming ... 27
----------------------------------------

CHUNK 67
File: unknown
Size: 358 chars
Content:
----------------------------------------
What if the input text is short? Dirichlet Multinomial Mixture model for short text clustering (GSDMM) The Movie Group Process Yin, Jianhua, and Jianyong Wang. "A dirichlet multinomial mixture model-based approach for short text clustering." In Proceedings of the 20th ACM SIGKDD international conference on Knowledge discovery and data mining, pp. 233-242. 
----------------------------------------

CHUNK 68
File: unknown
Size: 7 chars
Content:
----------------------------------------
2014 28
----------------------------------------

CHUNK 69
File: unknown
Size: 175 chars
Content:
----------------------------------------
What if the input text is short? Dirichlet Multinomial Mixture model for short text clustering (GSDMM) K p(d) = p(d | z = k)p(z = k) ∑ k=1 p(d | z = k) = Π p(w | z = k) w∈d 29
----------------------------------------

CHUNK 70
File: unknown
Size: 76 chars
Content:
----------------------------------------
What if the input text is short? Performance of the models on the TweetSet. 
----------------------------------------

CHUNK 71
File: unknown
Size: 38 chars
Content:
----------------------------------------
https://github.com/rwalk/gsdmm-rust 30
----------------------------------------

CHUNK 72
File: unknown
Size: 559 chars
Content:
----------------------------------------
What if there are user priors? “To improve topic-word distributions, we set up a model in which each topic prefers to generate words that are related to the words in a seed set” “To improve document-topic distributions, we encourage the model to select topics based on the existence of input seed words in that document” Jagarlamudi, Jagadeesh, Hal Daumé III, and Raghavendra Udupa. "Incorporating lexical priors into topic models." In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pp. 204-213. 
----------------------------------------

CHUNK 73
File: unknown
Size: 8 chars
Content:
----------------------------------------
2012. 31
----------------------------------------

CHUNK 74
File: unknown
Size: 373 chars
Content:
----------------------------------------
What if there are user priors? (seededLDA) SeededLDA allows one to specify seed words that can influence the discovered topics Ramesh, Arti, Dan Goldwasser, Bert Huang, Hal Daumé III, and Lise Getoor. "Understanding MOOC discussion forums using seeded LD3A2." In Proceedings of the ninth workshop on innovative use of NLP for building educational applications, pp. 28-33. 
----------------------------------------

CHUNK 75
File: unknown
Size: 5 chars
Content:
----------------------------------------
2014.
----------------------------------------

CHUNK 76
File: unknown
Size: 45 chars
Content:
----------------------------------------
What if there are user priors? (seededLDA) 33
----------------------------------------

CHUNK 77
File: unknown
Size: 489 chars
Content:
----------------------------------------
What if there are some topics are related? “ Topic proportions θ can be correlated, and the prevalence of these topics can be influenced by some set of covariates X through a standard regression model with covariates Roberts, Margaret E., Brandon M. Stewart, Dustin Tingley, and Edoardo M. Airoldi. "The structural topic model and applied social science." In Advances in neural information processing systems workshop on topic models: computation, application, and evaluation, vol. 4, no. 
----------------------------------------

CHUNK 78
File: unknown
Size: 21 chars
Content:
----------------------------------------
1, pp. 1-20. 2013. 34
----------------------------------------

CHUNK 79
File: unknown
Size: 297 chars
Content:
----------------------------------------
The Structural Topic Model • Topics can be correlated • Each document has its own prior distribution over topics, defined by covariate X rather than sharing a global mean • Word use within a topic can vary by covariate U Provide a way of “structuring” the prior distributions in the topic model 35
----------------------------------------

CHUNK 80
File: unknown
Size: 127 chars
Content:
----------------------------------------
The STM for Open-ended Questions in Survey Experiments Party ID, Treatment, and the Predicted Proportion in Fear Topic (1 of 3)
----------------------------------------

CHUNK 81
File: unknown
Size: 138 chars
Content:
----------------------------------------
How News Wires Describe China’s Rise, 1997-2006 Taiwanese Presidential Election Topic (1 of 80) with news-source specific content (2 of 5)
----------------------------------------

CHUNK 82
File: unknown
Size: 170 chars
Content:
----------------------------------------
Overview What is topic modeling? LDA topic modeling Evaluation methods LDA variants SeededLDA Structural Topic Model LLM based topic modeling BERTopic, TopicGPT, LLooM 38
----------------------------------------

CHUNK 83
File: unknown
Size: 405 chars
Content:
----------------------------------------
BERTopic in 3 steps 1. Each document is converted to its embedding representation using a pretrained language model 2. The dimensionality of these embeddings is reduced to optimize clustering 3. Topic representations are extracted using a class-based variation of TF-IDF Grootendorst, Maarten. "BERTopic: Neural topic modeling with a class-based TF-IDF procedure." arXiv preprint arXiv:2203.05794 (2022). 
----------------------------------------

CHUNK 84
File: unknown
Size: 2 chars
Content:
----------------------------------------
39
----------------------------------------

CHUNK 85
File: unknown
Size: 256 chars
Content:
----------------------------------------
Topic Representation N Classic TF-IDF W = tf ⋅ log( ) t,d t,d df t Custom Class TF-IDF: models the importance of words in clusters N W = tf ⋅ log(1 + ) t,c t,c tf t The average number of words per class A divided by the freq of term t across all classes 40
----------------------------------------

CHUNK 86
File: unknown
Size: 416 chars
Content:
----------------------------------------
Topic Representation and Dynamic Topic Model N Classic TF-IDF W = tf ⋅ log( ) t,d t,d df t Custom Class TF-IDF: models the importance of words in clusters N W = tf ⋅ log(1 + ) t,c t,c tf t Create the local representation of each Local representation of each topic: topic by multiplying the term frequency of N documents at timestamp t with the pre- W = tf ⋅ log(1 + ) t,c,i t,c,i calculated global IDF values tf t 41
----------------------------------------

CHUNK 87
File: unknown
Size: 274 chars
Content:
----------------------------------------
BERTopic in 3 steps Topic diversity: the percentage of unique words for all topics Topic coherence: normalized pointwise mutual information Grootendorst, Maarten. "BERTopic: Neural topic modeling with a class-based TF-IDF procedure." arXiv preprint arXiv:2203.05794 (2022). 
----------------------------------------

CHUNK 88
File: unknown
Size: 2 chars
Content:
----------------------------------------
42
----------------------------------------

CHUNK 89
File: unknown
Size: 49 chars
Content:
----------------------------------------
BertTopic https://huggingface.co/blog/bertopic 43
----------------------------------------

CHUNK 90
File: unknown
Size: 206 chars
Content:
----------------------------------------
TopicGPT: A Prompt-based Topic Modeling Framework Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 
----------------------------------------

CHUNK 91
File: unknown
Size: 2 chars
Content:
----------------------------------------
44
----------------------------------------

CHUNK 92
File: unknown
Size: 506 chars
Content:
----------------------------------------
TopicGPT: A Prompt-based Topic Modeling Framework 1) Topic Generation: Given a corpus and some manually-curated example topics, TopicGPT identifies additional topics in each corpus document. 2) Topic Assignment: Given the generated topics, TopicGPT assigns the most relevant topic to each document and provides a quote that supports this assignment. Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 
----------------------------------------

CHUNK 93
File: unknown
Size: 2 chars
Content:
----------------------------------------
45
----------------------------------------

CHUNK 94
File: unknown
Size: 587 chars
Content:
----------------------------------------
More Metrics for Topic Alignment Given a set of ground-truth classes and a set of predicted assignment clusters Purity: harmonic mean of purity and inverse purity to match each ground-truth category with the cluster that has the highest combined precision and recall. Adjusted Rand Index: pairwise agreement between two sets of clusters Normalized Mutual Information: the amount of shared information between two sets of clusters. Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 
----------------------------------------

CHUNK 95
File: unknown
Size: 2 chars
Content:
----------------------------------------
46
----------------------------------------

CHUNK 96
File: unknown
Size: 340 chars
Content:
----------------------------------------
Topical alignment between ground-truth labels and predicted assignments TopicGPT achieves the best performance across all settings and metrics compared to LDA, BERTopic, and SeededLDA Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 
----------------------------------------

CHUNK 97
File: unknown
Size: 2 chars
Content:
----------------------------------------
47
----------------------------------------

CHUNK 98
File: unknown
Size: 204 chars
Content:
----------------------------------------
Example topic assignments from TopicGPT and LDA Pham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. "TopicGPT: A prompt-based topic modeling framework." arXiv preprint arXiv:2311.01449 (2023). 
----------------------------------------

CHUNK 99
File: unknown
Size: 2 chars
Content:
----------------------------------------
48
----------------------------------------

CHUNK 100
File: unknown
Size: 278 chars
Content:
----------------------------------------
Concept Induction via LLooM (https://stanfordhci.github.io/lloom) Lam, Michelle S., Janice Teoh, James Landay, Jeffrey Heer, and Michael S. Bernstein. "Concept Induction: Analyzing Unstructured Text with High-Level Concepts Using LLooM." arXiv preprint arXiv:2404.12259 (2024). 
----------------------------------------

CHUNK 101
File: unknown
Size: 2 chars
Content:
----------------------------------------
49
----------------------------------------

CHUNK 102
File: unknown
Size: 2 chars
Content:
----------------------------------------
50
----------------------------------------

CHUNK 103
File: unknown
Size: 170 chars
Content:
----------------------------------------
Overview What is topic modeling? LDA topic modeling Evaluation methods LDA variants SeededLDA Structural Topic Model LLM based topic modeling BERTopic, TopicGPT, LLooM 51
----------------------------------------

CHUNK 104
File: unknown
Size: 99 chars
Content:
----------------------------------------
Topic Modelling with Scikit-learn Derek Greene School of Computer Science University College Dublin
----------------------------------------

CHUNK 105
File: unknown
Size: 323 chars
Content:
----------------------------------------
Overview • Scikit-learn • Introduction to topic modelling • Working with text data • Topic modelling algorithms • Non-negative Matrix Factorisation (NMF) • Topic modelling with NMF in Scikit-learn • Parameter selection for NMF • Practical issues Code, data, and slides: https://github.com/derekgreene/topic-model-tutorial 2
----------------------------------------

CHUNK 106
File: unknown
Size: 97 chars
Content:
----------------------------------------
Scikit-learn pip install scikit-learn conda install scikit-learn http://scikit-learn.org/stable 3
----------------------------------------

CHUNK 107
File: unknown
Size: 930 chars
Content:
----------------------------------------
Introduction to Topic Modelling Topic modelling aims to automatically discover the hidden thematic structure in a large corpus of text documents. Topics Documents LeBron James says President Trump 'trying to divide Topic 1 through sport' Basketball LeBron Basketball star LeBron James has praised the American football players who NBA have protested against Donald Trump, and accused the US president of "using ... sports to try and divide us". Trump said that NFL players who fail to stand during the national anthem should Topic 2 be sacked or suspended. NFL Football James praised the players' unity, and said: "The people run this country." American ... James, who plays for the Cleveland Cavaliers and has won three NBA championships, campaigned for Hillary Clinton, Trump's rival, during the 2016 presidential election campaign. Topic 3 Trump President Clinton A document is composed of terms related to one or more topics. 
----------------------------------------

CHUNK 108
File: unknown
Size: 5 chars
Content:
----------------------------------------
... 4
----------------------------------------

CHUNK 109
File: unknown
Size: 146 chars
Content:
----------------------------------------
Introduction to Topic Modelling • Topic modelling is an unsupervised text mining approach. • Input: A corpus of unstructured text documents (e.g. 
----------------------------------------

CHUNK 110
File: unknown
Size: 359 chars
Content:
----------------------------------------
news articles, tweets, speeches etc). No prior annotation or training set is typically required. Input Output Topic 1 Data Topic Topic 2 Pre- Modelling processing Algorithm Topic k • Output: A set of k topics, each of which is represented by: 1. A descriptor, based on the top-ranked terms for the topic. 2. Associations for documents relative to the topic. 5
----------------------------------------

CHUNK 111
File: unknown
Size: 121 chars
Content:
----------------------------------------
Introduction to Topic Modelling Top Terms for Topic 1 Top Terms for Topic 2 Top Terms for Topic 3 Top Terms for Topic 4 6
----------------------------------------

CHUNK 112
File: unknown
Size: 176 chars
Content:
----------------------------------------
Introduction to Topic Modelling In the output of topic modelling, a single document can potentially be associated with multiple topics... Politics or Health? Business or Sport?
----------------------------------------

CHUNK 113
File: unknown
Size: 962 chars
Content:
----------------------------------------
Application: News Media We can use topic modelling to uncover the dominant stories and subjects in a corpus of news articles. Rank Term Article Headline Weight 1 eu Archbishop accuses Farage of racism and 'accentuating fear' 0.20 2 brexit Cameron names referendum date as Gove declares for Brexit 0.20 Topic 1 3 uk Cameron: EU referendum is a 'once in a generation' decision 0.18 4 britain Remain camp will win EU referendum by a 'substantial margin' 0.18 5 referendum EU referendum: Cameron claims leaving EU could make cutting... 0.18 Rank Term Document Title Weight 1 trump Donald Trump: money raised by Hillary Clinton is 'blood money' 0.27 2 clinton Second US presidential debate – as it happened 0.27 Topic 2 3 republican Donald Trump hits delegate count needed for Republican nomination 0.26 4 donald Trump campaign reportedly vetting Christie, Gingrich as potential... 0.26 5 campaign Trump: 'Had I been president, Capt Khan would be alive today' 0.26 88
----------------------------------------

CHUNK 114
File: unknown
Size: 132 chars
Content:
----------------------------------------
Application: Social Media Topic modelling applied to 4,170,382 tweets from 1,200 prominent Twitter accounts, posted over 12 months. 
----------------------------------------

CHUNK 115
File: unknown
Size: 403 chars
Content:
----------------------------------------
Topics can be identified based on either individual tweets, or at the user profile level. Topic 1 Topic 2 Topic 3 Rank Term Rank Term Rank Term 1 space 1 #health 1 apple 2 #yearinspace 2 cancer 2 iphone 3 pluto 3 study 3 #ios 4 earth 4 risk 4 ipad 5 nasa 5 patients 5 mac 6 mars 6 care 6 app 7 mission 7 diabetes 7 watch 8 launch 8 #zika 8 apps 9 #journeytomars 9 drug 9 os 10 science 10 disease 10 tv 9
----------------------------------------

CHUNK 116
File: unknown
Size: 289 chars
Content:
----------------------------------------
Application: Political Speeches Analysis of 400k European Parliament speeches from 1999-2014 to uncover agenda and priorities of MEPs (Greene & Cross, 2017). 1200 1000 800 600 400 200 0 2000 2002 2004 2006 2008 2010 2012 2014 10 sehceepS fo rebmuN Financial crisis D Euro crisis A C B Year
----------------------------------------

CHUNK 117
File: unknown
Size: 227 chars
Content:
----------------------------------------
Other Applications Topic models have also been applied to discover the underlying patterns across a range of different non-textual datasets. LEGO colour themes as topic models https://nateaff.com/2017/09/11/lego-topic-models 11
----------------------------------------

CHUNK 118
File: unknown
Size: 22 chars
Content:
----------------------------------------
Working with Text Data
----------------------------------------

CHUNK 119
File: unknown
Size: 220 chars
Content:
----------------------------------------
Working with Text Data Most text data arrives in an unstructured form without any pre- defined organisation or format, beyond natural language. The vocabulary, formatting, and quality of the text can vary significantly. 
----------------------------------------

CHUNK 120
File: unknown
Size: 2 chars
Content:
----------------------------------------
13
----------------------------------------

CHUNK 121
File: unknown
Size: 535 chars
Content:
----------------------------------------
Text Preprocessing • Documents are textual, not numeric. The first step in analysing unstructured documents is tokenisation: split raw text into individual tokens, each corresponding to a single term. • For English we typically split a text document based on whitespace. Punctuation symbols are often used to split too: text = "Apple reveals new iPhone model" text.split() ['Apple', 'reveals', 'new', 'iPhone', 'model'] • Splitting by whitespace will not work for some languages: e.g. Chinese, Japanese, Korean; German compound nouns. 
----------------------------------------

CHUNK 122
File: unknown
Size: 88 chars
Content:
----------------------------------------
• For some types of text content, certain characters can have a special significance: 14
----------------------------------------

CHUNK 123
File: unknown
Size: 77 chars
Content:
----------------------------------------
Bag-of-Words Representation • How can we go from tokens to numeric features? 
----------------------------------------

CHUNK 124
File: unknown
Size: 642 chars
Content:
----------------------------------------
• Bag-of-Words Model: Each document is represented by a vector in a m-dimensional coordinate space, where m is number of unique terms across all documents (the corpus vocabulary). Example: Document 1: When we tokenise our corpus of 3 Forecasts cut as IMF issues documents, we have a vocabulary of warning 14 distinct terms Document 2: vocab = set() IMF and WBG meet to for doc in corpus: discuss economy tokens = tokenize(doc) for tok in tokens: vocab.add(tok) Document 3: print(vocab) WBG issues 2016 growth {'2016', 'Forecasts', 'IMF', 'WBG', 'and', warning 'as', 'cut', 'discuss', 'economy', 'growth', 'issues', 'meet', 'to', 'warning'} 15
----------------------------------------

CHUNK 125
File: unknown
Size: 710 chars
Content:
----------------------------------------
Bag-of-Words Representation • Each document can be represented as a term vector, with an entry indicating the number of time a term appears in the document: Document 1: Forecasts cut as IMF issues warning 0 1 1 0 0 1 1 0 0 0 1 0 0 1 16 6102 stsaceroF FMI GBW dna sa tuc ssucsid ymonoce htworg seussi teem ot gninraw 0 1 1 0 0 1 1 0 0 0 1 0 0 1 0 0 1 1 1 0 0 1 1 0 0 1 1 0 2 0 0 1 0 0 0 0 0 1 1 0 0 1 6102 stsaceroF FMI GBW dna sa tuc ssucsid ymonoce htworg seussi teem ot gninraw • By transforming all documents in this way, and stacking them in rows, we create a full document-term matrix: Document 2: IMF and WBG meet to discuss economy Document 3: 2016: WBG issues 2016 growth warning 3 Documents x 14 Terms
----------------------------------------

CHUNK 126
File: unknown
Size: 851 chars
Content:
----------------------------------------
Bag-of-Words in Scikit-learn • Scikit-learn includes functionality to easily transform a collection of strings containing documents into a document-term matrix. Our input, documents, is a list of strings. Each string is a separate document. from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer() A = vectorizer.fit_transform(documents) Our output, A, is a sparse NumPy 2D array with rows corresponding to documents and columns corresponding to terms. • Once the matrix has been created, we can access the list of all terms and an associated dictionary (vocabulary_) which maps each unique term to a corresponding column in the matrix. terms = vectorizer.get_feature_names_out() vocab = vectorizer.vocabulary_ len(terms) vocab["world"] 3288 3246 How many terms in the vocabulary? Which column corresponds to a term? 
----------------------------------------

CHUNK 127
File: unknown
Size: 2 chars
Content:
----------------------------------------
17
----------------------------------------

CHUNK 128
File: unknown
Size: 449 chars
Content:
----------------------------------------
Further Text Preprocessing • The number of terms used to represent documents is often reduced by applying a number of simple preprocessing techniques before building a document-term matrix: - Minimum term length: Exclude terms of length < 2 - Case conversion: Converting all terms to lowercase. - Stop-word filtering: Remove terms that appear on a pre-defined filter list of terms that are highly frequent and do not convey useful information (e.g. 
----------------------------------------

CHUNK 129
File: unknown
Size: 351 chars
Content:
----------------------------------------
and, the, while) - Minimum frequency filtering: Remove all terms that appear in very few documents. - Maximum frequency filtering: Remove all terms that appear in a very large number of documents. - Stemming: Process by which endings are removed from terms in order to remove things like tense or plurals: e.g. compute, computing, computer = comput 18
----------------------------------------

CHUNK 130
File: unknown
Size: 634 chars
Content:
----------------------------------------
Further Text Preprocessing • Further preprocessing steps can be applied directly using the CountVectorizer class by passing appropriate parameters - e.g.: from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer( stop_words=custom_list, min_df=20, max_df=1000, lowercase=False, ngram_range=2) A = vectorizer.fit_transform(documents) Parameter Explanation stop_words=custom_list Pass in a custom list containing terms to filter. min_df=20 Filter those terms that appear in < 20 documents. max_df=1000 Filter those terms that appear in > 1000 documents. lowercase=False Do not convert text to lowercase. 
----------------------------------------

CHUNK 131
File: unknown
Size: 92 chars
Content:
----------------------------------------
Default is True. ngram_range=2 Include phrases of length 2, instead of just single words. 19
----------------------------------------

CHUNK 132
File: unknown
Size: 524 chars
Content:
----------------------------------------
Term Weighting • As well as including or excluding terms, we can improve the usefulness of the document-term matrix by giving higher weights to more "important" terms. • TF-IDF: Common approach for weighting the score for a term in a document. Consists of two parts: - Term Frequency (TF): Number of times a given term appears in a single document. - Inverse Document Frequency (IDF): Function of total number of distinct documents containing a term. Effect is to penalise common terms that appear in almost every document. 
----------------------------------------

CHUNK 133
File: unknown
Size: 243 chars
Content:
----------------------------------------
w(t, D) = tf (t, d) (log( n ) + 1) n = total number df(t) ⇥ of documents • Example: the term "cat" appears in a given document 3 times and appears 50 times overall in a corpus of 1000 documents: 1000 w(cat, D) = 3 (log( ) + 1) = 11.987 50 ⇥ 20
----------------------------------------

CHUNK 134
File: unknown
Size: 692 chars
Content:
----------------------------------------
Term Weighting in Scikit-learn • A similar vectorisation approach can be used in Scikit-learn to produce a TF-IDF normalised document-term matrix: from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer() A = vectorizer.fit_transform(documents) The output, A, is a sparse NumPy array where the entries are all TF-IDF normalised. • Again we can perform additional preprocessing steps by passing the appropriate parameter values to TfidfVectorizer: from sklearn.feature_extraction.text import TfidfVectorizer vectorizer = TfidfVectorizer( stop_words=custom_list, min_df=20, max_df=1000, lowercase=False, ngram_range=2) A = vectorizer.fit_transform(documents) 21
----------------------------------------

CHUNK 135
File: unknown
Size: 329 chars
Content:
----------------------------------------
Text Preprocessing Pipeline • Typical text preprocessing steps for a document corpus... Case Filter Short Filter Stop- Tokenisation Conversion Terms Words Corpus of Raw Documents Min/Max Term Stemming Filtering Term Vectorisation Weighting Document Term Matrix • Note: Stemming is not included in scikit-learn. See NLTK package. 
----------------------------------------

CHUNK 136
File: unknown
Size: 114 chars
Content:
----------------------------------------
• Once we have our document-term matrix, we are ready to apply machine learning algorithms to explore the data. 22
----------------------------------------

CHUNK 137
File: unknown
Size: 15 chars
Content:
----------------------------------------
Topic Modelling
----------------------------------------

CHUNK 138
File: unknown
Size: 567 chars
Content:
----------------------------------------
Topic Modelling Algorithms Various different methods for topic modelling have been proposed. Two general approaches are popular: 1. Probabilistic approaches - View each document as a mixture of a small number of topics. - Words and documents get probability scores for each topic. - e.g. Latent Dirichlet Allocation (LDA) (Blei et al, 2003). 2. Matrix factorisation approaches - Apply methods from linear algebra to decompose a single matrix (e.g. document-term matrix) into a set of smaller matrices. - For text data, we can interpret these as a topic model. - e.g. 
----------------------------------------

CHUNK 139
File: unknown
Size: 63 chars
Content:
----------------------------------------
Non-negative Matrix Factorisation (NMF) (Lee & Seung, 1999). 24
----------------------------------------

CHUNK 140
File: unknown
Size: 382 chars
Content:
----------------------------------------
Non-negative Matrix Factorisation • Non-negative Matrix Factorisation (NMF): Family of linear algebra algorithms for identifying the latent structure in data represented as a non-negative matrix (Lee & Seung, 1999). • NMF can be applied for topic modeling, where the input is a document-term matrix, typically TF-IDF normalised. • Input: Document-term matrix A; Number of topics k. 
----------------------------------------

CHUNK 141
File: unknown
Size: 176 chars
Content:
----------------------------------------
• Output: Two k-dimensional factors W and H approximating A. m k m · n A n W k H NMF Factor H (topics x terms) Input Matrix Factor W (documents x terms) (documents x topics) 25
----------------------------------------

CHUNK 142
File: unknown
Size: 322 chars
Content:
----------------------------------------
Example: NMF Topic Modelling Apply NMF topic modelling to a small document-term matrix A representing a corpus of 6 documents, to generate k=3 topics... document 1 document 2 document 3 document 4 document 5 document 6 26 hcraeser loohcs noitacude esaesid tneitap htlaeh tegdub ecnanif gniknab sdnob 6 Documents x 10 Terms
----------------------------------------

CHUNK 143
File: unknown
Size: 559 chars
Content:
----------------------------------------
Example: NMF Topic Modelling Factor W Factor H Weights for 6 documents Weights for 10 terms relative to 3 topics relative to 3 topics Topic 1 Topic 2 Topic 3 Topic 1 Topic 2 Topic 3 research 0.0 0.0 1.0 document 1 0.0 1.0 1.0 school 0.0 0.1 0.1 document 2 0.0 0.0 1.0 education 0.0 0.0 1.0 document 3 0.7 0.0 0.0 disease 0.0 0.6 0.0 patient 0.0 0.7 0.0 document 4 0.7 0.0 0.0 health 0.0 1.0 0.0 document 5 0.0 0.0 1.0 budget 0.3 0.1 0.2 finance 0.6 0.0 0.0 document 6 0.0 1.0 1.0 0.7 0.0 0.0 banking 6 Rows x 3 Columns 0.3 0.0 0.0 bonds 10 Rows x 3 Columns 27
----------------------------------------

CHUNK 144
File: unknown
Size: 632 chars
Content:
----------------------------------------
Applying NMF in Scikit-learn • Scikit-learn includes a fast implementation of NMF. • By default, the values in factors W and H are given random initial values. The key required input parameter is the number of topics (components) k: from sklearn import decomposition Apply NMF to document-term model = decomposition.NMF(n_components=k) matrix A, extract the resulting W = model.fit_transform( A ) factors W and H H = model.components_ • When using random initialisation, the results can be different every time NMF is applied to the same data. More reliable results can be obtained if you initialise with SVD (Belford et al, 2018). 
----------------------------------------

CHUNK 145
File: unknown
Size: 144 chars
Content:
----------------------------------------
from sklearn import decomposition model = decomposition.NMF(n_components=k, init="nndsvd") W = model.fit_transform( A ) H = model.components_ 28
----------------------------------------

CHUNK 146
File: unknown
Size: 204 chars
Content:
----------------------------------------
Applying NMF in Scikit-learn • The H factor contains term weights relative to each of the k topics. Each row corresponds to a topic, and each column corresponds to a unique term in the corpus vocabulary. 
----------------------------------------

CHUNK 147
File: unknown
Size: 1109 chars
Content:
----------------------------------------
• Sorting the values in each row gives us a ranking of terms - the descriptor of each topic. import numpy as np For each topic, sort the row top_indices = np.argsort( H[topic_index,:] )[::-1] indices in reverse, then get top_terms = [] for term_index in top_indices[0:top]: the terms for the top indices. top_terms.append( terms[term_index] ) Repeat for all topics to get the full set of descriptors: Topic 01: eu, brexit, uk, britain, referendum, leave, vote, european, cameron, labour Topic 02: trump, clinton, republican, donald, campaign, president, hillary, cruz, sanders, election Topic 03: film, films, movie, star, hollywood, director, actor, story, drama, women Topic 04: league, season, leicester, goal, premier, united, city, liverpool, game, ball Topic 05: bank, banks, banking, financial, rbs, customers, shares, deutsche, barclays, lloyds Topic 06: health, nhs, care, patients, mental, doctors, hospital, people, services, junior Topic 07: album, music, band, song, pop, songs, rock, love, sound, bowie Topic 08: internet, facebook, online, people, twitter, media, users, google, company, amazon
----------------------------------------

CHUNK 148
File: unknown
Size: 599 chars
Content:
----------------------------------------
Applying NMF in Scikit-learn • The W factor contains document membership weights across the k topics. Each row corresponds to a different document, and each column corresponds to a topic. • Sorting the values gives us a ranking of the most relevant documents for each topic. For each topic, sort column top_indices = np.argsort( W[:,topic_index] )[::-1] indices in reverse, then get top_documents = [] the documents for the top for doc_index in top_indices[0:top]: top_documents.append( documents[doc_index] ) indices. The top documents for a topic might be summarised using titles or snippets: 01. 
----------------------------------------

CHUNK 149
File: unknown
Size: 671 chars
Content:
----------------------------------------
Donald Trump: money raised by Hillary Clinton is 'blood money' 02. Second US presidential debate – as it happened 03. Trump campaign reportedly vetting Christie, Gingrich as potential running mates 04. Donald Trump hits delegate count needed for Republican nomination 05. Trump: 'Had I been president, Capt Khan would be alive today' 06. Clinton seizes on Trump tweets for day of campaigning in Florida 07. Melania Trump defends husband's 'boy talk' in CNN interview 08. Hillary Clinton: 'I'm sick of the Sanders campaign's lies' 09. Donald Trump at the White House: Obama reports 'excellent conversation' 10. Donald Trump: Hillary Clinton has 'no right to be running' 30
----------------------------------------

CHUNK 150
File: unknown
Size: 820 chars
Content:
----------------------------------------
Applying NMF in Scikit-learn Topic 01: eu, brexit, uk, britain, referendum, leave, vote, european, cameron, labour Topic 02: trump, clinton, republican, donald, campaign, president, hillary, cruz, sanders, election Topic 03: film, films, movie, star, hollywood, director, actor, story, drama, women Topic 04: league, season, leicester, goal, premier, united, city, liverpool, game, ball Topic 05: bank, banks, banking, financial, rbs, customers, shares, deutsche, barclays, lloyds Topic 06: health, nhs, care, patients, mental, doctors, hospital, people, services, junior Topic 07: album, music, band, song, pop, songs, rock, love, sound, bowie Topic 08: internet, facebook, online, people, twitter, media, users, google, company, amazon 01. The lost albums loved by the stars – from ecstatic gospel to Italian prog 02. 
----------------------------------------

CHUNK 151
File: unknown
Size: 370 chars
Content:
----------------------------------------
How to write a banger for Beyoncé 03. Albums of the year 2016 – our readers respond 04. Why Nirvana's In Bloom is busting out all over 05. Dead Kennedys – 10 of the best 06. Mogwai – 10 of the best 07. Marillion – 10 of the best 08. 'In the Faroe Islands, everyone is in a band' 09. Pop, rock, rap, whatever: who killed the music genre? 10. Iggy Pop – 10 of the best 31
----------------------------------------

CHUNK 152
File: unknown
Size: 487 chars
Content:
----------------------------------------
Parameter Selection • The key parameter selection decision for topic modelling involves choosing the number of topics k. • Common approach: Measure and compare the topic coherence of models generated for different values of k. • Topic coherence: The extent to which the top terms representing a topic (i.e. the topic descriptor) are semantically related, relative to some "background corpus". • A variety of different measures exist for measuring coherence e.g. NPMI, UMass, TC-W2V etc. 
----------------------------------------

CHUNK 153
File: unknown
Size: 259 chars
Content:
----------------------------------------
(O'Callaghan et al, 2015). Rank Word Rank Word Rank Word 1 port 1 agriculture 1 farmer 2 sea 2 farmer 2 naval 3 maritime 3 beef 3 dairy 4 naval 4 food 4 maritime 5 vessel 5 dairy 5 nuclear "High coherence topic" "High coherence topic" "Low coherence topic" 32
----------------------------------------

CHUNK 154
File: unknown
Size: 115 chars
Content:
----------------------------------------
Parameter Selection • Typical approach for parameter selection: 1. Apply NMF for a "sensible" range k=[kmin,kmax]. 
----------------------------------------

CHUNK 155
File: unknown
Size: 220 chars
Content:
----------------------------------------
2. Calculate mean coherence of the topics produced for each k, relative to the overall corpus or a related background corpus. 3. Select the value of k giving the highest mean coherence. Number of Topics 33 ecnerehoC naeM
----------------------------------------

CHUNK 156
File: unknown
Size: 692 chars
Content:
----------------------------------------
Practical Issues • Preprocessing • Stop-word filtering often has a major impact. • TF-IDF often leads to more useful topics than raw frequencies. • Initialisation • Random initialisation of both NMF and LDA can lead to unstable results, particularly for larger datasets. • Scalability • NMF typically more scalable than LDA, but running times can increase considerably as number of topics k increases. • Parameter Selection • In many cases, there can be several "good" values of k. • Choice of coherence measure can produce different results. • Interpretation • Topic models reflect the structure of the data available. Best used carefully as an exploratory tool to aid human interpretation. 
----------------------------------------

CHUNK 157
File: unknown
Size: 2 chars
Content:
----------------------------------------
34
----------------------------------------

CHUNK 158
File: unknown
Size: 86 chars
Content:
----------------------------------------
Any Questions? derek.greene@ucd.ie https://github.com/derekgreene/topic-model-tutorial
----------------------------------------

CHUNK 159
File: unknown
Size: 396 chars
Content:
----------------------------------------
References • Pedregosa, F., et al. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research 12. Oct (2011): 2825-2830. • Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022. • Blei, D. M. (2012). Probabilistic topic models. Communications of the ACM, 55(4), 77-84. • Lee, D. D., & Seung, H. S. 
----------------------------------------

CHUNK 160
File: unknown
Size: 450 chars
Content:
----------------------------------------
(1999). Learning the parts of objects by Non-negative Matrix Factorization. Nature, 401(6755), 788. • Belford, M., Mac Namee, B., & Greene, D. Stability of Topic Modeling via Matrix Factorization. Expert Systems with Applications, 2018. • O’Callaghan, D., Greene, D., Carthy, J., & Cunningham, P. (2015). An analysis of the coherence of descriptors in topic modeling. Expert Systems with Applications, 42(13), 5645-5657. • Greene, D., & Cross, J. P. 
----------------------------------------

CHUNK 161
File: unknown
Size: 316 chars
Content:
----------------------------------------
(2017). Exploring the Political Agenda of the European Parliament Using a Dynamic Topic Modeling Approach. Political Analysis, 25(1), 77-94. • Rehurek, R., & Sojka, P. (2010). Software framework for topic modelling with large corpora. In Proceedings of the LREC 2010 Workshop on New Challenges for NLP Frameworks. 36
----------------------------------------

CHUNK 162
File: unknown
Size: 320 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM HOME LLLLMM OOVVEERRVVIIEEWW SSLLIIDDEESS || LLLLMM && RRAAGG GGUUIIDDEE LLM OVERVIEW SLIDES | LLM & RAG GUIDE LLM 101 - THIS SLIDE SESSION WALK THRU LLM APPLICATIONS, ARCHITECTURE AND BUILDING BLOCKS. SLIDES DESCRIBE OPEN SOURCE VS CLOSED LLM AND HOW TO CHOOSE. 
----------------------------------------

CHUNK 163
File: unknown
Size: 346 chars
Content:
----------------------------------------
SLIDES ALSO FOCUS ON LLM STEERABILITY. SESSION SLIDES ALSO DESCRIBE PROMPT ENGINEERING , TOKENIZATION AND FACTORS TO CONSIDER WHILE BUILDING APPLICATIONS USING LLM. AT LAST IT DESCRIBE PROCESS ARCHITECTUR, COMPONENT ARCHITECTURE AND HOW TO ESTIMATE COST OF LLM. AGENDA AGENDA LLM TOPICS https://www.dataknobs.com/generativeai/10-llms Page 1 of 61
----------------------------------------

CHUNK 164
File: unknown
Size: 173 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM LLM TOPICS LLM OVERVIEW LLM OVERVIEW APPLICATIONS OF LLM https://www.dataknobs.com/generativeai/10-llms Page 2 of 61
----------------------------------------

CHUNK 165
File: unknown
Size: 192 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM APPLICATIONS OF LLM NLP APPLICATIONS NLP APPLICATIONS SOFTWARE APPLICATIONS https://www.dataknobs.com/generativeai/10-llms Page 3 of 61
----------------------------------------

CHUNK 166
File: unknown
Size: 221 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SOFTWARE APPLICATIONS EVOLOVING APPLICATIONS OF LLMS EVOLOVING APPLICATIONS OF LLMS OPEN SOURCE VS CLOSE https://www.dataknobs.com/generativeai/10-llms Page 4 of 61
----------------------------------------

CHUNK 167
File: unknown
Size: 185 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM OPEN SOURCE VS CLOSE BUILDING BLOCKS OF LLMS BUILDING BLOCKS OF LLMS https://www.dataknobs.com/generativeai/10-llms Page 5 of 61
----------------------------------------

CHUNK 168
File: unknown
Size: 198 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM MULTI MODEL LLMS MULTI MODEL LLMS COMMON TERMINOLOGY LLMS COMMON TERMINOLOGY LLMS https://www.dataknobs.com/generativeai/10-llms Page 6 of 61
----------------------------------------

CHUNK 169
File: unknown
Size: 179 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM AI ASSISTANTS TYPES AI ASSISTANTS TYPES TYPES OF AI ASSISTANTS https://www.dataknobs.com/generativeai/10-llms Page 7 of 61
----------------------------------------

CHUNK 170
File: unknown
Size: 222 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM TYPES OF AI ASSISTANTS FEATURES OF AI ASSISTANTS FEATURES OF AI ASSISTANTS EXAMPLE FEATURES OF AI ASSISTA https://www.dataknobs.com/generativeai/10-llms Page 8 of 61
----------------------------------------

CHUNK 171
File: unknown
Size: 209 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM EXAMPLE FEATURES OF AI ASSISTA AI ASSISTANT EVALUATION METRIC AI ASSISTANT EVALUATION METRIC https://www.dataknobs.com/generativeai/10-llms Page 9 of 61
----------------------------------------

CHUNK 172
File: unknown
Size: 223 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM ASSISTANT BOT METRICS ASSISTANT BOT METRICS METRICS TO EVALUATE AI ASSISTA METRICS TO EVALUATE AI ASSISTA https://www.dataknobs.com/generativeai/10-llms Page 10 of 61
----------------------------------------

CHUNK 173
File: unknown
Size: 202 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM TECHNICAL METRICS AI ASSISTANT TECHNICAL METRICS AI ASSISTANT METRICS FOR SEARCH BOT https://www.dataknobs.com/generativeai/10-llms Page 11 of 61
----------------------------------------

CHUNK 174
File: unknown
Size: 233 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM METRICS FOR SEARCH BOT METRICS FOR RECOMMENDATION BOT METRICS FOR RECOMMENDATION BOT BEHAVIORAL METRICS FOR RECOMME https://www.dataknobs.com/generativeai/10-llms Page 12 of 61
----------------------------------------

CHUNK 175
File: unknown
Size: 220 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM BEHAVIORAL METRICS FOR RECOMME CRITERIA TO COMPARE LLMS CRITERIA TO COMPARE LLMS LLM TECHNOLOGY SLIDES https://www.dataknobs.com/generativeai/10-llms Page 13 of 61
----------------------------------------

CHUNK 176
File: unknown
Size: 213 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM LLM TECHNOLOGY SLIDES AI ASSISTANT TECH STACK AI ASSISTANT TECH STACK AI ASSISTANT ARCHITECTURE https://www.dataknobs.com/generativeai/10-llms Page 14 of 61
----------------------------------------

CHUNK 177
File: unknown
Size: 205 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM AI ASSISTANT ARCHITECTURE CONSIDERAIONS FOR BOT ARCHITEC CONSIDERAIONS FOR BOT ARCHITEC https://www.dataknobs.com/generativeai/10-llms Page 15 of 61
----------------------------------------

CHUNK 178
File: unknown
Size: 155 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM RAG SLIDES RAG SLIDES WHEN TO USE RAG https://www.dataknobs.com/generativeai/10-llms Page 16 of 61
----------------------------------------

CHUNK 179
File: unknown
Size: 194 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM WHEN TO USE RAG WHEN NOT TO USE RAG WHEN NOT TO USE RAG AI ASSISTANT WRAPPER https://www.dataknobs.com/generativeai/10-llms Page 17 of 61
----------------------------------------

CHUNK 180
File: unknown
Size: 190 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM AI ASSISTANT WRAPPER AI ASSISTANT ON YOUR DATA AI ASSISTANT ON YOUR DATA https://www.dataknobs.com/generativeai/10-llms Page 18 of 61
----------------------------------------

CHUNK 181
File: unknown
Size: 254 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM AI ASSISTANT FINETUNE MODEL AI ASSISTANT FINETUNE MODEL AI ASSISTANT CUSTOM MODEL AI ASSISTANT CUSTOM MODEL AI ASSISTANT BUILDING BLOCKS https://www.dataknobs.com/generativeai/10-llms Page 19 of 61
----------------------------------------

CHUNK 182
File: unknown
Size: 167 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM AI ASSISTANT BUILDING BLOCKS LLM FUNCTION CALLING https://www.dataknobs.com/generativeai/10-llms Page 20 of 61
----------------------------------------

CHUNK 183
File: unknown
Size: 201 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM LLM FUNCTION CALLING RAG OVERVIEW SLIDES RAG OVERVIEW SLIDES RAG ARCHITECTURE SLIDE https://www.dataknobs.com/generativeai/10-llms Page 21 of 61
----------------------------------------

CHUNK 184
File: unknown
Size: 203 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM RAG ARCHITECTURE SLIDE RAG RETRIEVER OPTIONS RAG RETRIEVER OPTIONS RAG NODE PROCESSOR https://www.dataknobs.com/generativeai/10-llms Page 22 of 61
----------------------------------------

CHUNK 185
File: unknown
Size: 215 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM RAG NODE PROCESSOR RAG NODE POST PROCESSOR RAG NODE POST PROCESSOR HOW TO FORM RESPONSE IN AI ASS https://www.dataknobs.com/generativeai/10-llms Page 23 of 61
----------------------------------------

CHUNK 186
File: unknown
Size: 196 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM HOW TO FORM RESPONSE IN AI ASS LLM ARCHITECTUE FOR BOT LLM ARCHITECTUE FOR BOT https://www.dataknobs.com/generativeai/10-llms Page 24 of 61
----------------------------------------

CHUNK 187
File: unknown
Size: 176 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM LLM CONCERNS SLIDES LLM CONCERNS SLIDES LLM THREATS SLIDES https://www.dataknobs.com/generativeai/10-llms Page 25 of 61
----------------------------------------

CHUNK 188
File: unknown
Size: 206 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM LLM THREATS SLIDES LLM CHALLENGES SLIDE LLM CHALLENGES SLIDE LLM ETHICAL CONCERNS SLIDES https://www.dataknobs.com/generativeai/10-llms Page 26 of 61
----------------------------------------

CHUNK 189
File: unknown
Size: 227 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM LLM ETHICAL CONCERNS SLIDES LLM UNCONTROLLED BEHAVIOR SLID LLM UNCONTROLLED BEHAVIOR SLID LLLM ETHICAL ISSUES https://www.dataknobs.com/generativeai/10-llms Page 27 of 61
----------------------------------------

CHUNK 190
File: unknown
Size: 163 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM LLLM ETHICAL ISSUES DATA OWNERSHIP ISSUES LLM https://www.dataknobs.com/generativeai/10-llms Page 28 of 61
----------------------------------------

CHUNK 191
File: unknown
Size: 222 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM DATA OWNERSHIP ISSUES LLM LLM GENERATED OUTPUT ISSUES LLM GENERATED OUTPUT ISSUES LLM ENVIRONMENT ISSUES https://www.dataknobs.com/generativeai/10-llms Page 29 of 61
----------------------------------------

CHUNK 192
File: unknown
Size: 202 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM LLM ENVIRONMENT ISSUES ENTERPRISE GRADE ANSWERS AI AS ENTERPRISE GRADE ANSWERS AI AS https://www.dataknobs.com/generativeai/10-llms Page 30 of 61
----------------------------------------

CHUNK 193
File: unknown
Size: 225 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM APPROACHES TO VERIFY AI ASSIST APPROACHES TO VERIFY AI ASSIST EXAMPLE BOT ASSISTANTS EXAMPLE BOT ASSISTANTS https://www.dataknobs.com/generativeai/10-llms Page 31 of 61
----------------------------------------

CHUNK 194
File: unknown
Size: 196 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM CUSTOMER ONBOARDING BOT CUSTOMER ONBOARDING BOT CUSTOMER ONBOARDING AI ASSISAT https://www.dataknobs.com/generativeai/10-llms Page 32 of 61
----------------------------------------

CHUNK 195
File: unknown
Size: 245 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM CUSTOMER ONBOARDING AI ASSISAT ARCHITECTURE SLIDE FOR CUSTOME ARCHITECTURE SLIDE FOR CUSTOME SLIDE59 SLIDE59 LLM TRAINING STEPS https://www.dataknobs.com/generativeai/10-llms Page 33 of 61
----------------------------------------

CHUNK 196
File: unknown
Size: 196 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM LLM TRAINING STEPS ARCHITECTURE LLM TRAINING ARCHITECTURE LLM TRAINING SLIDE62 https://www.dataknobs.com/generativeai/10-llms Page 34 of 61
----------------------------------------

CHUNK 197
File: unknown
Size: 141 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE62 SLIDE63 SLIDE63 https://www.dataknobs.com/generativeai/10-llms Page 35 of 61
----------------------------------------

CHUNK 198
File: unknown
Size: 157 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE64 SLIDE64 SLIDE65 SLIDE65 SLIDE66 https://www.dataknobs.com/generativeai/10-llms Page 36 of 61
----------------------------------------

CHUNK 199
File: unknown
Size: 133 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE66 SLIDE67 https://www.dataknobs.com/generativeai/10-llms Page 37 of 61
----------------------------------------

CHUNK 200
File: unknown
Size: 149 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE67 SLIDE68 SLIDE68 SLIDE69 https://www.dataknobs.com/generativeai/10-llms Page 38 of 61
----------------------------------------

CHUNK 201
File: unknown
Size: 149 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE69 SLIDE70 SLIDE70 SLIDE71 https://www.dataknobs.com/generativeai/10-llms Page 39 of 61
----------------------------------------

CHUNK 202
File: unknown
Size: 133 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE71 SLIDE72 https://www.dataknobs.com/generativeai/10-llms Page 40 of 61
----------------------------------------

CHUNK 203
File: unknown
Size: 157 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE72 SLIDE73 SLIDE73 SLIDE74 SLIDE74 https://www.dataknobs.com/generativeai/10-llms Page 41 of 61
----------------------------------------

CHUNK 204
File: unknown
Size: 141 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE75 SLIDE75 SLIDE76 https://www.dataknobs.com/generativeai/10-llms Page 42 of 61
----------------------------------------

CHUNK 205
File: unknown
Size: 149 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE76 SLIDE77 SLIDE77 SLIDE78 https://www.dataknobs.com/generativeai/10-llms Page 43 of 61
----------------------------------------

CHUNK 206
File: unknown
Size: 149 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE78 SLIDE79 SLIDE79 SLIDE80 https://www.dataknobs.com/generativeai/10-llms Page 44 of 61
----------------------------------------

CHUNK 207
File: unknown
Size: 141 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE80 SLIDE81 SLIDE81 https://www.dataknobs.com/generativeai/10-llms Page 45 of 61
----------------------------------------

CHUNK 208
File: unknown
Size: 141 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE82 SLIDE82 SLIDE83 https://www.dataknobs.com/generativeai/10-llms Page 46 of 61
----------------------------------------

CHUNK 209
File: unknown
Size: 141 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE83 SLIDE84 SLIDE84 https://www.dataknobs.com/generativeai/10-llms Page 47 of 61
----------------------------------------

CHUNK 210
File: unknown
Size: 141 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE85 SLIDE85 SLIDE86 https://www.dataknobs.com/generativeai/10-llms Page 48 of 61
----------------------------------------

CHUNK 211
File: unknown
Size: 149 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE86 SLIDE87 SLIDE87 SLIDE88 https://www.dataknobs.com/generativeai/10-llms Page 49 of 61
----------------------------------------

CHUNK 212
File: unknown
Size: 141 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE88 SLIDE89 SLIDE89 https://www.dataknobs.com/generativeai/10-llms Page 50 of 61
----------------------------------------

CHUNK 213
File: unknown
Size: 141 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE90 SLIDE90 SLIDE91 https://www.dataknobs.com/generativeai/10-llms Page 51 of 61
----------------------------------------

CHUNK 214
File: unknown
Size: 149 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE91 SLIDE92 SLIDE92 SLIDE93 https://www.dataknobs.com/generativeai/10-llms Page 52 of 61
----------------------------------------

CHUNK 215
File: unknown
Size: 141 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE93 SLIDE94 SLIDE94 https://www.dataknobs.com/generativeai/10-llms Page 53 of 61
----------------------------------------

CHUNK 216
File: unknown
Size: 673 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SLIDE95 SLIDE95 LARGE LANGUAGE MODELS (LLMS) LLMs are a type of artificial intelligence (AI) capable of processing and generating human-like text in response to a wide range of prompts and questions. Trained on massive datasets of text and code, they can perform various tasks such as: Generating di!erent creative text formats: poems, code, scripts, musical pieces, emails, letters, etc. Answering open ended, challenging, or strange questions in an informative way: drawing on their internal knowledge and understanding of the world. Translating languages: seamlessly converting text from one language to another. 
----------------------------------------

CHUNK 217
File: unknown
Size: 1015 chars
Content:
----------------------------------------
Writing di!erent kinds of creative content: stories, poems, scripts, musical pieces, etc., often indistinguishable from human-written content. RETRIEVAL AUGMENTED GENERATION (RAG) RAG is a novel approach that combines the strengths of LLMs with external knowledge sources. It works by: Retrieval: When given a prompt, RAG searches through an external database of relevant documents to find information related to the query. Augmentation: The retrieved information is then used to enrich the context provided to the LLM. This can be done by incorporating facts, examples, or arguments into the prompt. Generation: Finally, the LLM uses the enhanced context to generate a response that is grounded in factual information and tailored to the specific query. RAG o!ers several advantages over traditional LLM approaches: Improved factual accuracy: By anchoring responses in real-world data, RAG reduces the risk of generating false or misleading information. https://www.dataknobs.com/generativeai/10-llms Page 54 of 61
----------------------------------------

CHUNK 218
File: unknown
Size: 619 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM Greater adaptability: As external knowledge sources are updated, RAG can access the latest information, making it more adaptable to changing circumstances. Transparency: RAG facilitates a clear understanding of the sources used to generate responses, fostering trust and accountability. However, RAG also has its challenges: Data quality: The accuracy and relevance of RAG's outputs depend heavily on the quality of the external knowledge sources. Retrieval e"ciency: Finding the most relevant information from a large database can be computationally expensive. 
----------------------------------------

CHUNK 219
File: unknown
Size: 719 chars
Content:
----------------------------------------
Integration complexity: Combining two di!erent systems (retrieval and generation) introduces additional complexity in terms of design and implementation. PROMPT ENGINEERING Prompt engineering is a crucial technique for guiding LLMs towards generating desired outputs. It involves crafting prompts that: Clearly define the task: Specify what the LLM should do with the provided information. Provide context: Give the LLM enough background knowledge to understand the prompt and generate an appropriate response. Use appropriate language: Frame the prompt in a way that aligns with the LLM's capabilities and training data. ADVANTAGE OF USING RAG Better Accuracy: If factual correctness is crucial, RAG can be fantastic. 
----------------------------------------

CHUNK 220
File: unknown
Size: 1891 chars
Content:
----------------------------------------
It retrieves information from external sources, allowing the AI assistant to double-check its responses and provide well-sourced answers. Domain Knowledge: Imagine an AI assistant for medical diagnosis or legal or up to date tax laws. RAG can access medical databases to enhance its responses and ensure they align with established medical knowledge. Reduce Hallucination: LLMs can sometimes fabricate information, a phenomenon called hallucination in which they make up things. RAG mitigates this risk by grounding the response in retrieved data. Building Trust: By citing sources, RAG fosters trust with users. Users can verify the information and see the reasoning behind the response. DISADVANTAGES OF USING RAG Speed is Crucial: RAG involves retrieving information, which can add a slight delay to the response. If real-time response is essential, a pre- trained LLM might be su"cient. Limited Context: RAG works best when the user's query and context are clear. If the conversation is ambiguous, retrieved information might not be relevant. Privacy Concerns: If the AI assistant deals with sensitive user data, RAG might raise privacy concerns. External retrievals could potentially expose user information. WHEN TO FINETUNE LLM Consider fine-tuning a large language model (LLM) when you want it to perform better at a specific task or adapt to a particular domain. Here are exampple scenarios where fine-tuning is optimal Domain-Specific Nuances: If you need an LLM for financial analysis, legal document or on medical document - finething is better. Example word capital has meaning in finance domain. While a pre-trained LLM might understand language, it won't grasp legal, finance or medical specific terms or jargons. Fine-tuning on finance Q&A, document or legal documents will imrpove the LLM to that specific domain. https://www.dataknobs.com/generativeai/10-llms Page 55 of 61
----------------------------------------

CHUNK 221
File: unknown
Size: 650 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM Instruction Fine-Tuning: This is a recent advancement where you provide the LLM with instructions or demonstrations alongside training data. This can be useful for tasks where you want the LLM to follow a certain style or format, like writing safety instructions in a specific tone. Specialized Tasks: Imagine you want an LLM to write di!erent kinds of creative content, like poems or code. Fine-tuning on a dataset of poems can improve its poetry generation skills, while fine-tuning on code samples can enhance its code writing abilities. However, fine-tuning isn't always the right answer. 
----------------------------------------

CHUNK 222
File: unknown
Size: 2998 chars
Content:
----------------------------------------
Here are examples when you should not do fine tuning. In fact you should first use standard LLM, prompt engineering. Then try RAG. Later consider fine tuning. General Use Cases: If you need a broad LLM for various tasks, a pre-trained model will do better job. Pre-trained models are versatile, trained on diverse data and can handle many tasks well enough without specific fine-tuning. Limited Data: Fine-tuning works well when you have lot of data related to your specific task or domain. If you only add few records which most demo shows - fine-tuning might not be e!ective and could even harm the model's performance. Knowledge Integration: If your goal is to add propietary/latest/specific knowledge to the LLM, retrieval-augmented-generation (RAG)is better approach. In RAG LLM retrieves relevant information from a knowledge base. You can use out of box RAG. You can further optimize it by smmarizing knowledge,, creating embeddings with meta data etc. FINE TUNING STEPS How can LLMs be fine-tuned for summarization? LLMs (Large Language Models) like GPT-3 can be fine-tuned for summarization using the following approaches: Supervised training - The simplest approach is to fine-tune the LLM using a large dataset of text-summary pairs. The model is trained to generate the corresponding summary given the input text. This requires a sizable supervised dataset, which can be expensive to create. Public datasets like CNN/DailyMail can be used. Self-supervised training - The LLM is trained using the original text as input and the first few sentences as the "summary". This creates weak supervision from the data itself. The model is then fine-tuned on a smaller set of human-written summaries to improve accuracy. This approach requires less labeled data. Reinforcement learning - The LLM is first trained autoencoding - to reproduce the input text. Then, rewards are given based on the quality and conciseness of the generated summary. The model learns to generate better summaries through trial-and-error to maximize these rewards. However, this requires defining a good reward function. Filtering and post-processing - Generated summaries from the LLM can be filtered and refined using techniques like: • Extracting sentences with the highest similarity to human references(cid:0)• Removing repetitive sentences(cid:0)• Combining overlapping content into a single sentence, etc. This requires minimal fine-tuning of the base LLM but provides less control over the summary style. Prompting - The LLM can be "prompted" to generate a summary using natural language instructions. For example: In 2-3 short paragraphs, summarize the main points of the following text: This relies more on the pre-trained LLM abilities and requires less labeled data. But accuracy tends to be lower. So in short, there are a variety of approaches to fine-tune LLMs for summarization - from fully supervised to minimally supervised. The choice depends on the available data, required accuracy and custom need. 
----------------------------------------

CHUNK 223
File: unknown
Size: 96 chars
Content:
----------------------------------------
VERIFY LLM AND AI ASSISTANT ANSWERS https://www.dataknobs.com/generativeai/10-llms Page 56 of 61
----------------------------------------

CHUNK 224
File: unknown
Size: 903 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM If you are using AI Assistant, you should cross check facts/number given by AI Assistant Check in Vecor DB: If you are using Vector DB/RAG, you can check what value RAG provide. This will help to ensure that response generated by RAG is in line with value stored in vector DB. Use Second LLM: Other/aditional approach is you can ask a smaller question from second or same LLM and se what answer you get e.g. if there is 1 page of text and it says company Dataknobs has revenue of $78M, you can ask a smaller question "how much revneue Dataknobs has". However you need to consider additional cost of 2nd call? You may have more than one fact and multiple calls may be needed for each fact. Call to Search Engine: You can run a query on search engine programmatically and chec response. However depending on domain this result may or may not work. 
----------------------------------------

CHUNK 225
File: unknown
Size: 1435 chars
Content:
----------------------------------------
It may require parsing result from search engine. HOW TO EVALUATE LLM Method Description Perplexity Perplexity measures how well a language model predicts a sample of text. Lower perplexity indicates better performance. BLEU Score BLEU (Bilingual Evaluation Understudy) Score is commonly used to evaluate the quality of machine-translated text by comparing it to human-generated translations. ROUGE Score ROUGE (Recall-Oriented Understudy for Gisting Evaluation) Score is used to evaluate the quality of summaries produced by a language model by comparing them to reference summaries. Human Human evaluation involves having human judges assess the quality of text generated by the language model based on criteria Evaluation such as fluency, coherence, and relevance. Word Error WER measures the di!erence between the words generated by the language model and the reference text. Lower WER Rate (WER) indicates better performance. 100K-tokens Agenda Ai-assistant-architecture Ai-assistant-building-blocks Ai-assistant-custom-model Ai-assistant-evaluation-metric Ai-assistant-finetune-model Ai- assistant-on-your-data Ai-assistant-tech-stack Ai-assistant-wrapper DDAATTAAKKNNOOBBSS BBLLOOGG OOUURR PPRROODDUUCCTTSS 1100 UUSSEE CCAASSEESS BBUUIILLTT KKRREEAATTEEBBOOTTSS 1100 UUSSEE CCAASSEESS BBUUIILLTT BBYY DDAATTAAKKNNOOBBSS SSEETTUUPP CCHHAATTBBOOTTSS IINN MMIINNUUTTEESS https://www.dataknobs.com/generativeai/10-llms Page 57 of 61
----------------------------------------

CHUNK 226
File: unknown
Size: 6846 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM DDaattaakknnoobbss hhaass ddeevveellooppeedd aa wwiiddee rraannggee ooff pprroodduuccttss aanndd ssoolluuttiioonnss ppoowweerreedd bbyy GGeenneerraattiivvee AAII ((GGeennAAII)),, AAggeenntt AAII,, aanndd PPrree bbuuiilltt ffrroonntt eenndd tthhaatt yyoouu ccaann ccoonnfifigguurree ttrraaddiittiioonnaall AAII ttoo aaddddrreessss ddiivveerrssee iinndduussttrryy nneeeeddss.. TThheessee ssoolluuttiioonnss ssppaann fifinnaannccee,, hheeaalltthhccaarree,, rreeaall eessttaattee,, ee--ccoommmmeerrccee,, PPrree bbuuiilltt AAddmmiinn AApppp ttoo mmaannaaggee cchhaattbboott aanndd mmoorree.. CClliicckk oonn ttoo sseeee iinn--ddeepptthh llooookk aatt tthheessee uussee ccaasseess -- SSttoocckkss EEaarrnniinngg CCaallll AAnnaallyyssiiss,, EEccoommmmeerrccee AAnnaallyyssiiss wwiitthh PPrroommpptt mmaannaaggeemmeenntt UUII GGeennAAII,, FFiinnaanncciiaall PPllaannnneerr AAII AAssssiissttaanntt,, KKrreeaatteebboottss,, KKrreeaattee WWeebbssiitteess,, KKrreeaattee CCMMSS,, TTrraavveell AAggeenntt WWeebbssiittee,, RReeaall EEssttaattee PPeerrssoonnaalliizzaattiioonn aapppp AAggeenntt eettcc.. BBuuiilltt iinn cchhaatt hhiissttoorryy FFeeeeddbbaacckk LLoooopp AAvvaaiillaabbllee oonn -- GGCCPP,,AAzzuurree,,AAWWSS.. AAdddd RRAAGG wwiitthh uussiinngg ffeeww lliinneess ooff CCooddee.. AAII AAGGEENNTT TTUUTTOORRIIAALL AAdddd FFAAQQ ggeenneerraattiioonn ttoo cchhaattbboott AAGGEENNTT AAII TTUUTTOORRIIAALL HHeerree aarree sslliiddeess aanndd AAII AAggeenntt TTuuttoorriiaall.. AAggeennttiicc AAII rreeffeerrss ttoo AAII ssyysstteemmss tthhaatt ccaann aauuttoonnoommoouussllyy ppeerrcceeiivvee,, rreeaassoonn,, aanndd KKRREEAATTEEBBOOTTSS ttaakkee aaccttiioonnss ttoo aacchhiieevvee ssppeecciifificc ggooaallss wwiitthhoouutt ccoonnssttaanntt hhuummaann iinntteerrvveennttiioonn.. TThheessee AAII aaggeennttss uussee tteecchhnniiqquueess lliikkee rreeiinnffoorrcceemmeenntt lleeaarrnniinngg,, ppllaannnniinngg,, aanndd mmeemmoorryy ttoo aaddaapptt aanndd mmaakkee ddeecciissiioonnss iinn ddyynnaammiicc eennvviirroonnmmeennttss.. TThheeyy aarree ccoommmmoonnllyy uusseedd iinn aauuttoommaattiioonn,, rroobboottiiccss,, vviirrttuuaall aassssiissttaannttss,, aanndd ddeecciissiioonn--mmaakkiinngg ssyysstteemmss.. KKRREEAATTEEWWEEBBSSIITTEESS LLLLMM AANNDD AAII BBAASSEEDD WWEEBBSSIITTEE GGEENNEERRAATTIIOONN BBUUIILLDD DDAATTAAPPRROODDUUCCTTSS HHOOWW DDAATTAAKKNNOOBBSS HHEELLPP IINN BBUUIILLDDIINNGG DDAATTAA PPRROODDUUCCTTSS AAII ppoowweerreedd wweebbssiitteess ttoo ddoommaaiinnttee sseeaarrcchh PPrreemmiiuumm HHoossttiinngg -- AAzzuurree,, GGCCPP,,AAWWSS BBuuiillddiinngg ddaattaa pprroodduuccttss uussiinngg GGeenneerraattiivvee AAII ((GGeennAAII)) aanndd AAggeennttiicc AAII eennhhaanncceess aauuttoommaattiioonn,, iinntteelllliiggeennccee,, aanndd AAII wweebb ddeessiiggnneerr aaddaappttaabbiilliittyy iinn ddaattaa--ddrriivveenn aapppplliiccaattiioonnss.. GGeennAAII ccaann ggeenneerraattee ssttrruuccttuurreedd aanndd uunnssttrruuccttuurreedd ddaattaa,, aauuttoommaattee ccoonntteenntt AAggeenntt ttoo ggeenneerraattee wweebbssiittee ccrreeaattiioonn,, eennrriicchh ddaattaasseettss,, aanndd ssyynntthheessiizzee iinnssiigghhttss ffrroomm llaarrggee vvoolluummeess ooff iinnffoorrmmaattiioonn.. TThhiiss hheellppss iinn sscceennaarriiooss ssuucchh aass SSEEOO ppoowweerreedd bbyy LLLLMM aauuttoommaatteedd rreeppoorrtt ggeenneerraattiioonn,, aannoommaallyy ddeetteeccttiioonn,, aanndd pprreeddiiccttiivvee mmooddeelliinngg.. CCoonntteenntt mmaannaaggeemmeenntt ssyysstteemm ffoorr GGeennAAII BBuuyy aass SSaaaass AApppplliiccaattiioonn oorr mmaannaaggeedd sseerrvviicceess AAvvaaiillaabbllee oonn AAzzuurree MMaarrkkeettppllaaccee ttoooo.. AAII AAGGEENNTT FFOORR BBUUSSIINNEESSSS AANNAALLYYSSIISS AANNAALLYYZZEE RREEPPOORRTTSS,, DDAASSHHBBOOAARRDD AANNDD DDEETTEERRMMIINNEE TTOO--DDOO KKRREEAATTEEWWEEBBSSIITTEESS DDaattaaKKnnoobbss hhaass bbuuiilltt aann AAII AAggeenntt ffoorr ssttrruuccttuurreedd ddaattaa aannaallyyssiiss tthhaatt eexxttrraaccttss mmeeaanniinnggffuull iinnssiigghhttss ffrroomm ddiivveerrssee ddaattaasseettss ssuucchh aass ee--ccoommmmeerrccee mmeettrriiccss,, ssaalleess//rreevveennuuee rreeppoorrttss,, aanndd ssppoorrttss ssccoorreeccaarrddss.. TThhee aaggeenntt iinnggeessttss ssttrruuccttuurreedd ddaattaa ffrroomm ssoouurrcceess lliikkee CCSSVV fifilleess,, SSQQLL ddaattaabbaasseess,, aanndd AAPPIIss,, aauuttoommaattiiccaallllyy ddeetteeccttiinngg sscchheemmaass aanndd rreellaattiioonnsshhiippss wwhhiillee KKRREEAATTEE CCMMSS ssttaannddaarrddiizziinngg ffoorrmmaattss.. UUssiinngg ssttaattiissttiiccaall aannaallyyssiiss,, aannoommaallyy ddeetteeccttiioonn,, aanndd AAII--ddrriivveenn ffoorreeccaassttiinngg,, iitt iiddeennttiififieess ttrreennddss,, ccoorrrreellaattiioonnss,, aanndd oouuttlliieerrss,, pprroovviiddiinngg iinnssiigghhttss ssuucchh aass ssaalleess flfluuccttuuaattiioonnss,, rreevveennuuee lleeaakkss,, aanndd ppeerrffoorrmmaannccee mmeettrriiccss.. CCMMSS GGEENNAAII CCMMSS ffoorr GGeennAAII LLiinneeaaggee ffoorr GGeennAAII aanndd HHuummaann ccrreeaatteedd ccoonntteenntt KKRREEAATTEEHHUUBB TTrraacckk GGeennAAII aanndd HHuummaann EEddiitteedd ccoonntteenntt CCRREEAATTEE NNEEWW KKNNOOWWLLEEDDGGEE WWIITTHH PPRROOMMPPTT LLIIBBRRAARRYY TTrraaccee ppaaggeess tthhaatt uussee ccoonntteenntt AAbbiilliittyy ttoo ddeelleettee GGeennAAII ccoonntteenntt AAtt iittss ccoorree,, KKrreeaatteeHHuubb iiss ddeessiiggnneedd ttoo eennaabbllee ccrreeaattiioonn ooff nneeww ddaattaa aanndd tthhee ggeenneerraattiioonn ooff iinnssiigghhttss ffrroomm eexxiissttiinngg ddaattaasseettss.. IItt aaccttss aass aa bbrriiddggee bbeettwweeeenn rraaww ddaattaa aanndd mmeeaanniinnggffuull oouuttccoommeess,, pprroovviiddiinngg tthhee ttoooollss nneecceessssaarryy ffoorr oorrggaanniizzaattiioonnss ttoo eexxppeerriimmeenntt,, aannaallyyzzee,, aanndd ooppttiimmiizzee tthheeiirr ddaattaa pprroocceesssseess.. CCMMSS FFOORR WWEEBBSSIITTEESS AANNDD BBOOTTSS BBUUIILLDD BBUUDDGGEETT PPLLAANN FFOORR GGEENNAAII GGEENNEERRAATTEE SSLLIIDDEESS CCIIOO GGUUIIDDEE TTOO CCRREEAATTEE GGEENNAAII BBUUDDGGEETT FFOORR 22002255 EEFFFFOORRTTLLEESSSS SSLLIIDDEESS CCRREEAATTIIOONN CCIIOOss aanndd CCTTOOss ccaann aappppllyy GGeennAAII iinn IITT SSyysstteemmss.. TThhee gguuiiddee hheerree ddeessccrriibbee sscceennaarriiooss aanndd ssoolluuttiioonnss ffoorr IITT ssyysstteemm,, tteecchh ssttaacckk,, GGeennAAII ccoosstt aanndd hhooww ttoo aallllooccaattee bbuuddggeett.. OOnnccee CCIIOO aanndd CCTTOO ccaann aappppllyy tthhiiss ttoo IITT ssyysstteemm,, iitt ccaann bbee eexxtteennddeedd ffoorr GGiivvee pprroommpptt ttoo ggeenneerraattee sslliiddeess bbuussiinneessss uussee ccaasseess aaccrroossss ccoommppaannyy.. CCoonnvveerrtt sslliiddeess iinnttoo wweebbppaaggeess AAdddd SSEEOO ttoo sslliiddeess wweebbppaaggeess https://www.dataknobs.com/generativeai/10-llms Page 58 of 61
----------------------------------------

CHUNK 227
File: unknown
Size: 3488 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM WWHHAATT IISS KKRREEAATTEE AANNDD KKRREEAATTEEPPRROO GGEENN AAII SSLLIIDDEESS KKRREEAATTEE -- BBRRIINNGG YYOOUURR IIDDEEAASS TTOO LLIIFFEE KKRREEAATTEE eemmppoowweerrss yyoouu ttoo ccrreeaattee tthhiinnggss -- DDaattaasseett,, AArrttiicclleess,, PPrreesseennttaattiioonnss,, PPrrooppoossaallss,, WWeebb ddeessiiggnn,, WWeebbssiitteess aanndd AAII AAssssiissttaannttss KKrreeaattee iiss aa ppllaattffoorrmm iinncclliiddee sseett ooff ttoooollss tthhaatt iiggnniittee yyoouurr ccrreeaattvviiiittyy aanndd rreevvoolluuttiioonniizzee tthhee wwaayy yyoouu wwoorrkk.. KKRReeaatteePPrroo iiss eenntteerrpprriissee vveerrssiioonn.. CCOONNTTEENNTT CCOOMMPPAASSSS YYOOUURR SSTTOORRYY WWRRIITTIINNGG IISS AAUUTTOOMMAATTEEDD GGeenneerraattee aarrttiicclleess WWHHAATT IISS KKOONNTTRROOLLSS GGeenneerraattee iimmaaggeess KKOONNTTRROOLLSS -- AAPPPPLLYY CCRREEAATTVVIITTYY WWIITTHH RREESSPPOONNSSBBIILLIITTYY GGeenneerraattee rreellaatteedd aarrttiicclleess aanndd iimmaaggeess KKOONNTTRROOLLSS eennaabbllee aaddddiinngg gguuaarrddrraaiillss,, lliinneeaaggee,, aauuddiitt ttrraaiillss aanndd ggoovveerrnnaannccee.. KKOOnnttrroollss rreeccooggiizzeess tthhaatt ddii!!eerreenntt uussee GGeett ssuuggggeessttiioonn wwhhaatt ttoo wwrriittee nneexxtt ccaasseess ffoorr GGeenn AAII aanndd AAII hhaavvee vvaarryyiinngg lleevveellss ooff ccoonnttrrooll rreeqquuiirreemmeennttss.. KKoonnttrroollss pprroovviiddee ssttrruuccttuurree ttoo sseelleecctt rriigghhtt ccoonnttrroollss.. CCOONNTTEENNTT CCOOMMPPAASSSS WWHHAATT IISS KKNNOOBBSS KKNNOOBBSS -- EEXXPPEERRIIMMEENNTTAATTIIOONN AANNDD DDIIAAGGNNOOSSTTIICCSS FFRRAACCTTIIOONNAALL CCTTOO FFOORR GGEENNEERRAATTIIVVEE AAII AANNDD DDAATTAA WWeellll ddeefifinneedd ttuunnaabbllee ppaarraammtteerrss ffoorr LLLLMM AAPPII,, LLLLMM fifinnee ttuunniinngg ,, VVeeccttoorr DDBB.. TThheessee ppaarraammeetteerrss eennaabbllee ffaasstteerr PPRROODDUUCCTTSS eexxppeerriimmeennttaattiioonn aanndd ddiiaaggoossiiss ffoorr eevveerryy ssttaattee ooff GGeennAAII ddeevveellooppmmeenntt -- cchhuunnkkiinngg,, eemmbbeeddddiinngg,, uuppsseerrtt iinnttoo vveeccttoorr DDBB,, HHIIRREE EEXXPPEERRTTIISSEE WWIITTHHOOUUTT CCOOMMMMIITTMMEENNTT rreettrriieevveell,, ggeenneerraattiioonn aanndd ccrreeaattiinngg rreessppoonnsseess ffoorr AAII AAssiissttaanntt.. DDeelliivveerr EE22EE uussee ccaassee GGeenneerraattiivvee AAII eexxppeerrttiissee MMaacchhiinnee LLeeaarrnniinngg eexxppeerrttiissee KKRREEAATTEE AARRTTIICCLLEESS DDaattaa pprroodduucctt bbuuiillddiinngg eexxppeerrttiissee CCRREEAATTEE AARRTTIICCLLEESS AANNDD BBLLOOGGSS CClloouudd -- AAWWSS,, GGCCPP,,AAzzuurree CCrreeaattee aarrttiicclleess ffoorr BBllooggss,, WWeebbssiitteess,, SSoocciiaall MMeeddiiaa ppoossttss.. WWrriittee sseett ooff aarrttiicclleess ttooggeetthheerr ssuucchh aass cchhaapptteerrss ooff bbooookk,, oorr ccoommpplleettee bbooookk bbyy ggiivviinngg lliisstt ooff ttooppiiccss aanndd KKrreeaattee wwiillll ggeenneerraattee aallll aarrttiicclleess.. FFRRAACCTTIIOONNAALL CCTTOO KKRREEAATTEE SSLLIIDDEESS CCRREEAATTEE PPRREESSEENNTTAATTIIOONNSS,, PPRROOPPOOSSAALLSS AANNDD PPAAGGEESS DDeessiiggnn iimmppaaccttffuull pprreesseennttaattiioonn bbyy ggiivviinngg pprrmmpptt.. CCoonnvveerrtt yyoouurr tteexxtt aanndd iimmaaggee ccoonntteenntt iinnttoo pprreesseennttaattiioonnss ttoo wwiinn ccuussttoommeerrss.. SSeeaarrcchh iinn yyoouurr kknnoowwlleeddbbee bbaassee ooff pprreesseennttaattiioonnss aanndd ccrreeaattee pprreesseennttaattiioonnss oorr ddii!
----------------------------------------

CHUNK 228
File: unknown
Size: 1007 chars
Content:
----------------------------------------
!eerreenntt iinndduussttrryy.. PPuubblliisshh tthheessee pprreesseennttaattiioonn wwiitthh oonnee cclliicckk.. GGeenneerraattee SSEEOO ffoorr ppuubblliicc pprreesseennttaattiioonnss ttoo iinnddeexx aanndd ggeett ttrraa""cc.. KKRREEAATTEE WWEEBBSSIITTEESS AAGGEENNTT TTOO PPUUBBLLIISSHH YYOOUURR WWEEBBSSIITTEE DDAAIILLYY AAII ppoowweerreedd wweebbssiittee ggeenneerraattiioonn eennggiinnee.. IItt eemmppoowweerr uusseerr ttoo rreeffrreesshh wweebbssiittee ddaaiillyy.. KKrreeaattee WWeebbssiittee AAII aaggeenntt ddooeess wwoorrkk ooff rreeaaddiinngg ccoonneenntt,, wweebbssiittee bbuuiillddeerr,, SSEEOO,, ccrreeaattee lliigghhtt wweeiigghhtt iimmaaggeess,, ccrreeaattee mmeettaa ddaattaa,, ppuubblliisshh wweebbssiittee,, ssuubbmmiitt ttoo sseeaarrcchh eennggiinnee,, ggeenneerraattee ssiitteemmaapp aanndd tteesstt wweebbssiitteess.. KKRREEAATTEE AAII AASSSSIISSTTAANNTTSS BBUUIILLDD AAII AASSSSIISSTTAANNTT IINN LLOOWW CCOODDEE//NNOO CCOODDEE https://www.dataknobs.com/generativeai/10-llms Page 59 of 61
----------------------------------------

CHUNK 229
File: unknown
Size: 3433 chars
Content:
----------------------------------------
LLM Overview Slides | LLM & RAG Guide 21/04/25, 10:50 PM SSeett uupp AAII AAssssiissttaanntt tthhaatt ggiivvee ppeerrssoonniizzeedd rreessppoonnssss ttoo yyoouurr ccuussttoommeerrss iinn mmiinnuutteess.. AAdddd RRAAGG ttoo AAII aassssiissttaanntt wwiitthh mmiinniimmaall ccooddee-- iimmpplleemmeenntt vveeccttoorr DDBB,, ccrreeaattee cchhuunnkkss ttoo ggeett ccoonntteexxttuuaall aannsswweerr ffrroomm yyoouurr kknnoowwlleebbaassee.. BBuuiilldd qquuaalliittyy ddaattaasseett wwiitthh uuss ffoorr fifinnee ttuunniinngg aanndd ttrraaiinniinngg aa ccuussoomm LLLLMM.. CCRREEAATTEE AAII AAGGEENNTT BBUUIILLDD AAII AAGGEENNTTSS -- 55 TTYYPPEESS AAII aaggeenntt iinnddeeppeennddeennttllyy cchhoooosseess tthhee bbeesstt aaccttiioonnss iitt nneeeeddss ttoo ppeerrffoorrmm ttoo aacchhiieevvee tthheeiirr ggooaallss.. AAII aaggeennttss mmaakkee rraattiioonnaall ddeecciissiioonnss bbaasseedd oonn tthheeiirr ppeerrcceeppttiioonnss aanndd ddaattaa ttoo pprroodduuccee ooppttiimmaall ppeerrffoorrmmaannccee aanndd rreessuullttss.. HHeerree aarree ffeeaattuurreess ooff AAII AAggeenntt,, TTyyppeess aanndd DDeessiiggnn ppaatttteerrnnss DDEEVVEELLOOPP DDAATTAA PPRROODDUUCCTTSS WWIITTHH KKRREEAATTEE AANNDD AABB EEXXPPEERRIIMMEENNTT DDEEVVEELLOOPP DDAATTAA PPRROODDUUCCTTSS AANNDD CCHHEECCKK UUSSEERR RREESSPPOONNSSEE TTHHRRUU EEXXPPEERRIIMMEENNTT AAss ppeerr HHBBRR DDaattaa pprroodduucctt rreeqquuiirree vvaalliiddaattiioonn ooff bbootthh 11.. wwhheetthheerr aallggoorriitthhmm wwoorrkk 22.. wwhheetthheerr uusseerr lliikkee iitt.. BBuuiillddeerrss ooff ddaattaa pprroodduucctt nneeeedd ttoo bbaallaannccee bbeettwweeeenn iinnvveessttiinngg iinn ddaattaa--bbuuiillddiinngg aanndd eexxppeerriimmeennttiinngg.. OOuurr pprroodduucctt KKRREEAATTEE ffooccuuss oonn bbuuiillddiinngg ddaattaasseett aanndd aappppss ,, AABBEExxppeerriimmeenntt ffooccuuss oonn aabb tteessttiinngg.. BBootthh aarree ddeessiiggnneedd ttoo mmeeeett ddaattaa pprroodduucctt ddeevveellooppmmeenntt lliiffeeccyyccllee IINNNNOOVVAATTEE WWIITTHH EEXXPPEERRIIMMEENNTTSS EEXXPPEERRIIMMEENNTT FFAASSTTEERR AANNDD CCHHEEAAPPEERR WWIITTHH KKNNOOBBSS IInn ccoommpplleexx pprroobblleemmss yyoouu hhaavvee ttoo rruunn hhuunnddrreeddss ooff eexxppeerriimmeennttss.. PPlluurraalliittyy ooff mmeetthhoodd rreeqquuiirree iinn mmaacchhiinnee lleeaarrnniinngg iiss eexxttrreemmeellyy hhiigghh.. WWiitthh DDaattaakknnoobbss aapppprrooaacchh,, yyoouu ccaann eexxppeerriimmeenntt tthhrruu kknnoobbss.. RRAAGG FFOORR UUNNSSTTRRUUCCTTRREEDD AANNDD SSTTRRUUCCTTRREEDD DDAATTAA RRAAGG UUSSEE CCAASSEESS AANNDD IIMMPPLLEEMMEENNTTAATTIIOONN HHeerree aarree sseevveerraall vvaalluuee pprrooppoossiittiioonnss ffoorr RReettrriieevvaall--AAuuggmmeenntteedd GGeenneerraattiioonn ((RRAAGG)) aaccrroossss ddii!!eerreenntt ccoonntteexxttss:: UUnnssttrruuccttrreedd DDaattaa,, SSttrruuccttrreedd DDaattaa,, GGuuaarrddrraaiillss.. WWHHYY KKNNOOBBSS MMAATTTTEERR KKNNOOBBSS AARREE LLEEVVEERRSS UUSSIINNGG WWHHIICCHH YYOOUU MMAANNAAGGEE OOUUTTPPUUTT SSeeee DDrriivveettrraaiinn aapppppprrooaacchh ffoorr bbuuiillddiinngg ddaattaa pprroodduucctt,, AAII pprroodduucctt.. IItt hhaass 44 sstteeppss aanndd lleevveerrss aarree kkeeyy ttoo ssuucccceessss.. KKnnoobbss aarree aabbssttrraacctt mmeecchhaanniissmm oonn iinnppuutt tthhaatt yyoouu ccaann ccoonnttrrooll.. MMAAIINN PPAAGGEESS RREELLAATTEEDD PPAAGGEESS EEXXPPLLOORREE MMOORREE CCOONNTTAACCTT UUSS https://www.dataknobs.com/generativeai/10-llms Page 60 of 61
----------------------------------------

CHUNK 230
File: unknown
Size: 145 chars
Content:
----------------------------------------
CSEP 517 Natural Language Processing Word Embeddings Luke Zettlemoyer (Slides adapted from Danqi Chen, Greg Durrett, Chris Manning, Dan Jurafsky)
----------------------------------------

CHUNK 231
File: unknown
Size: 309 chars
Content:
----------------------------------------
How to represent words? N-gram language models it is 76 F and P(w ∣ ) It is 76 F and ___. [0.0001, 0.1, 0, 0, 0.002, ..., 0.3, ..., 0] red sunny Text classification ⊺ P(y = 1 ∣ x) = σ(θ w + b) I like this movie. 👍 [0, 1, 0, 0, 0, ..., 1, ..., 1] (1) w [0, 1, 0, 1, 0, ..., 1, ..., 1] I don’t like this movie. 
----------------------------------------

CHUNK 232
File: unknown
Size: 12 chars
Content:
----------------------------------------
👎 w(2) don’t
----------------------------------------

CHUNK 233
File: unknown
Size: 413 chars
Content:
----------------------------------------
Representing words as discrete symbols In traditional NLP, we regard words as discrete symbols: hotel, conference, motel — a localist representation one 1, the rest 0’s Words can be represented by one-hot vectors: hotel = [0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0] motel = [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0] Vector dimension = number of words in vocabulary (e.g., 500,000) Challenge: How to compute similarity of two words?
----------------------------------------

CHUNK 234
File: unknown
Size: 300 chars
Content:
----------------------------------------
Representing words by their context Distributional hypothesis: words that occur in similar contexts tend to have similar meanings J.R.Firth 1957 • “You shall know a word by the company it keeps” • One of the most successful ideas of modern statistical NLP! These context words will represent banking.
----------------------------------------

CHUNK 235
File: unknown
Size: 98 chars
Content:
----------------------------------------
Distributional hypothesis C1: A bottle of ___ is on the table. “tejuino” C2: Everybody likes ___. 
----------------------------------------

CHUNK 236
File: unknown
Size: 65 chars
Content:
----------------------------------------
C3: Don’t have ___ before you drive. C4: We make ___ out of corn.
----------------------------------------

CHUNK 237
File: unknown
Size: 129 chars
Content:
----------------------------------------
Distributional hypothesis C1 C2 C3 C4 tejuino 1 1 1 1 C1: A bottle of ___ is on the table. loud 0 0 0 0 C2: Everybody likes ___. 
----------------------------------------

CHUNK 238
File: unknown
Size: 199 chars
Content:
----------------------------------------
motor-oil 1 0 0 0 C3: Don’t have ___ before you drive. tortillas 0 1 0 1 choices 0 1 0 0 C4: We make ___ out of corn. wine 1 1 1 0 “words that occur in similar contexts tend to have similar meanings”
----------------------------------------

CHUNK 239
File: unknown
Size: 259 chars
Content:
----------------------------------------
Words as vectors • We’ll build a new model of meaning focusing on similarity • Each word is a vector • Similar words are “nearby in space” • A first solution: we can just use context vectors to represent the meaning of words! • word-word co-occurrence matrix:
----------------------------------------

CHUNK 240
File: unknown
Size: 4692 chars
Content:
----------------------------------------
Words as vectors u v cos(u, v) = · u v k kk k <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""LLLLwwwwUUUUwwwwXXXX9999BBBBKKKK88880000YYYY7777mmmmxxxx5555uuuullllvvvvUUUUssssqqqqXXXXbbbb00008888AAAA0000===="""">>>>AAAAAAAAAAAACCCCSSSSHHHHiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqEEEEssss3333gggg0000WWWWooooIIIICCCCUUUURRRRQQQQTTTTddddCCCC0000YYYY3333LLLLCCCCvvvvYYYYBBBBTTTTQQQQiiiiTTTT6666aaaaQQQQddddOOOOssssmmmmEEEEmmmmUUUUmmmmhhhhppppPPPPkkkk8888NNNNyyyy7777dddd++++QQQQ1111uuuuXXXXCCCCjjjjiiiizzzzkkkkllllbbbbqqqqbbbbYYYYeeeeGGGGDDDDjjjj3333nnnnHHHHuuuu5555dddd44444444ffffMMMMyyyyqqqqVVVVZZZZbbbb0000YYYYhhhhZZZZXXXXVVVVttttffffWWWWNNNN4444mmmmZZZZppppaaaa3333ttttnnnndddd8888////ccccPPPP2222hhhhKKKKnnnngggghhhhMMMMGGGGppppggggzzzzLLLLttttoooo++++kkkkooooTTTTRRRRiiiiDDDDQQQQUUUUVVVVYYYYyyyy0000YYYY0000FFFFQQQQ6666DDDDPPPPSSSS8888ggggeeee3333uuuudddd8888aaaaEEEEiiiiEEEEppppjjjjxxxx7777UUUUKKKKCCCCZZZZuuuuiiiiHHHHooooRRRRDDDDSSSShhhhGGGGSSSSkkkkuuuueeee6666WWWWEEEEuuuuKKKK00006666IIIIVVVVNNNN8888PPPP0000iiiiQQQQ7777ggggzzzz99998888mmmmJJJJ3333CCCCaaaa++++ggggEEEEAAAAuuuuFFFF00007777kkkkMMMMHHHHdddd7777nnnn66661111ZZZZSSSSllllzzzznnnnhhhhuuuuOOOO2222MMMM4444LLLL4444eeee6666zzzzDDDDyyyyzzzzbbbbFFFFWWWWttttCCCCeeeeAAAAyyyyssssWWWWeeeekkkkDDDDGGGGaaaaooooeeee++++aaaazzzz0000++++UUUU4444CCCCUUUUmmmmkkkkMMMMEEEENNNNSSSSddddmmmmwwwwrrrrVVVVmmmm6666KKKKhhhhKKKKKKKKYYYYkkkkaaaazzzzkkkkJJJJJJJJLLLLEEEECCCCAAAA9999QQQQjjjj3333QQQQ0000jjjjVVVVBBBBIIIIppppJJJJttttOOOOggggssssjjjjggggiiiiVVVVaaaa6666MMMMOOOOBBBBCCCCvvvv0000jjjjBBBBiiiiffffpppp7777IIIIkkkkWWWWhhhhllllKKKKPPPPQQQQ1111555533335555kkkkXXXXLLLLRRRRyyyy8888XXXX////vvvvEEEE6666iiiiggggiiiissss3333ppppVVVVGGGGccccKKKKBBBBLLLLhhhh6666aaaaIIIIggggYYYYVVVVBBBBxxxxmmmmKKKKccccKKKKuuuu1111QQQQQQQQrrrrNNNNhhhhIIIIEEEE4444QQQQFFFF1111bbbbddddCCCC3333EEEEcccc6666OOOO6666WWWWzzzzLLLL++++kkkkQQQQ7777MMMMUUUUvvvvLLLL5555PPPPmmmmeeeeddddWWWW2222qqqqvvvvbbbb9999RRRRbbbbllll2222MMMM4444uuuujjjjCCCCIIII7777AAAAMMMMaaaaggggAAAAGGGG1111yyyyCCCCGGGGrrrrggggDDDDddddddddAAAAAAAAGGGGDDDDyyyyCCCCVVVV////AAAAOOOOPPPPoooowwwwnnnn4444888833334444NNNNLLLL6666mmmmrrrrQQQQVVVVjjjjNNNNnnnnMMMMIIII////qqqqBBBBQQQQ++++AAAAbbbbQQQQvvvvrrrrTTTTUUUU<<<<////llllaaaatttteeeexxxxiiiitttt>>>> V u v i i i=1 cos(u, v) = V V 2 2 Pu v i=1 i i=1 i <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""EEEExxxx11114444HHHHCCCCNNNNmmmmllllwwwweeeeaaaattttqqqqmmmm////BBBBBBBBddddVVVVuuuuIIIIIIIIyyyyGGGGZZZZIIII===="""">>>>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==<<<<////llllaaaatttteeeexxxxiiiitttt>>>> q q P P What is the range of ? cos( ⋅ )
----------------------------------------

CHUNK 241
File: unknown
Size: 215 chars
Content:
----------------------------------------
Words as vectors Problem: not all counts are equal, words can randomly co-occur • Solution: re-weight by how likely it is for the two words to co-occur by simple chance • PPMI = Positive Pointwise Mutual Information
----------------------------------------

CHUNK 242
File: unknown
Size: 307 chars
Content:
----------------------------------------
Sparse vs dense vectors • Still, the vectors we get from word-word occurrence matrix are sparse (most are 0’s) & long (vocabulary size) • Alternative: we want to represent words as short (50-300 dimensional) & dense (real-valued) vectors • The focus of this lecture • The basis of all the modern NLP systems
----------------------------------------

CHUNK 243
File: unknown
Size: 2576 chars
Content:
----------------------------------------
Dense vectors 0.286 0.792 0 1 0.177   B 0.107C B  C employees = B 10.109 C B C B 0.542C B  C B 0.349 C B C B 0.271 C B C B 0.487 C B C <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""rrrrllllyyyyVVVV9999zzzz4444BBBBFFFFddddXXXXNNNN5555AAAATTTTSSSSBBBBwwwwwwwwvvvveeee44448888tttt4444VVVVssss===="""">>>>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==<<<<////llllaaaatttteeeexxxxiiiitttt>>>> @ A
----------------------------------------

CHUNK 244
File: unknown
Size: 373 chars
Content:
----------------------------------------
Why dense vectors? • Short vectors are easier to use as features in ML systems • Dense vectors may generalize better than storing explicit counts • They do better at capturing synonymy • co-occurs with “car”, co-occurs with “automobile” w w 1 2 • Different methods for getting dense vectors: • Singular value decomposition (SVD) • word2vec and friends: “learn” the vectors!
----------------------------------------

CHUNK 245
File: unknown
Size: 119 chars
Content:
----------------------------------------
Word2vec and friends (Mikolov et al, 2013): Distributed Representations of Words and Phrases and their Compositionality
----------------------------------------

CHUNK 246
File: unknown
Size: 9913 chars
Content:
----------------------------------------
Word2vec • Input: a large text corpora, V, d 0.124 0.224 • V: a pre-defined vocabulary     0.430 0.130 • d: dimension of word vectors (e.g. 300) v cat = 0 1 v dog = 0 0.2001 0.290     • B 0.329 C Text corpora: B 0.276 C B C B C <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""7uuQAVVW8vvAMmmMqVVE6kkH3OOLLNNqMvvCToooM77EcZZr+ggtlSSE3wwmUrraeHH6NllWczziPjj/16670BB7h++71jjuaPPL/NNqYQQM===="""">>>>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<Ppp/MDDlX//aG44tHPPeG33xq++i4AAtA55>=WW=BB<qq///l00a==t<<e//xlliaattt>eexxiitt>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""ZXXyS77U1JJh1OOktbbD+iilSHHYAYYwTNNUcXXUIwwEQbboYssQaII+aSS3JLLM4OOeVmmiZkkaujjCEXXkjbbTXSSTjssYzJJ50ww/YssM===="""">>>>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<Pkk/sHHlPeeaf88t3yyeeuuxqUUibTTtQqq>=yy=77<<<///lllaaattteeexxxiiittt>>> @ A • @ A Wikipedia + Gigaword 5: 6B • Twitter: 27B 0.234 0.290 0.266 0.441 • Common Crawl: 840B v = 0 1 v = 0  1 the language 0.239 0.762 B 0.199C B 0.982 C • d B C Output: f : V R B  C <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""bhhQ4ZZLxMM8ciiA+77DOuurkSS7pxxt2ddRpZZ93HHsf//rvuukcZZ1/AAW1hhp+++qazzsodd+w//Q+WWUH88hYRRsTRRdG00qjZZRAMM4===="""">>>>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<K++/fQQlg//a3eet3YYe899x6PPiSttta11>zmm9uu<GG/ggl==a==t<<e//xlliaattt>eexxiitt>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""okkndCCKY++GG++HtZZR+ooyseeDyRRYjEEypaakattkXrroy77Hhvvfz55jB77gREE52ggUlIIdHooy033tqXXxerrCQJJi+SSavssVMssE===="""">>>>AAAAAAAAAAAACCCCOOOOHHHHiiiiccccbbbbZZZZBDDBBPPNSaaSxttytttNBAABFEEEMMMIfZZZfXX7WSS/tddHussbm448a66i1pput887j44ReyyS6bb7yEENH99BoLL8SAAGJ00K4FFYaXXUd22VjooEVkkDoOOCgzzsYggJUxxeBJJ9CGGq/DDbeIICtppRGbboBeeVU66MyUUCIDDDYss2wBBdOyyS355tljjKJVVkBeepmmm2dwwfnvvolWWr5aagm33k3EEJk77wrss/DjjwsYYsZCCL+DD/i224nDDM6nnbcyy8WaatPXXe0HHPVvvLvssipIIIxuuVYYY3MVV+ieeBXeePvkkX0ggEEppQnvvvURR12ZZ5C66oW77eP88HUooiP22rAppizXXq/XX5+6667wwwzcc03KKQmPPKvbbgX225+bb7cYY3Kmm1WSS5k99mpMMaCppnPDDp5DDm4oodceem6XX1/ff9mOOYX11d7vvJ9aae5jjWWxxV3001v99bn22XLSSK7rruzvvs/uub800lH22yGffZxPPOvXXNv++YTyycpVVGx99jKww2a++W566sEJJrdkk0g44NS11mqhhQUwwAr66oNPPFWZZDcaaRwyyQtvvoKQQ4qmmTmZZrxAARRCCwZggKIUUJUddQnFFwmCClUjjUGhh4eKK+RttFIHHXrAAUPoorIll43DDaPCCg9ZZj6XXYvhhj699V6UUHQddxUQQwavvnKpp0166INCCp9NNYTiiTONN4MVVmMHHuOnn4wCCAkffyfQQtajj1N99amhh6XYYcgiiDpZZdOHHtzggZuDDgvKKDX00DD11CiKKD2LLP4++uRffQEDD5Yrry6IIdoAAuoYYECYYEGYYJWZZP5TTqaiiC4DDyfPPJxzzGT99Gi00oQggxuhhysLLlgFF2SQQvTWWtkRRraIIuOxx3S11TjGG49KKOoWWgruugG11I966O811DZVVE1jjvF++bUmmqwQQBeVVebBBxmAA4DooND77baGG9aGGeFeedXrrwwOONLAAQGttnm11bG77fzVV+6bbdUzzqebbXoddqeAA1wNNb/TTy9wwJ3TT6f//Fp++fKggw0ffSAOO6gRRimVVSYvvUsaau8XXfhootnQQyM//lHDD3KXXQ3ccivNNXLRRkP66aittgAQQUHggIAqquUvvmbzzTf44F+vvNO223+oo0qPPulyywIbblkTTT9CCGQMMNkeeg1RRkDqqvcBBI2QQ3niiSY66AYZZ1ZMMkNTTDQ33Apff+uSSYC77DECC1pffoFMMWJYYlZ22Q+CCslSSAF88tvjjPMddKuIIJDDDojWWfnQQnAMMd2HHMw77s7NN61xxHDttdxCCqBzzN2qqtyFFXkgg0mEEKhpp65ppcd88Rstt91DDPT885k//G9ppx1GGyk++J+sshNMMxe66F5SSNrjjpYWWOx99uHii10mm/8kkfUSSff//KDff4EvvV2ii5nYYnExxeSFF1uxxZ0sso6yyr3jjd300o9HH1AbbY+aamr//VVSSJXZZImmmiSssK711PVaa72YY6T//Uv66T3vvet11VTUUFihhGFyyN1ddal99phjjENOOgqhh7MkkQfhhg2RRNoBBHn88OyddbtVVbGHHAKooua11BtRRZSSS2ZjjVDGG81mmrpRR7UIITJhhDA00OaKKOODDN+RRmDzzvCllXS33hLAAucLLBrjj/EWWP0ttvNhhkuddruKKXCZZOC887XwwWtzzfeTTJ9jjvCaagCrrhJFFV+00cebb9/ggPBrrSx997O55j188WhkkCuPPChoobS115/qqCCqqfH++ZSTTJ2ffjMCC4HDD5pTTJqeeKrOOfBMMkFrrN/FFzgQQkKiinaLLDx88cDllLCrrJFUUDuiifzEElB++HAOOHRSSsxEEhBtt/C885woo9T66a8005gSSdrYYx9dd6wwwdDccpTkk9fOOfe++WpkkKXHHafvvelyyc3zz+XffUpnnE3kk+0ffy9HHHYWWl5+++bOOAzzzUb99jzWWqGrrqfVVO6vvoTOO=dee</uu/8aalPQQaL//tjCCe6PPxqnniU55tQ22>=99=DD<FF/aalyyafft<<e//xlliaattt>eexxiitt>> @ A @ A ! 
----------------------------------------

CHUNK 247
File: unknown
Size: 2165 chars
Content:
----------------------------------------
<<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""vvvv4444LLLLUUUU3333ffffRRRRnnnnmmmmQQQQUUUUVVVVAAAAppppJJJJPPPPSSSSDDDDxxxxJJJJssssttttTTTTKKKKuuuueeeeIIII===="""">>>>AAAAAAAAAAAACCCCBBBBnnnniiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqEEEEssssRRRRBBBBoooovvvvggggqqqqiiiiQQQQiiiiKKKKKKKK6666KKKKbbbbllllxxxxWWWWssssQQQQ9999ooooYYYYppppllllMMMMJJJJuuuu3333QQQQyyyyUUUUyyyyYYYYmmmmSSSSggggllllddddOOOOXXXXGGGGXXXX3333HHHHjjjjQQQQhhhhGGGG3333ffffooooMMMM7777////8888ZZZZJJJJmmmm4444WWWW2222HHHHrrrrhhhhwwwwOOOOOOOOddddeeee7777rrrr0000nnnnSSSSBBBBhhhhVVVV2222nnnnGGGG++++rrrrddddLLLLCCCC4444ttttLLLLyyyySSSSnnnnmmmm1111ssssrrrraaaa++++ssssbbbbllllllllbbbb++++++++0000llllEEEEggggllllJJJJkkkk0000ssssmmmmJJJJCCCCddddAAAACCCCnnnnCCCCKKKKCCCCddddNNNNTTTTTTTTUUUUjjjjnnnnUUUUQQQQSSSSFFFFAAAAeeeeMMMMttttIIIIPPPPhhhhZZZZeeee6666333377774444llllUUUUVVVVPPPPBBBBbbbbPPPPUUUUqqqqIIIIHHHH6666MMMM++++ppppxxxxHHHHFFFFSSSSBBBBuuuuppppZZZZ++++9999HHHH55557777AAAAFFFFPPPPUUUUnnnn7777AAAA44442222kkkkFFFFAAAA////QQQQiiii5555EEEEeeeeBBBBEEEEFFFF2222MMMM77774444LLLLeeee3333bbbbVVVVqqqqTTTTkkkkTTTTwwwwHHHHnnnniiiiFFFFqqqqQQQQKKKKCCCCjjjjRRRR66669999ppppccccXXXXCCCCppppzzzzGGGGhhhhGGGGvvvvMMMMkkkkFFFFJJJJdddd11110000mmmm0000nnnnyyyyGGGGppppKKKKWWWWZZZZkkkkXXXXPPPPFFFFSSSSRRRRRRRRKKKKEEEEhhhh6666hhhhPPPPuuuuooooZZZZyyyyFFFFBBBBPPPPllllZZZZ5555MMMM3333xxxxvvvvDDDDQQQQKKKKCCCCGGGGMMMMhhhhDDDDTTTTFFFFNNNNZZZZyyyyoooovvvvyyyyccccyyyyFFFFCCCCssss1111iiiiggggPPPPTTTTmmmmdddd++++ooooZZZZrrrr1111cccc////MMMM////rrrrppppjjjjoooo66668888zzzzPPPPKKKKkkkk1111QQQQTTTTjjjjqqqqeeeeLLLLooooppppRRRRBBBBLLLLWWWWCCCCeeeeCCCCQQQQyyyyppppJJJJFFFFiiiizzzzkkkkSSSSEEEEIIIISSSS2222ppppuuuuhhhhXXXXiiiiAAAAJJJJMMMMLLLLaaaaJJJJFFFFccccxxxxIIIIbbbbiiiizzzzLLLL8888++++TTTT1111nnnnHHHHNNNNddddWWWWrrrruuuu9999UUUUmmmm1111ffffllllHHHHEEEEUUUUQQQQZZZZ77774444AAAAAAAAccccAAAARRRReeeeccccggggjjjjqqqq4444AAAAgggg3333QQQQBBBBBBBBgggg8888ggggmmmmffffwwwwCCCCtttt6666ssssJJJJ++++vvvvFFFFeeeerrrrcccc++++ppppqqqq0000llllqqqq5555jjjjZZZZBBBBXXXX9999ggggffffffff4444AAAAqqqqoooouuuuYYYYnnnngggg========<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
----------------------------------------

CHUNK 248
File: unknown
Size: 24 chars
Content:
----------------------------------------
Word2vec word = “sweden”
----------------------------------------

CHUNK 249
File: unknown
Size: 50 chars
Content:
----------------------------------------
Word2vec Continuous Bag of Words (CBOW) Skip-grams
----------------------------------------

CHUNK 250
File: unknown
Size: 110 chars
Content:
----------------------------------------
Skip-gram • The idea: we want to use words to predict their context words • Context: a fixed window of size 2m
----------------------------------------

CHUNK 251
File: unknown
Size: 9 chars
Content:
----------------------------------------
Skip-gram
----------------------------------------

CHUNK 252
File: unknown
Size: 5271 chars
Content:
----------------------------------------
Skip-gram: objective function • For each position , predict context words within t = 1,2,...T context size m, given center word : w j all the parameters to be optimized T (✓) = P (w w ; ✓) t+j t L | t=1 m j m,j=0 Y Y     6 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""eeeeVVVVYYYY3333kkkk2222hhhh9999ooooFFFFNNNNiiii4444RRRRVVVVzzzzOOOO++++rrrrddddBBBBhhhhuuuuffffttttGGGGssss===="""">>>>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======<<<<////llllaaaatttteeeexxxxiiiitttt>>>> • The objective function is the (average) negative log likelihood: J(θ) T 1 1 J(✓) = log (✓) = log P (w w ; ✓) t+j t   T L   T | t=1 m j m,j=0 X X     6 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""22223333uuuuttttKKKKwwwwnnnn7777ZZZZJJJJEEEE6666uuuurrrrppppMMMMOOOOKKKKPPPPMMMMccccwwww5555eeeeqqqqOOOOkkkk===="""">>>>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<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
----------------------------------------

CHUNK 253
File: unknown
Size: 14606 chars
Content:
----------------------------------------
How to define ? P(w ∣ w ; θ) t+j t • We have two sets of vectors for each word in the vocabulary d u R : embedding for target word i i 2 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""QQQQssssggggoooo7777bbbbHHHHXXXXmmmmddddtttt////5555AAAAAAAAiiiioooowwwwyyyyqqqqkkkkJJJJ////9999EEEE++++0000===="""">>>>AAAAAAAAAAAACCCCBBBBnnnniiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEEJJJJ3333UUUUrrrr1111qqqq////oooohhhh5555FFFFWWWWCCCCyyyyCCCCpppp5555KKKKIIIIooooMMMMeeeeiiiiFFFF44449999VVVV7777AAAAeeee0000MMMMWWWWyyyy2222mmmm3333bbbbppppZZZZhhhhNNNN2222NNNN0000IIIIJJJJOOOOXXXXnnnnxxxxrrrr3333jjjjxxxxooooIIIIhhhhXXXXffff4444MMMM3333////44442222bbbbttttggggddddttttffffTTTTDDDDwwwweeeeGGGG++++GGGGmmmmXXXXllllBBBBwwwwppppnnnnSSSSjjjjvvvvNNNNttttllllZZZZaaaaWWWWVVVV1111bbbbXXXXyyyyuuuuuuuuVVVVjjjjcccc2222tttt7777RRRR11117777dddd6666++++llll4444llllQQQQSSSS2222iiiiQQQQxxxxjjjj2222UUUUnnnnwwwwIIIIppppyyyyJJJJmmmmhhhhTTTTMMMM88881111ppppJJJJ5555EEEEUUUURRRRwwwwGGGGnnnn7777WWWWBBBB0000VVVVffffjjjjttttBBBByyyyooooVVVViiii8888WWWWddddHHHHiiiiffffUUUUiiii////BBBBAAAAssssJJJJAAAARRRRrrrrIIII3333kkkk22224444eeee9999CCCCOOOOtttthhhhEEEEGGGGZZZZpppp7777jjjjPPPPUUUUYYYYwwwwJJJJNNNNhhhhSSSSCCCC7777zzzzeeee////7777vvvvllll11111111aaaassss4444EEEEaaaaJJJJGGGG4444MMMM1111KKKKFFFFGGGGRRRRqqqq++++////ddddXXXXrrrrxxxxyyyySSSSNNNNqqqqNNNNCCCCEEEEYYYY6666WWWW6666rrrrppppNNNNooooLLLL8888NNNNSSSSMMMM8888JJJJppppXXXXuuuummmmlllliiiiiiiiaaaaYYYYjjjjPPPPCCCCAAAAddddgggg0000VVVVOOOOKKKKLLLLKKKKyyyyyyyyZZZZvvvv5555OOOOjjjjYYYYKKKKHHHH0000UUUUxxxxttttKKKKUUUU0000GGGGiiiiiiii////pppp7777IIIIccccKKKKTTTTUUUUOOOOAAAAppppMMMMZZZZ3333GGGGjjjjmmmmvvvvccccKKKK8888TTTT++++vvvvmmmm++++rrrrwwwwwwwwssssuuuuYYYYSSSSFFFFJJJJNNNNBBBBZZZZkkkkuuuuCCCCllllOOOOOOOOddddIIIIyyyyKKKKTTTTFFFFCCCCffffSSSSUUUUoooo0000HHHHxxxxuuuuCCCCiiiiWWWWTTTTmmmmVVVVkkkkSSSSGGGGWWWWGGGGKKKKiiiiTTTTXXXXIIIIVVVVEEEE4444IIII7777////////IIIIiiiiaaaaZZZZ3333WWWWXXXXKKKKffffmmmm3333ppppxxxxVVVV66665555eeeezzzzOOOOMMMMppppwwwwAAAAEEEEddddwwwwAAAAiiii6666ccccQQQQxxxx2222uuuuooooQQQQFFFFNNNNIIIIPPPPAAAAIIIIzzzz////AAAAKKKKbbbb9999aaaaTTTT9999WWWWKKKK9999WWWWxxxx////TTTT1111ppppIIII1111mmmm9999mmmmHHHHPPPP7777AAAA++++ffffwwwwBBBB1111FFFFZZZZkkkkZZZZ<<<<////llllaaaatttteeeexxxxiiiitttt>>>> d v R : embedding for context word i’ i 0 2 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""jjjjllllnnnnCCCCkkkkKKKKyyyyjjjjEEEEggggmmmmmmmmzzzzyyyyrrrrWWWWVVVVffffCCCCHHHH8888VVVVFFFFvvvvPPPPBBBB4444===="""">>>>AAAAAAAAAAAACCCCCCCCXXXXiiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEEJJJJ33334444WWWWeeeettttXXXX1111KKKKOOOOXXXXxxxxSSSSJJJJ6666KKKKooookkkkIIIIeeeeiiiixxxx66668888VVVVjjjjFFFFffffkkkkAAAAbbbbyyyy2222aaaa7777aaaaZZZZdddduuuuNNNNmmmmFFFF3333UUUUyyyygggghhhhVVVVyyyy////++++FFFFSSSS8888eeeeFFFFPPPPHHHHqqqqPPPP////DDDDmmmmvvvv3333HHHHTTTT5555qqqqCCCCttttDDDDwwwwYYYYeeee777788880000wwwwMMMM8888++++PPPPOOOOVVVVPPPPaaaaccccbbbb6666ttttppppeeeeWWWWVVVV1111bbbbXXXX11110000kkkkZZZZ5555cccc2222tttt7777ZZZZ9999ffffeeee22222222++++qqqqKKKKJJJJGGGGEEEENNNNkkkkjjjjEEEEIIII9999nnnn2222ssssaaaaKKKKccccCCCCddddrrrrQQQQTTTTHHHHPPPPaaaajjjjiiiiXXXXFFFFoooocccc9999ppppyyyyxxxx9999dddd555533335555rrrrTTTTKKKKVVVViiiikkkkbbbbjjjjXXXXkkkk5555hhhh6666IIIIRRRR4444IIIIFFFFjjjjCCCCCCCCttttZZZZFFFF6666NNNNuuuuqqqqGGGGWWWWAAAA////9999IIIIBBBB1111nnnnvvvvZZZZSSSSddddZZZZKKKKjjjjLLLLRRRRKKKKHHHH55556666VVVV333322220000OOOO////ZZZZFFFFaaaaffffqqqqTTTTIIIIEEEEWWWWiiiiVVVVuuuuQQQQCCCChhhhSSSSoooo9999++++yyyyvvvvbbbbjjjj8888iiiiSSSSUUUUiiiiFFFFJJJJhhhhwwwwrrrr1111XXXXGGGGddddWWWWHHHHssssppppllllppppooooRRRRTTTTrrrrNNNNyyyyNNNN1111EEEE0000xxxxmmmmSSSSEEEEBBBB7777RRRRjjjjqqqqMMMMAAAAhhhhVVVVVVVV44446666////SSSSRRRRDDDDxxxx0000bbbbppppooooyyyyCCCCSSSSppppooooRRRRGGGGUUUU////XXXX3333RRRRIIIIppppDDDDppppSSSSaaaahhhhbbbbzzzzrrrrzzzzGGGG9999WWWW8888llll4444vvvv////eeeeZZZZ1111EEEEBBBB5555ddddeeeeyyyykkkkSSSSccccaaaaCCCCrrrrIIIIbbbbFFFFGGGGQQQQccccKKKKQQQQjjjjllllMMMMeeeeCCCC++++kkkkxxxxSSSSoooovvvvnnnnEEEEEEEEEEEEwwwwkkkkMMMM7777cccciiiiMMMMssssQQQQSSSSEEEE22223333CCCCKKKK5555ssssQQQQ3333PPPPmmmmXXXXFFFF0000nnnnzzzzrrrrOOOOoooo6666VVVVffffffff2222vvvvFFFFKKKK7777KKKKuuuuIIIIoooowwwwSSSSEEEEccccwwwwSSSSmmmm4444ccccAAAAEEEE1111uuuuIIIIEEEE6666NNNNIIIIDDDDAAAAIIIIzzzzzzzzDDDDKKKK7777xxxxZZZZTTTT9999aaaaLLLL9999WWWW55559999zzzzFFFFqqqqXXXXrrrrGGGGLLLLmmmmAAAAPPPP7777AAAA++++vvvvwwwwBBBBvvvvMMMMiiiiaaaaVVVVwwww========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> • u v Use inner product to measure how likely word i i i · 0 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""RRRRzzzzTTTTZZZZ0000bbbbVVVVGGGG1111ttttXXXX3333mmmm7777GGGGXXXXeeeessssooooGGGGaaaabbbb////HHHHjjjjRRRRIIII===="""">>>>AAAAAAAAAAAACCCCCCCC3333iiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEENNNN3333UUUUrrrr1111qqqq////oooohhhh66669999LLLLCCCC2222iiiipppp5555KKKKIIIIooooMMMMeeeeiiiiFFFF44448888VVVVbbbbCCCCuuuu0000IIIIWWWWwwww2222mmmm3333bbbbppppZZZZjjjjffffssssbbbbggggoooollll5555OOOO7777FFFFvvvv++++LLLLFFFFggggyyyyJJJJeeee////QQQQPPPPeeee////DDDDdddduuuu2222ggggjjjjaaaa++++mmmmDDDDgggg8888dddd4444MMMMMMMM////OOOOCCCChhhhFFFFGGGGllllHHHHeeeeffffLLLLqqqqqqqqyyyyssssrrrrqqqq1111vvvvVVVVDDDDddddrrrrWWWW9999ssss7777uuuu3333vvvv2222////kkkkFFFFXXXXiiiiVVVVRRRRiiii0000ssssGGGGCCCCCCCCXXXXkkkkffffIIIIEEEEUUUUYYYY5555aaaaSSSSjjjjqqqqWWWWbbbbkkkkPPPPppppEEEEEEEExxxxQQQQEEEEjjjjvvvvWWWWBBBB8888XXXXffffiiii9999CCCCZZZZGGGGKKKKCCCCnnnn6666nnnnppppwwwwnnnnxxxxYYYYjjjjTTTTkkkkNNNNKKKKIIIIYYYYaaaaSSSSPPPP5555ddddnnnn0000QQQQIIIIzzzz0000KKKKooooiiiizzzzNNNNffffQQQQooooHHHHOOOOBBBBQQQQaaaa////kkkkiiiiTTTT3333MMMM////ooooSSSSeeee7777bbbbDDDDaaaaffffppppzzzzAAAACCCCXXXXiiiiVVVVuuuuSSSSBBBBiiiijjjjRRRR9999uuuu3333PPPPQQQQSSSShhhhwwwwGGGGhhhhOOOOuuuuMMMMUUUUNNNNKKKK9999VVVV0000nnnn0000VVVV6666GGGGppppKKKKaaaaYYYYkkkkbbbbwwww2222SSSSBBBBVVVVJJJJEEEEBBBB6666jjjjIIIIeeeekkkkbbbbyyyyllllFFFFMMMMllllJJJJffffNNNNffffssssnnnnhhhhssssVVVVFFFFCCCCGGGGAAAAllllppppiiiimmmmssss4444UUUU33339999PPPPZZZZCCCChhhhWWWWaaaahhhhooooHHHHpppprrrrMMMM4444UUUUyyyy11116666hhhhffffiiiiffff1111000099991111ddddOOOOllllllllllllCCCCeeeeppppJJJJhhhhzzzzPPPPFFFF0000UUUUppppgggg1111rrrrAAAAIIIIhhhhggggYYYYUUUUkkkkmmmmwwwwZZZZllllNNNNDDDDEEEEJJJJbbbbUUUU3333AAAArrrrxxxxCCCCEEEEmmmmEEEEttttYYYYmmmmvvvvZZZZkkkkJJJJwwwwFFFF11119999eeeeJJJJtttt2222zzzzppppuuuussss00003333ddddvvvvzzzzRRRRuuuuuuuuqqqqjjjjKKKKMMMMKKKKjjjjkkkkAAAAddddnnnnAAAAIIIIXXXXXXXXIIIIAAAAWWWWuuuuAAAAFFFFtttt0000AAAAEEEEYYYYPPPPIIIIAAAAnnnn8888AAAAJJJJeeeerrrrUUUUffffrrrr2222XXXXqqqqzzzz3333uuuueeeettttFFFFaaaauuuuccccOOOOQQQQRRRR////YYYYHHHH11118888AAAA6666ZZZZPPPPmmmm2222ssss====<<<<////llllaaaatttteeeexxxxiiiitttt>>>> appears with context word i’, the larger the better “softmax” we learned last time! exp(u v ) w w P (w w ) = t · t+j t+j t | exp(u v ) w k k V t · <<<<<<<<<<<<<lllllllllllllaaaaaaaaaaaaattttttttttttteeeeeeeeeeeeexxxxxxxxxxxxxiiiiiiiiiiiiittttttttttttt ssssssssssssshhhhhhhhhhhhhaaaaaaaaaaaaa1111111111111_____________bbbbbbbbbbbbbaaaaaaaaaaaaassssssssssssseeeeeeeeeeeee66666666666664444444444444============="""""""""""""YYYheeJYYYYYYxxxP++wxxxxxxUUU+zzMUUUUUU1116++v111111xxxL99/xxxxxx444rffs444444JJJUOOOJJJJJJ555frrf555555AAA2oozAAAAAAllldxxallllllDDD3ssSDDDDDDTTTt44RTTTTTT333Z330333333JJJaUUcJJJJJJ///lnnv//////DDDdXXFDDDDDDpppqSSGpppppp+++attL++++++pppQWWspppppp555QJJc555555333v//r333333QQQESSaQQQQQQpppKbbhppppppgggM11sggggggiiiXgg/iiiiii+++y88c++++++UUUwoo0UUUUUU=============""""""""""""">>>>>>>>>>>>>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<884QQQQQQ444///m444444nnnlxxXnnnnnnXXXaxx+XXXXXX+++tzzO++++++eeeejjOeeeeeeOOOxWWeOOOOOOcccijjaccccccaaatIIUaaaaaa000>GGR000000ZZZaaAZZZZZZBBBeeLBBBBBBrrrEETrrrrrrDDDKKwDDDDDDwwwqqjwwwwwwhhh55VhhhhhhVVV333VVVVVVHHHxxOHHHHHHOOOXX+OOOOOO///TTK//////KKKcc6KKKKKK666++R666666YYYLLnYYYYYYnnnTTxnnnnnnxxxBBaxxxxxxKKKvvYKKKKKKccc99PccccccNNNLL/NNNNNN///LLp//////pppRRYppppppeee994eeeeee666CCP666666PPP//IPPPPPPoooPPXooooooTTTrr7TTTTTT444JJ8444444777bb5777777ppp88KppppppPPPnndPPPPPPvvvookvvvvvvkkkVV8kkkkkk999zzm999999mmm++UmmmmmmnnnOOYnnnnnnUUU++RUUUUUURRRvv8RRRRRRyyyGGPyyyyyyNNNff4NNNNNN444CCR444444uuuKK9uuuuuu+++22Q++++++fffgg7ffffff+++bbP+++++++++++G++++++OOOggrOOOOOODDDHHjDDDDDDNNN66WNNNNNNoooqqEoooooo444IIP444444OOOYYvOOOOOOeee//0eeeeeeoooUUCooooooXXXaafXXXXXXeeeHHUeeeeeeooo66RooooooQQQBBzQQQQQQGGGggHGGGGGGKKKNNaKKKKKK00000R000000RRRBB4RRRRRR444BBf444444aaaRRoaaaaaaoooddOoooooo222BBx22222299999q999999ooosshoooooogggBBMggggggqqqTTaqqqqqqaaavvLaaaaaaIIIBBoIIIIIIooo99Loooooouuu++tuuuuuutttAAgttttttggghhOggggggNNN333NNNNNN333AAg333333ggg55cggggggddd33fddddddvvvnngvvvvvvAAA++vAAAAAAlllIItllllll+++KKw++++++hhhggJhhhhhhSSS00/SSSSSS///VVw//////DDDuuYDDDDDDtttXXttttttt22299t222222GGGEEmGGGGGGbbbLLFbbbbbbXXXhhQXXXXXXRRRJJdRRRRRRiii11viiiiii000HHz000000PPPMMFPPPPPPSSSppvSSSSSS///aa2//////QQQ//FQQQQQQXXXBB8XXXXXXwwwwwPwwwwwwggg==Mgggggg///==v//////333<<f333333ggg//zggggggCCCllCCCCCCCAAAaa/AAAAAAccctt4ccccccLLLeeQLLLLLL///xx=//////lllii=llllll<<<tt<<<<<<<///>>///////llllllllllaaaaaaaaaatttttttttteeeeeeeeeexxxxxxxxxxiiiiiiiiiitttttttttt>>>>>>>>>> 2 P ✓ = u , v are all the parameters in this model! 
----------------------------------------

CHUNK 254
File: unknown
Size: 2235 chars
Content:
----------------------------------------
k k {{ } { }} <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""uuuuEEEE6666wwwwEEEEgggg++++ccccbbbbVVVVDDDDNNNNnnnn7777TTTT6666DDDD222277776666YYYYVVVV5555++++NNNN9999kkkk===="""">>>>AAAAAAAAAAAACCCCGGGGHHHHiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqkkkkssss3333gggg0000VVVVwwwwIIIITTTTUUUURRRRQQQQTTTTddddCCCC0000YYYY3333LLLLCCCCvvvvYYYYBBBBTTTTQQQQiiiiTTTT6666aaaaQQQQddddOOOOnnnnkkkkwwwwcccc1111MMMMooooIIIIZZZZ////hhhhxxxxllll9999xxxx44440000IIIIRRRRtttt999933335555NNNN00007777TTTTggggNNNNpppp6666hhhhooooHHHHDDDDOOOOffffddddyyyy7777zzzz1111eeeeLLLLLLLLggggCCCC0000////wwwwyyyySSSSiiiiuuuurrrraaaa++++ssssbbbb5555cccc3333KKKK1111vvvvbbbbOOOO7777llll55551111////6666CCCCttttooookkkkRRRRSSSS1111qqqqKKKKRRRRiiiiGGGGTTTTXXXXIIII4444ooooJJJJHHHHrrrrIIIIWWWWccccBBBBCCCCssssGGGG0000ttttGGGGAAAAkkkk++++wwwwjjjjjjjjeeee6666mmmm////mmmmddddMMMMZZZZOOOOKKKKRRRR++++EEEEjjjjTTTTGGGGLLLLmmmmBBBBGGGGQQQQQQQQccccpppp9999TTTTAAAAllllppppyyyyqqqq++++cccc2222DDDDBBBBkkkkQQQQffffIIIIPPPPttttVVVVLLLL++++AAAAwwwwNNNNDDDDzzzz0000yyyyRRRRzzzzRRRR3333ZZZZ2222hhhhnnnn++++UUUUccccaaaa7777YYYYmmmmVVVVuuuuttttmmmmXXXXUUUUzzzzBBBB11114444mmmmVVVVkkkkFFFFqqqqqqqqEEEEDDDDTTTTrrrrUUUU7777ttttffffkkkkSSSSTTTTggggIIIIVVVVAAAABBBBVVVVGGGGqqqqZZZZ5555kkkkxxxxOOOOCCCCmmmmRRRRwwwwKKKKllllggggWWWWccccVVVVOOOOFFFFIIIIssssJJJJHHHHZZZZEEEEBBBB66662222kkkkaaaakkkkooooAAAAppppJJJJ88880000PPPPyyyy////CCCCJJJJVVVVvvvvrrrrYYYYjjjj6666TTTT++++IIIIeeeeBBBBcccc////dddd2222RRRRkkkkkkkkCCCCppppSSSSeeeeDDDDppppyyyyttttmmmmeeeeaaaattttGGGGbbbbiiiiffff99995555vvvvQQQQTTTT8888aaaayyyyffffllllYYYYZZZZwwwwAAAACCCC++++llll8888kkkkJJJJ8888IIIIDDDDBBBBGGGGeeeeppppYYYYTTTT7777XXXXDDDDIIIIKKKKYYYYqqqqIIIIJJJJooooZZZZLLLLrrrrXXXXTTTTEEEEddddEEEEkkkkkkkkoooo6666CCCCwwwwrrrrOOOOggggRRRRrrrr8888eeeeRRRRllll0000rrrr6666ooooWWWW2222bbbbddddeeeerrrriiiissssNNNNWWWW6666LLLLOOOOMMMMrrrrooooCCCCBBBB2222jjjjUUUU2222SSSShhhhKKKK9999RRRRAAAA99996666iiiiJJJJWWWWooooiiiiiiiiJJJJ////SSSSCCCC3333ttttCCCC77778888WWWWyyyy8888GGGGhhhh////GGGG55557777yyyy0000ZZZZBBBBQQQQ9999hhhh++++ggggPPPPjjjjOOOOkkkk3333uuuueeee6666gggg1111wwww========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Q: Why two sets of vectors? Any issues?
----------------------------------------

CHUNK 255
File: unknown
Size: 2345 chars
Content:
----------------------------------------
How to train the model Calculating all the gradients together! ✓ = u , v k k {{ } { }} <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""uuuuEEEE6666wwwwEEEEgggg++++ccccbbbbVVVVDDDDNNNNnnnn7777TTTT6666DDDD222277776666YYYYVVVV5555++++NNNN9999kkkk===="""">>>>AAAAAAAAAAAACCCCGGGGHHHHiiiiccccbbbbVVVVDDDDLLLLSSSSssssNNNNAAAAFFFFJJJJ3333UUUUVVVV66662222vvvvqqqqkkkkssss3333gggg0000VVVVwwwwIIIITTTTUUUURRRRQQQQTTTTddddCCCC0000YYYY3333LLLLCCCCvvvvYYYYBBBBTTTTQQQQiiiiTTTT6666aaaaQQQQddddOOOOnnnnkkkkwwwwcccc1111MMMMooooIIIIZZZZ////hhhhxxxxllll9999xxxx44440000IIIIRRRRtttt999933335555NNNN00007777TTTTggggNNNNpppp6666hhhhooooHHHHDDDDOOOOffffddddyyyy7777zzzz1111eeeeLLLLLLLLggggCCCC0000////wwwwyyyySSSSiiiiuuuurrrraaaa++++ssssbbbb5555cccc3333KKKK1111vvvvbbbbOOOO7777llll55551111////6666CCCCttttooookkkkRRRRSSSS1111qqqqKKKKRRRRiiiiGGGGTTTTXXXXIIII4444ooooJJJJHHHHrrrrIIIIWWWWccccBBBBCCCCssssGGGG0000ttttGGGGAAAAkkkk++++wwwwjjjjjjjjeeee6666mmmm////mmmmddddMMMMZZZZOOOOKKKKRRRR++++EEEEjjjjTTTTGGGGLLLLmmmmBBBBGGGGQQQQQQQQccccpppp9999TTTTAAAAllllppppyyyyqqqq++++cccc2222DDDDBBBBkkkkQQQQffffIIIIPPPPttttVVVVLLLL++++AAAAwwwwNNNNDDDDzzzz0000yyyyRRRRzzzzRRRR3333ZZZZ2222hhhhnnnn++++UUUUccccaaaa7777YYYYmmmmVVVVuuuuttttmmmmXXXXUUUUzzzzBBBB11114444mmmmVVVVkkkkFFFFqqqqqqqqEEEEDDDDTTTTrrrrUUUU7777ttttffffkkkkSSSSTTTTggggIIIIVVVVAAAABBBBVVVVGGGGqqqqZZZZ5555kkkkxxxxOOOOCCCCmmmmRRRRwwwwKKKKllllggggWWWWccccVVVVOOOOFFFFIIIIssssJJJJHHHHZZZZEEEEBBBB66662222kkkkaaaakkkkooooAAAAppppJJJJ88880000PPPPyyyy////CCCCJJJJVVVVvvvvrrrrYYYYjjjj6666TTTT++++IIIIeeeeBBBBcccc////dddd2222RRRRkkkkkkkkCCCCppppSSSSeeeeDDDDppppyyyyttttmmmmeeeeaaaattttGGGGbbbbiiiiffff99995555vvvvQQQQTTTT8888aaaayyyyffffllllYYYYZZZZwwwwAAAACCCC++++llll8888kkkkJJJJ8888IIIIDDDDBBBBGGGGeeeeppppYYYYTTTT7777XXXXDDDDIIIIKKKKYYYYqqqqIIIIJJJJooooZZZZLLLLrrrrXXXXTTTTEEEEddddEEEEkkkkkkkkoooo6666CCCCwwwwrrrrOOOOggggRRRRrrrr8888eeeeRRRRllll0000rrrr6666ooooWWWW2222bbbbddddeeeerrrriiiissssNNNNWWWW6666LLLLOOOOMMMMrrrrooooCCCCBBBB2222jjjjUUUU2222SSSShhhhKKKK9999RRRRAAAA99996666iiiiJJJJWWWWooooiiiiiiiiJJJJ////SSSSCCCC3333ttttCCCC77778888WWWWyyyy8888GGGGhhhh////GGGG55557777yyyy0000ZZZZBBBBQQQQ9999hhhh++++ggggPPPPjjjjOOOOkkkk3333uuuueeee6666gggg1111wwww========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> T T 1 11 1 J(✓) = logJ((✓✓)) == log (✓) = log P (w w l;o✓g)P (w J(✓w) ;=✓)? 
----------------------------------------

CHUNK 256
File: unknown
Size: 9490 chars
Content:
----------------------------------------
t+j t t+j t ✓   T L   TT L   T | | r <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""ooooFFFFttttCCCCjjjj5555NNNNEEEE4444VVVVIIIIaaaa6666vvvvccccNNNNKKKKQQQQllllllllyyyy3333hhhhbbbbvvvvttttMMMM===="""">>>>AAAAAAAAAAAACCCCAAAA3333iiiiccccbbbbZZZZDDDDLLLLSSSSssssNNNNAAAAFFFFIIIIYYYYnnnnXXXXmmmmuuuu9999VVVVdddd3333ppppJJJJlllliiiiEEEEuuuuiiiimmmmJJJJCCCCLLLLooooRRRRiiii22227777EEEEVVVVQQQQVVVV7777ggggSSSSaaaaUUUUkkkk++++mmmmkkkkHHHHTTTTqqqqZZZZhhhhJJJJkkkkTTTTooooYYYYSSSSCCCCGGGG1111////FFFFjjjjQQQQttttFFFF3333PPPPooooSSSS7777nnnnwwwwbbbbpppp5555eeeeFFFFttttvvvv4444wwww8888PPPPGGGGffffcccczzzzhhhhzzzz////iiiiAAAARRRRXXXXKKKKPPPPjjjjffffFFFFssssLLLLiiii0000vvvvLLLLKKKK6666uuuu5555ttttffffzzzz6666xxxxuuuubbbbWWWWddddmmmmFFFFnnnntttt66667777jjjjVVVVFFFFFFFFWWWWoooo7777GGGGIIIIVVVVTTTTMMMMAAAAzzzzQQQQSSSSXXXXrrrrIIIIYYYYccccBBBBWWWWssssmmmmiiiikkkkEEEEUUUUCCCCNNNNYYYYIIII++++tttteeeejjjjeeeeuuuuOOOOBBBBKKKKcccc1111jjjjeeeeYYYY++++DDDDhhhhPPPPkkkkRRRRddddCCCCUUUUPPPPOOOOQQQQUUUU0000VVVVrrrruuuuwwww77770000kkkkIIIIBBBBLLLLQQQQzzzzDDDD3333ssssMMMMYYYYXXXXhhhhbbbbmmmmssssDDDDxxxxxxxxWWWWWWWW7777UUUUHHHHTTTTKKKKzzzzlllljjjj2222PPPPLLLLhhhhTTTTKKKKJJJJKKKKppppqqqquuuu3333CCCCllll9999eeeeJJJJaaaaRRRRooooxxxxiiiiVVVVSSSSAAAA1111iiii3333XXXXSSSSddddDDDDPPPPQQQQCCCCGGGGnnnngggggggg3333zzzzXXXXqqqqppppZZZZAAAArrrrQQQQPPPPXXXXddddYYYYyyyyKKKKCCCCFFFFiiii2222ssss////GGGGNNNNwwwwzzzzttttIIII++++NNNN00007777DDDDBBBBWWWW5555kkkkmmmm0000xxxx++++7777vvvviiiiQQQQwwwwiiiirrrrQQQQddddRRRRYYYYDDDDoooojjjjwwwwJJJJ6666eeeerrrrYYYY3333MMMM////2222qqqqttttFFFFMMMMNNNNzzzzPPPP++++MMMMyyyySSSSZZZZFFFFJJJJOOOOllllkkkkUUUUppppssssLLLLGGGG2222BBBB4444FFFFYYYYnnnneeee4444YYYYhhhhTTTTFFFFwwwwAAAABBBBQQQQxxxxcccc1111ffffbbbbddddooooDDDDBBBBRRRRRRRRNNNNbbbbHHHHkkkkTTTTggggjjjjtttt77778888jjjjzzzzUUUUTTTT8888qqqquuuuUUUU3333bbbbvvvvTTTToooouuuuVVVVqqqq2222kkkkccccOOOOXXXXJJJJAAAADDDDkkkkmmmmJJJJuuuuOOOOSSSSMMMMVVVVMMMMggggNNNNqqqqZZZZIIIIaaaaooooeeeeSSSSRRRRPPPPJJJJNNNNXXXX8888mmmmYYYY9999WWWWSSSS////WWWWuuuu////UUUUxxxxaaaaVVVV2222wwwwppppjjjjNNNN77775555IIII++++sssszzzzxxxx9999vvvvaaaappppddddbbbb<<<<////llllaaaatttteeeexxxxiiiitttt>>>> t=1 m j m,j=0t=1 m j m,j=0 X X X X     6     6 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""22223333uuuuttttKKKKwwwwnnnn7777ZZZZJJJJEEEE6666uuuurrrrppppMMMMOOOOKKKKPPPPMMMMccccwwww5555eeeeqqqqOOOOkkkk===="""">>>>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<<<<////llllaaaatttteeeexxxxiiiitttt>>>> <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""22223333uuuuttttKKKKwwwwnnnn7777ZZZZJJJJEEEE6666uuuurrrrppppMMMMOOOOKKKKPPPPMMMMccccwwww5555eeeeqqqqOOOOkkkk===="""">>>>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<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Q: How many parameters are in total? We can apply stochastic gradient descent (SGD)! (t+1) (t) ✓ = ✓ ⌘ J (✓) ✓   r <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""2222xxxxbbbbrrrrEEEEJJJJRRRR++++XXXXVVVVhhhhUUUUccccyyyyssssjjjjVVVVyyyyGGGGPPPPSSSSHHHHiiiicccc0000HHHHYYYY===="""">>>>AAAAAAAAAAAACCCCJJJJnnnniiiiccccbbbbVVVVDDDDLLLLSSSSggggNNNNBBBBEEEEJJJJzzzz1111GGGGeeeeMMMMrrrr6666ttttHHHHLLLLYYYYBBBBAAAAiiiiYYYYttttggggVVVVQQQQSSSS++++CCCC6666EEEEUUUU8888RRRRTTTTBBBBRRRRyyyyKKKK5555LLLL77772222RRRRiiiihhhhsssszzzzOOOOLLLLjjjjOOOO9999QQQQlllljjjjyyyyNNNNVVVV77778888FFFFSSSS8888eeeeIIIIiiiiLLLLeeee////BBBBQQQQnnnnDDDD////BBBBZZZZMMMMFFFFBBBBdddd1111UUUU1111PPPPVVVV5555RRRRKKKKYYYYddddBBBB1111333355552222pppp6666ZZZZnnnnZZZZuuuuffffnnnnCCCCQQQQnnnnFFFFxxxxaaaaXXXXllllllllttttbbbbSSSS22223333jjjjBBBBJJJJpppphhhhmmmmvvvvssss0000QQQQmmmm++++iiiiYYYYCCCCwwww6666VVVVQQQQvvvvIIII4444CCCCJJJJbbbb9999JJJJNNNNYYYYcccc4444kkkkvvvvwwww66666666pppp4444NNNN////eeeetttt7777rrrroooo1111IIII1111BBBBXXXX2222UUUUhhhh7777EEEEccccKKKKddddEEEEWWWWzzzzBBBBAAAAKKKK4444WWWWllllYYYYxxxx88887777HHHHOOOOEEEE2222rrrr++++CCCCuuuutttt9999OOOOnnnnxxxx////RRRRLLLLssssOOOOUUUUeeee9999WWWW1111BBBBffffQQQQWWWWRRRRhhhhDDDDAAAAffffeeee////2222LLLLyyyyppppjjjjsssshhhhKKKKWWWWyyyyWWWW3333VVVVHHHHooooHHHH++++JJJJNNNNyyyyFFFFllllMMMMkkkkEEEEttttLLLLAAAA33338888VVVVssssKKKKyyyymmmmCCCCttttkkkkEEEEooooxxxxppppeeeemmmm6666KKKKQQQQQQQQ4444aaaaBBBBZZZZOOOO8888XXXX////QQQQzzzzwwww1111NNNNggggXXXXbbbbjjjjjjjjTTTTUUUUssssVVVVxxxxNNNNwwwwEEEE++++eeeejjjjMMMMPPPPtttt22222222SSSSoooouuuu2222EEEE22222222ffffQQQQjjjjppppSSSSvvvv0000////kkkkEEEEBBBBvvvvTTTTiiiiyyyyPPPPbbbbGGGGQQQQNNNN2222zzzzGGGG9999vvvvKKKKPPPP7777nnnnNNNNTTTTNNNNssssHHHHwwwwWWWW5555UUUUGGGGmmmmGGGGXXXXLLLLHHHHxxxxoooonnnnYYYYmmmmKKKKSSSSZZZZ0000mmmmBBBBllllttttCCCCcccc0000ZZZZyyyypppp4444llllwwwwLLLLSSSSwwwwffff6666WWWWssssAAAAxxxxooooYYYY2222mmmmSSSSLLLLNNNNggggTTTTvvvv99998888llll////SSSSWWWWOOOO////6666rrrrllllVVVV7777////KKKKggggffffHHHHIIII6666iiiiaaaaNNNNAAAANNNNsssskkkkWWWWqqqqRRRRCCCCPPPPHHHHJJJJIIIITTTTcccckkkk5555qqqqppppEEEE4444YYYYeeeeSSSSBBBBPPPPZZZZEEEEBBBBeeeennnnEEEEffffnnnn2222XXXXllll11113333ssssaaaattttUUUU88885555kkkkZZZZooooPPPP8888ggggPPPPPPPPxxxxCCCCRRRRNNNNyyyyppppFFFFMMMM====<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
----------------------------------------

CHUNK 257
File: unknown
Size: 5358 chars
Content:
----------------------------------------
<latexit sha1_base64="5JOEV/IhfbnbaD9GxpGyWmwbF9o=">AAACM3icbVDLSsNAFJ34rPVVdelmsAgVtCRS0E2hqAtpNxWsCk0Nk+nEDp1JwsyNUEL+yY0/4kIQF4q49R+c1ix8HRg4nHMud+7xY8E12PaTNTU9Mzs3X1goLi4tr6yW1tYvdJQoyjo0EpG68olmgoesAxwEu4oVI9IX7NIfHo/9y1umNI/CcxjFrCfJTcgDTgkYySs1mxUXBgzIDq7jPdyunNQd7Erex7BLd4ziBorQ1MnSVoZdnUgv5XUnu25NonYe9bgJe6WyXbUnwH+Jk5MyytH2Sg9uP6KJZCFQQbTuOnYMvZQo4FSwrOgmmsWEDskN6xoaEsl0L53cnOFto/RxECnzQsAT9ftESqTWI+mbpCQw0L+9sfif100gOOylPIwTYCH9WhQkAkOExwXiPleMghgZQqji5q+YDogpCUzNRVOC8/vkv+Riv+rUqrWzWrlxlNdRQJtoC1WQgw5QA52iNuogiu7QI3pBr9a99Wy9We9f0Skrn9lAP2B9fAKhIqYl</latexit> Skip-gram with negative sampling (SGNS) Idea: recast problem as binary classification! P (D = 1 t, c) =  (u v ) t c | · • <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""++++eeeeQQQQ6666DDDDddddAAAAqqqqXXXXMMMMFFFFHHHHXXXX0000OOOOwwwwYYYYllllYYYYQQQQ5555TTTTwwww9999TTTT22224444===="""">>>>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<<<<////llllaaaatttteeeexxxxiiiitttt>>>> Target word is positive example 1 • All words not in context are negative  (x) = 1 + exp( x)   <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""QQQQvvvv4444DDDDTTTTdddd6666PPPP1111PPPPmmmmvvvvwwww3333zzzzCCCC7777YYYY////ccccLLLLIIIIeeeekkkkIIIIGGGGAAAA===="""">>>>AAAAAAAAAAAACCCCCCCC3333iiiiccccbbbbVVVVDDDDLLLLSSSSggggMMMMxxxxFFFFMMMM3333UUUUVVVV66662222vvvvUUUUZZZZdddduuuuQQQQoooovvvvQQQQIIIIppppaaaaJJJJCCCCLLLLooooRRRRiiiimmmm5555ccccVVVVrrrrAAAAPPPP6666AAAAwwwwllllkkkk2222bbbbaaaa0000GGGGRRRRmmmmSSSSDDDDLLLLSSSSMMMMnnnnTTTTvvvvxxxxllll9999xxxx44440000IIIIRRRRtttt////6666AAAAOOOO////////GGGGttttJJJJ2222FFFFtttthhhh66664444ccccDDDDjjjjnnnnXXXXuuuu66669999xxxx444488885555UUUU9999ppppxxxxvvvvqqqq3333ccccyyyyuuuurrrraaaa++++kkkkZZZZ++++ssss7777CCCC1111vvvvbbbbOOOO7777ZZZZ++++8888ffffNNNNFFFFWWWWUUUUSSSSEEEEIIIIbbbbJJJJOOOOKKKKRRRRbbbbPPPPttttYYYYUUUUcccc5555CCCC2222ttttBBBBMMMMcccc9999qqqqOOOOJJJJccccXXXXCCCC55557777TTTTllllDDDD2222++++mmmmffffuuuuuuuuBBBBSSSSssssWWWWiiii8888FFFF6666PPPPYYYY++++ooooJJJJ3333AAAA9999ZZZZwwwwAAAAjjjjWWWWRRRRuuuurrrraaaaRRRRVVVVeeeexxxxvvvvssssDDDDllllUUUUQQQQVVVVeeeeQQQQTTTTeeeeQQQQmmmmKKKKRRRRooookkkkiiiiJJJJ4444AAAAllll00006666iiiissssuuuunnnnoooo8888qqqqkkkkaaaa5555eeeeccccqqqqjjjjMMMMDDDDXXXXCCCCYYYYooooIIIIyyyyWWWWQQQQoooodddd66661111vvvv9999xxxxeeeeRRRRBBBBJJJJBBBBQQQQ000000004444VVVVqqqqqqqqDDDDnnnnFFFFhhhh7777KKKKZZZZaaaaaaaaEEEEUUUU4444nnnnBBBBTTTTddddRRRRNNNNMMMMZZZZkkkkiiiiPPPPuuuu0000YYYY2222iiiiIIIIBBBBVVVVVVVVeeeeOOOOvvvvttttllllAAAAoooo++++NNNN0000ooooNNNNBBBBJJJJEEEE2222FFFFGGGGssss7777UUUU3333xxxxMMMMppppFFFFkkkkqqqqNNNNhhhhWWWW88886666BBBBddddYYYYDDDDtttteeeehhhhNNNNxxxxffff++++8888TTTTqqqqKKKKDDDDSSSSyyyy9999llllYYYYZZZZxxxxooooGGGGppppLLLL5555ooooiiiiDDDDhhhhUUUUEEEEddddwwwwGGGGggggzzzzssssMMMMUUUUmmmmJJJJ5555mmmmNNNNDDDDMMMMJJJJHHHHMMMM3333AAAArrrrJJJJAAAAJJJJttttAAAAttttIIIImmmmvvvvYYYYEEEEJJJJAAAAiiiiyyyy8888vvvvkkkk++++ZZZZZZZZFFFFTTTTllllVVVVddddHHHHddddeeeeqqqqllll1111nnnncccceeeeTTTTBBBBEEEESSSSiiiiCCCCMMMMkkkkDDDDggggAAAAttttTTTTAAAALLLLaaaaiiiiDDDDBBBBiiiiDDDDggggEEEETTTTyyyyDDDDVVVV////BBBBmmmmPPPPVVVVkkkkvvvv1111rrrrvvvv1111MMMMWWWW////NNNNWWWWddddnnnnMMMMIIIIffffggggDDDD6666////MMMMHHHH4444ccccOOOOZZZZBBBBgggg========<<<<////llllaaaatttteeeexxxxiiiitttt>>>> To compute loss, pick K random words as negative examples: K 1 J(✓) = P (D = 1 t, c) P (D = 0 t , c) i   |   K | i=1 X
----------------------------------------

CHUNK 258
File: unknown
Size: 4859 chars
Content:
----------------------------------------
Continuous Bag of Words (CBOW) T L(✓) = P (w w , m j m, j = 0) t t+j | { }     6 t=1 Y <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""3333++++llll6666AAAAbbbbcccc66663333xxxxGGGGDDDDhhhhSSSSVVVVFFFFppppwwwwKKKKTTTTAAAAllllCCCCKKKK8888ffffUUUU===="""">>>>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<<<<////llllaaaatttteeeexxxxiiiitttt>>>> 1 v ̄ = v t t+j 2m m j m,j=0 X     6 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""uuuu3333qqqqEEEE2222VVVVmmmmppppSSSSooooWWWWttttPPPPssssbbbbLLLLZZZZccccSSSSmmmm88888888TTTTLLLLffffQQQQ4444===="""">>>>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<<<<////llllaaaatttteeeexxxxiiiitttt>>>>
----------------------------------------

CHUNK 259
File: unknown
Size: 200 chars
Content:
----------------------------------------
GloVe: Global Vectors • Let’s take the global co-occurrence statistics: X i,j • Training faster • Scalable to very large corpora (Pennington et al, 2014): GloVe: Global Vectors for Word Representation
----------------------------------------

CHUNK 260
File: unknown
Size: 93 chars
Content:
----------------------------------------
GloVe: Global Vectors (Pennington et al, 2014): GloVe: Global Vectors for Word Representation
----------------------------------------

CHUNK 261
File: unknown
Size: 4777 chars
Content:
----------------------------------------
FastText: Sub-Word Embeddings • Similar as Skip-gram, but break words into n-grams with n = 3 to 6 3-grams: <wh, whe, her, ere, re> where: 4-grams: <whe, wher, here, ere> 5-grams: <wher, where, here> 6-grams: <where, where> • u v u v Replace by g j i j · · <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""ooooXXXX8888MMMM9999OOOO0000FFFFffff2222eeeekkkkffffvvvvBBBBmmmmSSSSkkkkooooLLLLppppIIII7777yyyy8888XXXXYYYY===="""">>>>AAAAAAAAAAAACCCCCCCCHHHHiiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAEEEENNNN3333UUUUrrrr1111qqqq////oooohhhh44449999uuuuFFFFggggEEEETTTTyyyyUUUURRRRQQQQYYYY9999FFFFLLLLxxxx4444rrrr2222FFFFZZZZooooQQQQ9999hhhhssssNNNNuuuu3333aaaazzzzWWWW7777YYYY3333RRRRRRRRKKKKyyyyNNNNGGGGLLLLffff8888WWWWLLLLBBBB0000WWWW8888++++hhhhOOOO8888++++WWWW////ccccttttBBBBGGGG00009999ccccHHHHAAAA444477770000ZZZZZZZZuuuuYYYYFFFFCCCCaaaaNNNNKKKKOOOO88886666XXXXVVVVVVVVllllaaaaXXXXllllllllddddqqqq66667777XXXXNNNNjjjjaaaa3333ttttnnnnffffssss3333bbbb2222OOOOEEEEqqqqnnnnEEEEppppIIII0000FFFFEEEE////IIIIuuuuQQQQIIIIoooowwwwyyyykkkkllllbbbbUUUU88883333IIIIXXXXSSSSIIIIJJJJiiiiggggNNNNGGGGuuuussssHHHHooooqqqqvvvvCCCC7777YYYYyyyyIIIIVVVVFFFFffffxxxxWWWWTTTTxxxxLLLLiiiixxxxWWWWjjjjAAAAaaaaUUUUQQQQxxxx0000kkkkbbbbyyyy7777ccccNNNN++++jjjjPPPPQQQQwwwwiiiiLLLLIIII00009999yyyynnnnssss44441111BBBBoooo++++CCCCOOOONNNNcccc////////eeeetttt++++ttttOOOOwwww5555kkkkCCCCLLLLhhhhKKKK3333JJJJHHHHVVVVQQQQoooouuuuXXXXbbbbnnnn////1111QQQQ4444DDDDQQQQmmmmXXXXGGGGOOOOGGGGllllOOOOqqqq5555TTTTqqqqKKKK9999DDDDEEEEllllNNNNMMMMSSSSNNNN5555rrrrZZZZ8888qqqqkkkkiiiiAAAA8888QQQQggggPPPPSSSSMMMM5555SSSSjjjjmmmmCCCCggggvvvvmmmmzzzz6666SSSSwwww2222OOOOjjjjhhhhDDDDAAAASSSS0000hhhhTTTTXXXXccccKKKKrrrr++++nnnnsssshhhhQQQQrrrrNNNNQQQQkkkkDDDDkkkkxxxxnnnnccccaaaaOOOOaaaa9999wwwwrrrrxxxxPPPP6666++++XXXX6666uuuujjjjCCCCyyyyyyyyhhhhPPPPUUUUkkkk00004444nnnniiii2222KKKKUUUUggggaaaa1111ggggEEEEUUUUqqqqMMMMKKKKSSSSSSSSYYYYMMMM0000mmmmhhhhiiiiAAAAssssqqqqbbbbkkkkVVVV4444iiiiGGGGSSSSCCCCGGGGuuuuTTTTXXXXcccc2222EEEE4444MMMM6666////vvvvEEEEgggg6666ppppwwww3333XXXXaaaabbbbgggg3333ZZZZ////XXXXmmmmZZZZRRRRllllHHHHFFFFRRRRyyyyAAAAIIII3333AAAACCCCXXXXHHHHAAAAOOOOmmmmuuuuAAAAaaaattttEEEEAAAAbbbbYYYYPPPPAAAAAAAAnnnnssssAAAALLLLeeeeLLLLUUUUeeeerrrrWWWWffffrrrrzzzzXXXXqqqqffffttttVVVVaaaassssccccmmmmYYYYffff////IIIIHHHH11118888QQQQ1111eeee4444JJJJoooovvvv<<<<////llllaaaatttteeeexxxxiiiitttt>>>> g n-grams(w ) X i 2 <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""vvvvjjjjRRRRrrrr++++MMMMXXXXnnnnddddBBBBSSSS33339999DDDD++++OOOOssss22222222ZZZZGGGGPPPPUUUURRRRTTTTaaaaYYYY===="""">>>>AAAAAAAAAAAACCCCKKKKHHHHiiiiccccbbbbVVVVBBBBNNNNSSSS8888NNNNAAAAFFFFNNNNzzzzUUUUrrrr1111qqqq////qqqqhhhh66669999LLLLBBBBaaaahhhhHHHHiiiiyyyyJJJJCCCCHHHHqqqqzzzz6666MMMMWWWWjjjjggggqqqq1111CCCCUUUU8888JJJJmmmmuuuu2222nnnnXXXXbbbbjjjjZZZZhhhh99996666VVVVaaaaQQQQnnnn6666OOOOFFFF////++++KKKKFFFFxxxxFFFFFFFFvvvvPPPPppppLLLL3333LLLLYYYYRRRRttttHHHHVVVVggggYYYYZZZZiiiiZZZZxxxx777744443333ffffiiiiyyyy4444BBBBttttvvvv++++ttttAAAAppppzzzz8888wwwwuuuuLLLLSSSS8888XXXXllll0000ssssrrrrqqqq2222vvvvppppGGGGeeeeXXXXOOOOrrrrqqqqaaaaNNNNEEEEUUUUddddaaaaggggkkkkYYYYjjjjUUUUrrrrUUUU88880000EEEE1111yyyyyyyyBBBBnnnnAAAAQQQQ7777DDDDZZZZWWWWjjjjIIIISSSS++++YYYYDDDDdddd++++////3333zzzzkkkk3333wwwwyyyyYYYY0000jjjjyyyySSSS1111zzzzCCCCMMMMWWWWTTTTsssskkkkXXXXcccckkkkDDDDTTTTggggkkkkYYYYyyyySSSSuuuuffffuuuujjjjooooJJJJvvvvbbbbSSSSLLLLXXXXSSSS6666xxxxddddIIIIEEEE9999QQQQHHHHrrrrQQQQVVVVSSSSTTTTUUUUWWWWffffXXXXeeee4444////uuuuZZZZGGGGxxxxLLLLoooo++++UUUUGGGGaaaaZZZZJJJJ4444JJJJ0000UUUU4444EEEE++++EEEEccccaaaaZZZZNNNN6666ddddVVVV66667777YYYYNNNNXXXXssssMMMMPPPPEEEEuuuuccccnnnnFFFFRRRRQQQQjjjjkkkkuuuuvvvv////OOOOpppp2222IIIIppppqqqqEEEETTTTAAAAIIIIVVVVRRRROOOOuuuuWWWWYYYY8888ffffQQQQTTTTooookkkkCCCCTTTTggggXXXXLLLLSSSSmmmm6666iiiiWWWWUUUUxxxxoooonnnn3333RRRRZZZZyyyy1111BBBBJJJJQQQQqqqqbbbbbbbb6666ffffjjjjQQQQDDDDOOOO8888ZZZZppppYYYYOOOODDDDSSSSJJJJkkkknnnnAAAAYYYY////VVVV3333xxxxOOOOpppp2222VVVVssssPPPPQQQQ99998888kkkkRRRRzzzzvvvvqqqqaaaaWWWW8888kkkk////uuuueeee1111EEEEgggghhhhOOOO2222iiiimmmmXXXXccccQQQQJJJJMMMM0000ssssllllHHHHQQQQSSSSIIIIwwwwRRRRHHHHjjjjUUUUGGGGuuuu5555wwwwxxxxSSSSiiiiIIIIooooSSSSGGGGEEEEKKKKmmmm55552222xxxxbbbbRRRRHHHHFFFFKKKKFFFFgggguuuuiiii2222ZZZZEEEEppppzzzzppppkkkk2222ddddJJJJ88887777DDDDmmmm2222DDDDXXXXnnnn6666qqqqhhhhSSSSPPPP8888vvvvrrrrKKKKKKKKIIIIddddttttIIIIuuuuqqqqyyyyEEEEHHHHHHHHqqqqIIII4444uuuu0000CCCCVVVVqqqqIIIIIIIIooooeeee0000TTTTNNNN6666QQQQ++++////WWWWkkkk////VVVViiiiffffVVVViiiiffffkkkk2222jjjjBBBByyyymmmmeeee22220000RRRR9999YYYYXXXX9999////UUUUuuuuKKKKeeeetttt<<<<////llllaaaatttteeeexxxxiiiitttt>>>> • More to come! Contextualized word embeddings (Bojanowski et al, 2017): Enriching Word Vectors with Subword Information
----------------------------------------

CHUNK 262
File: unknown
Size: 237 chars
Content:
----------------------------------------
Trained word embeddings available • word2vec: https://code.google.com/archive/p/word2vec/ • GloVe: https://nlp.stanford.edu/projects/glove/ • FastText: https://fasttext.cc/ Differ in algorithms, text corpora, dimensions, cased/uncased...
----------------------------------------

CHUNK 263
File: unknown
Size: 26 chars
Content:
----------------------------------------
Evaluating Word Embeddings
----------------------------------------

CHUNK 264
File: unknown
Size: 461 chars
Content:
----------------------------------------
Extrinsic vs intrinsic evaluation 👎 Extrinsic evaluation • Let’s plug these word embeddings ML model into a real NLP system and see whether this improves performance 0.31 0.01 1.87 −3.17 1.23 • (−0.28) (−0.91) (0.03) (−0.18) (1.59) Could take a long time but still the most important evaluation metric I don’t like this movie Intrinsic evaluation • Evaluate on a specific/intermediate subtask • Fast to compute • Not clear if it really helps the downstream task
----------------------------------------

CHUNK 265
File: unknown
Size: 220 chars
Content:
----------------------------------------
Intrinsic evaluation Word similarity Example dataset: wordsim-353 353 pairs of words with human judgement http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/ Cosine similarity: Metric: Spearman rank correlation
----------------------------------------

CHUNK 266
File: unknown
Size: 36 chars
Content:
----------------------------------------
Intrinsic evaluation Word Similarity
----------------------------------------

CHUNK 267
File: unknown
Size: 2443 chars
Content:
----------------------------------------
Intrinsic evaluation Word analogy man: woman king: ? ≈ arg max (cos(u , u u + u )) i b a c   i <<<<llllaaaatttteeeexxxxiiiitttt sssshhhhaaaa1111____bbbbaaaasssseeee66664444====""""JJJJrrrrppppggggXXXXOOOOIIIIkkkk2222wwwwxxxxyyyy6666PPPPeeeeooooggggrrrrggggqqqqRRRRjjjj0000rrrrjjjj22224444===="""">>>>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======<<<<////llllaaaatttteeeexxxxiiiitttt>>>> semantic syntactic Chicago:Illinois Philadelphia: ? bad:worst cool: ? 
----------------------------------------

CHUNK 268
File: unknown
Size: 76 chars
Content:
----------------------------------------
≈ ≈ More examples at http://download.tensorflow.org/data/questions-words.txt
----------------------------------------

CHUNK 269
File: unknown
Size: 138 chars
Content:
----------------------------------------
What can go wrong with word embeddings? • What’s wrong with learning a word’s “meaning” from its usage? • What data are we learning from? 
----------------------------------------

CHUNK 270
File: unknown
Size: 44 chars
Content:
----------------------------------------
• What are we going to learn from this data?
----------------------------------------

CHUNK 271
File: unknown
Size: 174 chars
Content:
----------------------------------------
What do we mean by bias? • Identify she - he axis in word vector space, project words onto this axis Bolukbasi et al. (2016) • Nearest neighbor of (b - a + c) Manzini et al. 
----------------------------------------

CHUNK 272
File: unknown
Size: 6 chars
Content:
----------------------------------------
(2019)
----------------------------------------

CHUNK 273
File: unknown
Size: 205 chars
Content:
----------------------------------------
Debiasing • Identify gender subspace with gendered words homemaker • Project words onto this she subspace homemaker’ • Subtract those woman projections from the he original word man Bolukbasi et al. (2016)
----------------------------------------

CHUNK 274
File: unknown
Size: 217 chars
Content:
----------------------------------------
Hardness of Debiasing • Not that effective...and the male and female words are still clustered together • Bias pervades the word embedding space and isn’t just a local property of a few words Gonen and Goldberg (2019)
----------------------------------------

CHUNK 275
File: unknown
Size: 78 chars
Content:
----------------------------------------
Contextual Token Representations ULMfit, OpenAI GPT, ELMo, BERT, XLM Noe Casas
----------------------------------------

CHUNK 276
File: unknown
Size: 120 chars
Content:
----------------------------------------
Background: Language Modeling T T ... </s> 1 2 • Data: Monolingual Corpus softmax softmax ... softmax project. project. 
----------------------------------------

CHUNK 277
File: unknown
Size: 175 chars
Content:
----------------------------------------
... project. • Task: predict next token given previous tokens (causal): embed 1 embed 2 embed 3 Model P(T | T ...T ) i 1 i−1 • Usual models: LSTM, Transformer. <s> T ... T 1 N
----------------------------------------

CHUNK 278
File: unknown
Size: 278 chars
Content:
----------------------------------------
Contextual embeddings: intuition • Same word can have different meaning depending on the context. Example: - Please, type everything in lowercase. - What type of flowers do you like most? • Classic word embeddings offer the same vector representation regardless of the context. 
----------------------------------------

CHUNK 279
File: unknown
Size: 67 chars
Content:
----------------------------------------
• Solution: create word representations that depend on the context.
----------------------------------------

CHUNK 280
File: unknown
Size: 299 chars
Content:
----------------------------------------
Articles Model Alias Org. Article Reference Universal Language Model Fine-tuning for Text Classification ULMfit fast.ai Howard and Ruder Deep contextualized word representations ELMo AllenNLP Peters et al. Improving Language Understanding by Generative Pre-Training OpenAI GPT OpenAI Radford et al. 
----------------------------------------

CHUNK 281
File: unknown
Size: 179 chars
Content:
----------------------------------------
BERT: Pre-training of Deep Bidirectional Transformers for BERT Google Language Understanding Devlin et al. Cross-lingual Language Model Pretraining XLM Facebook Lample and Conneau
----------------------------------------

CHUNK 282
File: unknown
Size: 113 chars
Content:
----------------------------------------
Overview • Train model in one of multiple tasks that lead to word representations. • Release pre-trained models. 
----------------------------------------

CHUNK 283
File: unknown
Size: 121 chars
Content:
----------------------------------------
• Use pre-trained models, options: A. Fine-tune model on final task. B. Directly encode token representations with model.
----------------------------------------

CHUNK 284
File: unknown
Size: 365 chars
Content:
----------------------------------------
Overview (graphical) Phase 1: Phase 2: semi-supervised training downstream task fine-tuning *LM task Downstream task LM task head Downstream task (projection + softmax) head contextual transfer learning representations Language Language Modeling Modeling Architecture Architecture small learning rate or directly freeze monolingual task-specific weights corpus data
----------------------------------------

CHUNK 285
File: unknown
Size: 335 chars
Content:
----------------------------------------
Differences Alias Model Token Tasks Language ULMfit LSTM word Causal LM English ELMo LSTM word Bidirectional LM English Causal LM OpenAI GPT Transformer subword English + Classification Masked LM BERT Transformer subword + Next sentence Multilingual prediction Causal LM XLM Transformer subword +Masked LM Multilingual + Translation LM
----------------------------------------

CHUNK 286
File: unknown
Size: 110 chars
Content:
----------------------------------------
ULMFiT • Task: causal LM T T </s> 1 2 ... softmax softmax ... softmax • Model: 3-layer LSTM project. project. 
----------------------------------------

CHUNK 287
File: unknown
Size: 102 chars
Content:
----------------------------------------
... project. • Tokens: words LSTM LSTM ... LSTM LSTM LSTM ... LSTM LSTM LSTM ... LSTM </s> E ... E 1 N
----------------------------------------

CHUNK 288
File: unknown
Size: 94 chars
Content:
----------------------------------------
ELMO • T 1 T 2 ... T N Task: bidirectional LM softmax softmax ... softmax • project. project. 
----------------------------------------

CHUNK 289
File: unknown
Size: 179 chars
Content:
----------------------------------------
... project. Model: 2-layer biLSTM • Tokens: words LSTM LSTM ... LSTM LSTM ... LSTM LSTM LSTM LSTM ... LSTM LSTM ... LSTM LSTM charCNN charCNN charCNN charCNN ... <s> C C </s> 1 N
----------------------------------------

CHUNK 290
File: unknown
Size: 153 chars
Content:
----------------------------------------
OpenAI GPT • Task: causal LM Output tokens he will be late </s> • Model: self-attention layers softmax softmax softmax softmax softmax project. project. 
----------------------------------------

CHUNK 291
File: unknown
Size: 148 chars
Content:
----------------------------------------
project. project. project. • Tokens: subwords Self-attention layers Token </s>] he will be late embeddings + + + + + Positional 0 1 2 3 4 embeddings
----------------------------------------

CHUNK 292
File: unknown
Size: 211 chars
Content:
----------------------------------------
BERT he will br late [SEP] you should leave now [SEP] Output tokens This output is used for classification tasks softmax softmax softmax softmax softmax softmax softmax softmax softmax softmax project. project. 
----------------------------------------

CHUNK 293
File: unknown
Size: 418 chars
Content:
----------------------------------------
project. project. project. project. project. project. project. project. Self-attention Layers Token [CLS] he [MASK] be late [SEP] you [MASK] leave now [SEP] embeddings + + + + + + + + + + + Positional 0 1 2 3 4 5 6 7 8 9 10 embeddings + + + + + + + + + + + Segment A A A A A A B B B B B embeddings 15% of tokens get masked • Tasks: masked LM + next sentence prediction • Model: self-attention layers • Tokens: subwords
----------------------------------------

CHUNK 294
File: unknown
Size: 821 chars
Content:
----------------------------------------
Masked Language take [/s] drink now Modeling (MLM) Transformer Token [/s] [MASK] a seat X[MASK] LhaMve a [MASK] [/s] [MASK] relax and embeddings + + + + + + + + + + + + Position 0 1 2 3 4 5 6 7 8 9 10 11 embeddings + + + + + + + + + + + + Language en en en en en en en en en en en en embeddings Translation Language curtains were les bleus Modeling (TLM) Transformer Token [/s] the [MASK] [MASK] blue [/s] [/s] [MASK] rideaux étaient [MASK] [/s] embeddings + + + + + + + + + + + + Position 0 1 2 3 4 5 0 1 2 3 4 5 embeddings + + + + + + + + + + + + Language en en en en en en fr fr fr fr fr fr embeddings • Tasks: LM + masked LM + Translation LM Masked LM with • Model: self-attention layers parallel sentences • Tokens: subwords Projection and softmax are omitted *figure from “Cross-lingual Language Model Pretraining”
----------------------------------------

CHUNK 295
File: unknown
Size: 102 chars
Content:
----------------------------------------
Downstream Tasks • Natural Language Inference (NLI) or Cross-lingual NLI. • Text classification (e.g. 
----------------------------------------

CHUNK 296
File: unknown
Size: 173 chars
Content:
----------------------------------------
sentiment analysis). • Next sentence prediction. • Supervised and Unsupervised Neural Machine Translation (NMT). • Question Answering (QA). • Named Entity Recognition (NER).
----------------------------------------

CHUNK 297
File: unknown
Size: 249 chars
Content:
----------------------------------------
Further reading • “Looking for ELMo's friends: Sentence-Level Pretraining Beyond Language Modeling”, Bowman et al., 2018 • “What do you learn from context? Probing for sentence structure in contextualized word representations”, Tenney et al., 2018. 
----------------------------------------

CHUNK 298
File: unknown
Size: 140 chars
Content:
----------------------------------------
• “Assessing BERT’s Syntactic Abilities”, Goldberg, 2018 • “Learning and Evaluating General Linguistic Intelligence”, Yogatama et al., 2019.
----------------------------------------

CHUNK 299
File: unknown
Size: 233 chars
Content:
----------------------------------------
Differences with other representations Note the differences of contextual token representations with: • Non-word representations like in (CoVe): Learned in Translation: Contextualized Word Vectors by McCann et al. 2017 [salesforce]. 
----------------------------------------

CHUNK 300
File: unknown
Size: 178 chars
Content:
----------------------------------------
• Fixed-size sentence representations like in Massively Multilingual Sentence Embeddings for Zero-Shot Cross- Lingual Transfer and Beyond by Artetxe and Schewnk, 2018 [facebook].
----------------------------------------

CHUNK 301
File: unknown
Size: 246 chars
Content:
----------------------------------------
Other resources • https://nlp.stanford.edu/seminar/details/jdevlin.pdf • http://jalammar.github.io/illustrated-bert/ • https://medium.com/dissecting-bert/dissecting-bert- part2-335ff2ed9c73 • https://github.com/huggingface/pytorch-pretrained-BERT
----------------------------------------

CHUNK 302
File: unknown
Size: 552 chars
Content:
----------------------------------------
Summary Phase 1: Phase 2: semi-supervised training downstream task fine-tuning *LM task Downstream task task-specific data LM task head Downstream task monolingual (projection + softmax) transfer learning head corpus model model Alias Model Token Tasks Language ULMfit LSTM word Causal LM English ELMo LSTM word Bidirectional LM English Causal LM OpenAI GPT Transformer subword English + Classification Masked LM BERT Transformer subword Multilingual + Next sentence prediction Causal LM XLM Transformer subword +Masked LM Multilingual + Translation LM
----------------------------------------

CHUNK 303
File: unknown
Size: 12 chars
Content:
----------------------------------------
Bonus slides
----------------------------------------

CHUNK 304
File: unknown
Size: 169 chars
Content:
----------------------------------------
Are these really token representations? • They are a linear projection away from he will be late token space. softmax softmax softmax softmax project. project. project. 
----------------------------------------

CHUNK 305
File: unknown
Size: 105 chars
Content:
----------------------------------------
project. • Word-level nearest neighbours in Model corpus finds same word with same usage. he will be late
----------------------------------------

CHUNK 306
File: unknown
Size: 1410 chars
Content:
----------------------------------------
Information Extraction: Methodologies and Applications Jie Tang, Mingcai Hong, Duo Zhang, Bangyong Liang, and Juanzi Li Jie Tang (corresponding author) Affiliation: Department of Computer Science, Tsinghua University Telephone: +8610-62788788-20 Fax number: +8610-62789831 E-mail: jietang@tsinghua.edu.cn Post mail address: 10-201, East Main Building, Tsinghua University, Beijing, 100084. China. Mingcai Hong Affiliation: Department of Computer Science, Tsinghua University Telephone: +8610-62788788-20 Fax number: +8610-62789831 E-mail: hmc@keg.cs.tsinghua.edu.cn Post mail address: 10-201, East Main Building, Tsinghua University, Beijing, 100084. China. Duo Zhang Affiliation: Department of Computer Science, Tsinghua University Telephone: +8610-62788788-20 Fax number: +8610-62789831 E-mail: zhangduo@keg.cs.tsinghua.edu.cn Post mail address: 10-201, East Main Building, Tsinghua University, Beijing, 100084. China. Bangyong Liang Affiliation: NEC Labs China Telephone: +86013601002822 Fax number: +8610-62789831 E-mail: liangbangyong@research.nec.com.cn Post mail address: 11th Floor, Innovation Plaza, Tsinghua Science Park, Beijing, 100084, China Juanzi Li Affiliation: Department of Computer Science, Tsinghua University Telephone: +8610-62781461 Fax number: +8610-62789831 E-mail: ljz@keg.cs.tsinghua.edu.cn Post mail address: 10-201, East Main Building, Tsinghua University, Beijing, 100084. China. 
----------------------------------------

CHUNK 307
File: unknown
Size: 1 chars
Content:
----------------------------------------
1
----------------------------------------

CHUNK 308
File: unknown
Size: 219 chars
Content:
----------------------------------------
Keyword List Computer Science Data Resource Management Data Extraction Data Management Data Mining Knowledge Discovery Software Natural Language Processors Information Systems Information Theory Information Processing 2
----------------------------------------

CHUNK 309
File: unknown
Size: 802 chars
Content:
----------------------------------------
Information Extraction: Methodologies and Applications Jie Tang1, Mingcai Hong1, Duo Zhang1, Bangyong Liang2, and Juanzi Li1 1Department of Computer Science, Tsinghua University 10-201, East Main Building, Tsinghua University, Beijing, 100084. China 2NEC Labs China 11th Floor, Innovation Plaza, Tsinghua Science Park, Beijing, 100084. China liangbangyong@research.nec.com.cn ABSTRACT This chapter is concerned with the methodologies and applications of information extraction. Information is hidden in the large volume of web pages and thus it is necessary to extract useful information from the web content, called Information Extraction. In information extraction, given a sequence of instances, we identify and pull out a sub-sequence of the input that represents information we are interested in. 
----------------------------------------

CHUNK 310
File: unknown
Size: 1360 chars
Content:
----------------------------------------
In the past years, there was a rapid expansion of activities in the information extraction area. Many methods have been proposed for automating the process of extraction. However, due to the heterogeneity and the lack of structure of Web data, automated discovery of targeted or unexpected knowledge information still presents many challenging research problems. In this chapter, we will investigate the problems of information extraction and survey existing methodologies for solving these problems. Several real-world applications of information extraction will be introduced. Emerging challenges will be discussed. INTRODUCTION Information Extraction (IE), identifying and pulling out a sub-sequence from a given sequence of instances that represents information we are interested in, is an important task with many practical applications. Information extraction benefits many text/web applications, for example, integration of product information from various websites, question answering, contact information search, finding the proteins mentioned in a biomedical journal article, and removal of the noisy data. Our focus will be on methodologies of automatic information extraction from various types of documents (including plain texts, web pages, and emails, etc.). Specifically, we will discuss three of the most popular methods: rule learning based 1
----------------------------------------

CHUNK 311
File: unknown
Size: 2351 chars
Content:
----------------------------------------
method, classification model based method, and sequential labeling based method. All these methods can be viewed as supervised machine learning approaches. They all consist of two stages: extraction and training. In extraction, the sub-sequence that we are interested in are identified and extracted from given data using learned model(s) by different methods. Then the extracted data are annotated as specified information on the basis of the predefined metadata. In training, the model(s) are constructed to detect the sub-sequence. In the models, the input data is viewed as a sequence of instances, for example, a document can be viewed as either a sequence of words or a sequence of text lines (it depends on the specific application). All these methodologies have immediate real-life applications. Information extraction has been applied, for instance, to part-of-speech tagging (Ratnaparkhi, 1998), named entity recognition (Zhang, 2004), shallow parsing (Sha, 2003), table extraction (Ng, 1999; Pinto, 2003; Wang, 2002), and contact information extraction (Kristjansson, 2004). In the rest of the chapter, we will describe the three types of the state-of-the-art methods for information extraction. This is followed by presenting several applications to better understand how the methods can be utilized to help businesses. The chapter will have a mix of research and industry flavor, addressing research concepts and looking at the technologies from an industry perspective. After that, we will discuss the challenges the information extraction community faced. Finally, we will give the concluding remark. METHODOLOGIES Information extraction is an important research area, and many research efforts have been made so far. Among these research work, rule learning based method, classification based method, and sequential labeling based method are the three state-of-the-art methods. Rule Learning based Extraction Methods In this section, we review the rule based algorithms for information extraction. Numerous information systems have been developed based on the method, including: AutoSlog (Riloff, 1993), Crystal (Soderland, 1995), (LP)2 (Ciravegna, 2001), iASA (Tang, 2005b), Whisk (Soderland, 1999), Rapier (Califf, 1998), SRV (Freitag, 1998), WIEN (Kushmerick, 1997), Stalker (Muslea, 1998; Muslea, 1999a), BWI (Freitag, 2000), etc. 
----------------------------------------

CHUNK 312
File: unknown
Size: 298 chars
Content:
----------------------------------------
See (Muslea, 1999b; Siefkes, 2005; Peng, 2001) for an overview. In general, the methods can be grouped into three categories: dictionary based method, rule based method, and wrapper induction. Dictionary based method Traditional information extraction systems first construct a pattern (template) 2
----------------------------------------

CHUNK 313
File: unknown
Size: 2650 chars
Content:
----------------------------------------
dictionary, and then use the dictionary to extract needed information from the new untagged text. These extraction systems are called as dictionary based systems (also called pattern based systems) including: AutoSlog (Riloff, 1993), AutoSlog-TS (Riloff, 1996), and CRYSTAL (Soderland, 1995). The key point in the systems is how to learn the dictionary of patterns that can be used to identify the relevant information from a text. AutoSlog (Riloff, 1993) was the first system to learn text extraction dictionary from training examples. AutoSlog builds a dictionary of extraction patterns that are called concept nodes. Each AutoSlog concept node has a conceptual anchor that activates it and a linguistic pattern, which, together with a set of enabling conditions, guarantees its applicability. The conceptual anchor is a triggering word, while the enabling conditions represent constraints on the components of the linguistic pattern. For instance, in order to extract the target of the terrorist attack from the sentence The Parliament was bombed by the guerrillas. One can use a concept that consists of the triggering word bombed together with the linguistic pattern <subject> passive-verb. Applying such an extraction pattern is straightforward: first, the concept is activated because the sentence contains the triggering word bombed; then the linguistic pattern is matched against the sentence and the subject is extracted as the target of the terrorist attack. AutoSlog uses a predefined set of 13 linguistic patterns; the information to be extracted can be one of the following syntactic categories: subject, direct object, or noun phrase. In general, the triggering word is a verb, but if the information to be extracted is a noun phrase, the triggering word may also be a noun. In Figure 1, we show a sample concept node. The slot “Name” is a concise, human readable description of the concept. The slot “Trigger” defines the conceptual anchor, while the slot “Variable Slots” represents that the information to be extracted is the subject of the sentence. Finally, the subject must be a physical target (see “Constraints:”), and the enabling conditions require the verb to be used in its passive form. Concept Node: Name: target-subject-passive-verb-bombed Trigger: bombed Variable Slots: (target (*S *1)) Constraints: (class phys-target *S*) Constant Slots: (type bombing) Enabling Conditions: ((passive)) Figure 1. Example of AutoSlog concept node AutoSlog needs to parse the natural language sentence using a linguistic parser. The parser is used to generate syntax elements of a sentence (such as subject, verb, preposition phrase). 
----------------------------------------

CHUNK 314
File: unknown
Size: 167 chars
Content:
----------------------------------------
Then the output syntax elements are matched against the linguistic pattern and fire the best matched pattern as the result pattern to construct a pattern dictionary. 3
----------------------------------------

CHUNK 315
File: unknown
Size: 1304 chars
Content:
----------------------------------------
AutoSlog need tag the text before extracting patterns. This disadvantage has been improved by AutoSlog-TS (Riloff, 1996). In AutoSlog-TS, one does not need to make a full tag for the input data and only needs to tag the data whether it is relevant to the domain or not. The procedure of AutoSlog is divided into two stages. In the first stage, the sentence analyzer produces a syntactic analysis for each sentence and identifies the noun phrases using heuristic rules. In the second stage, the pre-classified text is inputted to the sentence analyzer again with the pattern dictionary generated in the first stage. The sentence analyzer activates all the patterns that are applicable in each sentence. The system then computes relevance statistics for each pattern and uses a rank function to rank the patterns. In the end, only the top patterns are kept in the dictionary. Riloff et al (1999) propose using bootstrapping to generate the dictionary with a few tagged texts (called seed words). The basic idea is to use a mutual bootstrapping technique to learn extraction patterns using the seed words and then exploit the learned extraction patterns to identify more seed words that belong to the same category. In this way, the pattern dictionary can be learned incrementally as the process continues. 
----------------------------------------

CHUNK 316
File: unknown
Size: 838 chars
Content:
----------------------------------------
See also Crystal (Soderland, 1995). Rule based method Different from the dictionary based method, the rule based method use several general rules instead of dictionary to extract information from text. The rule based systems have been mostly used in information extraction from semi-structured web page. A usual method is to learn syntactic/semantic constraints with delimiters that bound the text to be extracted, that is to learn rules for boundaries of the target text. Two main rule learning algorithms of these systems are: bottom-up method which learns rules from special cases to general ones, and top-down method which learns rules from general cases to special ones. There are proposed many algorithms, such as (LP)2 (Ciravegna, 2001), iASA (Tang, 2005b), Whisk (Soderland, 1999), Rapier (Califf, 1998), and SRV (Freitag, 1998). 
----------------------------------------

CHUNK 317
File: unknown
Size: 931 chars
Content:
----------------------------------------
Here we will take (LP)2 and iASA as examples in our explanation. (LP)2 (LP)2 (Ciravegna, 2001) is one of the typical bottom-up methods. It learns two types of rules that respectively identify the start boundary and the end boundary of the text to be extracted. The learning is performed from examples in a user-defined corpus (training data set). Training is performed in two steps: initially a set of tagging rules is learned; then additional rules are induced to correct mistakes and imprecision in extraction. Three types of rules are defined in (LP)2: tagging rules, contextual rules, and correction rules. A tagging rule is composed of a pattern of conditions on a connected sequence of words and an action of determining whether or not the current position is a boundary of an instance. Table 1 shows an example of the tagging rule. The first column represents a sequence of words. The second to the fifth columns represent 4
----------------------------------------

CHUNK 318
File: unknown
Size: 857 chars
Content:
----------------------------------------
Part-Of-Speech, Word type, Lookup in a dictionary, and Name Entity Recognition results of the word sequence respectively. The last column represents the action. In the example of Table 1, the action “<Speaker>” indicates that if the text match the pattern, the word “Patrick” will be identified as the start boundary of a speaker. Table 1. Example of initial tagging rule Pattern Action Word POS Kind Lookup Name Entity ; : Punctuation Patrick NNP Word Person’s first name <Speaker> Person Stroh NNP Word , , Punctuation assistant NN Word Job title professor NN Word , , Punctuation SDS NNP Word The tagging rules are induced as follows: (1) First, a tag in the training corpus is selected, and a window of w words to the left and w words to the right is extracted as constraints in the initial rule pattern. (2) Then all the initial rules are generalized. 
----------------------------------------

CHUNK 319
File: unknown
Size: 1016 chars
Content:
----------------------------------------
The generalization algorithm could be various. For example, based on NLP knowledge, the two rules (at 4 pm) and (at 5 pm) can be generalized to be (at DIGIT pm). Each generalized rule is tested on the training corpus and an error score E=wrong/matched is calculated. (3) Finally, the k best generalizations for each initial rule are kept in a so called best rule pool. This induction algorithm is also used for the other two types of rules discussed below. Table 2 indicates a generalized tagging rule for the start boundary identification of the Speaker. Table 2. Example of generalized tagging rule Pattern Action Word POS Kind Lookup Name Entity ; : Punctuation Word Person’s first name <Speaker> Person Word Punctuation assistant NN Word Jobtitle professor NN Word Another type of rules, contextual rules, is applied to improve the effectiveness of the system. The basic idea is that <tag > might be used as an indicator of the x occurrence of <tag >. For example, consider a rule recognizing an end boundary y 5
----------------------------------------

CHUNK 320
File: unknown
Size: 1171 chars
Content:
----------------------------------------
between a capitalized word and a lowercase word. This rule does not belong to the best rule pool as its low precision on the corpus, but it is reliable if used only when closing to a tag <speaker>. Consequencely, some non-best rules are recovered, and the ones which result in acceptable error rate will be preserved as the contextual rules. The correction rules are used to reduce the imprecision of the tagging rules. For example, a correction rule shown in Table 3 is used to correct the tagging mistake “at <time> 4 </time> pm” since “pm” should have been part of the time expression. So, correction rules are actions that shift misplaced tags rather than adding new tags. Table 3. Example of correction rule Pattern Action Word Wrong tag Move tag to At 4 </stime> pm </stime> After all types of rules are induced, information extraction is carried out in the following steps: (cid:122) The learned tagging rules are used to tag the texts. (cid:122) Contextual rules are applied in the context of introduced tags in the first step. (cid:122) Correction rules are used to correct mistaken extractions. (cid:122) All the identified boundaries are to be validated, e.g. 
----------------------------------------

CHUNK 321
File: unknown
Size: 1615 chars
Content:
----------------------------------------
a start tag (e.g. <time>) without its corresponding close tag will be removed, and vice versa. See also Rapier (Califf, 1998; Califf, 2003) for another IE system which adopts the bottom-up learning strategy. iASA Tang et al (2005b) propose an algorithm for learning rules for information extraction. The key idea of iASA is that it tries to induce the ‘similar’ rules first. In iASA, each rule consists of three patterns: body pattern, left pattern, and right pattern, respectively representing the text fragment to be extracted (called target instance), the w words previous to the target instance, and w words next to the target instance. Thus, the rule learning tries to find patterns not only in the context of a target instance, but also in the target instance itself. Tang et al define similarity between tokens (it can be word, punctuation, and name entity), similarity between patterns, and similarity between rules. In learning, iASA creates an initial rule set from the training data set. Then it searches for the most similar rules from the rule set and generalizes a new rule using the two rules. The new rule is evaluated on the training corpus and a score of the rule is calculated. If its score exceeds a threshold, it would be put back to the rule set. The processing continues until no new rules can be generalized. The other type of strategy for learning extraction rules is the top-down fashion. The method starts with the most generalized patterns and then gradually adds constraints into the patterns in the learning processing. See SRV (Freitag, 1998) and Whisk (Soderland, 1999) as examples. 
----------------------------------------

CHUNK 322
File: unknown
Size: 1 chars
Content:
----------------------------------------
6
----------------------------------------

CHUNK 323
File: unknown
Size: 1386 chars
Content:
----------------------------------------
Wrapper induction Wrapper induction is another type of rule based method which is aimed at structured and semi-structured documents such as web pages. A wrapper is an extraction procedure, which consists of a set extraction rules and also program codes required to apply these rules. Wrapper induction is a technique for automatically learning the wrappers. Given a training data set, the induction algorithm learns a wrapper for extracting the target information. Several research works have been studied. The typical wrapper systems include WIEN (Kushmerick, 1997), Stalker (Muslea, 1998), and BWI (Freitag, 2000). Here, we use WIEN and BWI as examples in explaining the principle of wrapper induction. WIEN WIEN (Kushmerick, 1997) is the first wrapper induction system. An example of the wrapper defined in WIEN is shown in Figure 2, which aims to extract “Country” and “Area Code” from the two HTML pages: D1 and D2. D1: <B>Congo</B> <I>242</I><BR> D2: <B>Egypt</B> <I>20</I><BR> Rule: *‘<B>’(*)‘</B>’*‘<I>’(*)‘</I>’ Output: Country_Code {Country@1}{AreaCode@2} Figure 2. Example of wrapper induction The rule in Figure 2 has the following meaning: ignore all characters until you find the first occurrence of ‘<B>’ and extract the country name as the string that ends at the first ‘</B>’. Then ignore all characters until ‘<I>’ is found and extract the string that ends at ‘</I>’. 
----------------------------------------

CHUNK 324
File: unknown
Size: 324 chars
Content:
----------------------------------------
In order to extract the information about the other country names and area codes, the rule is applied repeatedly until it fails to match. In the example of Figure 2, we can see that the WIEN rule can be successfully to be applied to both documents D1 and D2. The rule defined above is an instance of the so called LR class. 
----------------------------------------

CHUNK 325
File: unknown
Size: 973 chars
Content:
----------------------------------------
A LR wrapper is defined as a vector <l , r , ..., l , r > of 2K delimiters, with each pair <l, r> 1 1 k k i i corresponding to one type of information. The LR wrapper requires that resources format their pages in a very simple manner. Specifically, there must exist delimiters that reliably indicate the left- and right-hand boundaries of the fragments to be extracted. The classes HLRT, OCLR, and HOCLRT are extensions of LR that use document head and tail delimiters, tuple delimiters, and both of them, respectively. The algorithm for learning LR wrappers (i.e. learn ) is shown in Figure 3. LR In Figure 3, E represents the example set; notation cands(k, E) represent the l candidates for delimiter l given the example set E. The candidates are generated by k enumerating the suffixes of the shortest string occurring to the left of each instance of attribute k in each example; valid(u, k, E) refers to the constraints to validate a l candidate u for delimiter l . k 7
----------------------------------------

CHUNK 326
File: unknown
Size: 1987 chars
Content:
----------------------------------------
procedure learn (examples E) LR { for each 1≤k≤K for each u∈cands(k, E) l if valid(u, k, E) then l ←u and terminate this loop l k for each 1≤k≤K for each u∈cands(k, E) r if valid(u, k, E) then r ←u and terminate this loop r k return LR wrapper <l , r, ..., l, r> 1 1 k k } Figure 3. The learn algorithm LR LR wrapper class is the simplest wrapper class. See (Kushmerick, 2000) for variant wrapper classes. Stalker (Muslea, 1998; Muslea, 1999a) is another wrapper induction system that performs hierarchical information extraction. It can be used to extract data from such documents with multiple levels. In Stalker, rules are induced by a covering algorithm which tries to generate rules until all instances of an item are covered and returns a disjunction of the found rules. A Co-Testing approach has been also proposed to support active learning in Stalker. See (Muslea, 2003) for details. BWI The Boosted Wrapper Induction (BWI) system (Freitag, 2000) targets at making wrapper induction techniques suitable for free text, which uses boosting to generate and combine the predictions from numerous extraction patterns. In BWI, a document is treated as a sequence of tokens, and the IE task is to identify the boundaries of different type of information. Let indices i and j denote the boundaries, we can use <i, j> to represent an instance. A wrapper W = <F, A, H> learned by BWI consists of two sets of patterns that are used respectively to detect the start and the end boundaries of an instance. Here F = {F , F , ..., F } identifies the start boundaries and A = {A , A , ..., A } identifies the 1 2 T 1 2 T end boundaries; and a length function H(k) that estimates the maximum-likelihood probability that the field has length k. To perform extraction using the wrapper W, every boundary i in a document is first given a “start” score F(i)=∑ C F (i) and an “end” score A(i)=∑ C A (i). Here, k Fk k k Ak k C is the weight for F , and F (i) = 1 if i matches F , otherwise F (i) = 0. 
----------------------------------------

CHUNK 327
File: unknown
Size: 407 chars
Content:
----------------------------------------
For A(i), Fk k k k k the definition is similar. W then classifies text fragment <i, j> as follows: ⎧1 if F(i)A(j)H(j−i)>τ (1) W(i, j)=⎨ ⎩0 otherwise where τ is a numeric threshold. Learning a wrapper W involves determining F, A, and H. The function H reflects the prior probability of various field lengths. BWI estimates these probabilities by constructing a frequency histogram H(k) recording the number 8
----------------------------------------

CHUNK 328
File: unknown
Size: 1244 chars
Content:
----------------------------------------
of fields of length k occurring in the training set. To learn F and A, BWI boosts LearnDetector, an algorithm for learning a single detector. Figure 4 shows the learning algorithm in BWI. procedure BWI (example sets S and E) { F ←AdaBoost(LearnDetector, S) A←AdaBoost(LearnDetector, E) H ←field length histogram from S and E return wrapper W = <F, A, H> } Figure 4. The BWI algorithm In BWI, AdaBoost algorithm runs in iterations. In each iteration, it outputs a weak learner (called hypotheses) from the training data and also a weight for the learner representing the percentage of the correctly classified instances by applying the weak learner to the training data. AdaBoost simply repeats this learn-update cycle T times, and then returns a list of the learned weak hypotheses with their weights. BWI invokes LearnDetector (indirectly through AdaBoost) to learn the “fore” detectors F, and then T more times to learn the “aft” detectors A. LearnDetector iteratively builds from a empty detector. At each step, LearnDetector searches for the best extension of length L (a lookahead parameter) or less to the prefix and suffix of the current detector. The procedure returns when no extension yields a better score than the current detector. 
----------------------------------------

CHUNK 329
File: unknown
Size: 389 chars
Content:
----------------------------------------
More detailed experiments and results analysis about BWI is discussed in (Kauchak, 2004). Classification based Extraction Methods In this section, we introduce another principled approach to information extraction using supervised machine learning. The basic idea is to cast information extraction problem as that of classification. In this section, we will describe the method in detail. 
----------------------------------------

CHUNK 330
File: unknown
Size: 823 chars
Content:
----------------------------------------
We will also introduce several improving efforts to the approach. Classification model Let us first consider a two class classification problem. Let {(x , y ), ... , (x , y )} 1 1 n n be a training data set, in which x denotes an instance (a feature vector) and i y ∈{−1,+1} denotes a classification label. A classification model usually consists of i two stages: learning and prediction. In learning, one attempts to find a model from the labeled data that can separate the training data, while in prediction the learned model is used to identify whether an unlabeled instance should be classified as +1 or -1. (In some cases, the prediction results may be numeric values, e.g. ranging from 0 to 1. Then an instance can be classified using some rules, e.g. classified as +1 when the prediction value is larger than 0.5.) 9
----------------------------------------

CHUNK 331
File: unknown
Size: 2155 chars
Content:
----------------------------------------
Support Vector Machines (SVMs) is one of the most popular methods for classification. Now, we use SVM as example to introduce the classification model (Vapnik, 1998). Support vector machines (SVMs) are linear functions of the form f(x) = wTx + b, where wTx is the inner product between the weight vector w and the input vector x. The main idea of SVM is to find an optimal separating hyper-plane that maximally separates the two classes of training instances (more precisely, maximizes the margin between the two classes of instances). The hyper-plane then corresponds to a classifier (linear SVM). The problem of finding the hyper-plane can be stated as the following optimization problem: 1 (2) Minimize: wTw 2 s.t.:y (wTx +b)≥1,i=1,2,...,n i i To deal with cases where there may be no separating hyper-plan due to noisy labels of both positive and negative training instances, the soft margin SVM is proposed, which is formulated as: 1 n (3) Minimize: wTw+C∑ξ 2 i i=1 s.t.:y (wTx +b)≥1−ξ,i=1,2,...,n i i i where C≥0 is the cost parameter that controls the amount of training errors allowed. It is theoretically guaranteed that the linear classifier obtained in this way has small generalization errors. Linear SVM can be further extended into non-linear SVMs by using kernel functions such as Gaussian and polynomial kernels (Boser, 1992; Schölkopf, 1999; Vapnik, 1999). When there are more than two classes, we can adopt the “one class versus all others” approach, i.e., take one class as positive and the other classes as negative. Boundary detection using classification model We are using a supervised machine learning approach to IE, so our system consists of two distinct phases: learning and extracting. In the learning phase our system uses a set of labeled documents to generate models which we can use for future predictions. The extraction phase takes the learned models and applies them to new unlabelled documents using the learned models to generate extractions. The method formalizes the IE problem as a classification problem. It is aimed at detecting the boundaries (start boundary and end boundary) of a special type of information. 
----------------------------------------

CHUNK 332
File: unknown
Size: 613 chars
Content:
----------------------------------------
For IE from text, the basic unit that we are dealing with can be tokens or text-lines in the text. (Hereafter, we will use token as the basic unit in our explanation.) Then we try to learn two classifiers that are respectively used to identify the boundaries. The instances are all tokens in the document. All tokens that begin with a start-label are positive instances for the start classifier, while all the other tokens become negative instances for this classifier. Similarly, the positive instances for the end classifier are the last tokens of each end-label, and the other tokens are negative instances. 10
----------------------------------------

CHUNK 333
File: unknown
Size: 288 chars
Content:
----------------------------------------
Start classifier Start Not start Dr. Trinkle's primary research interests lie in the areas of robotic manipulation End End classifier Not end Learning Extracting Start classifier Professor Steve Skiena will be at CMU Monday, January 13, and Tuesday, February 14. End classifier Figure 5. 
----------------------------------------

CHUNK 334
File: unknown
Size: 2196 chars
Content:
----------------------------------------
Example of Information Extraction as classification Figure 5 gives an example of IE as classification. There are two classifiers – one to identify starts of target text fragments and the other to identify ends of text fragments. Here, the classifiers are based on token only (however other patterns, e.g. syntax, can also be incorporated into). Each token is classified as being a start or non-start and an end or non-end. When we classify a token as a start, and also classify one of the closely following token as an end, we view the tokens between these two tokens as a target instance. In the example, the tokens “Dr. Trinkle’s” is annotated as a “speaker” and thus the token “Dr.” is a positive instance and the other tokens are as negative instances in the speaker-start classifier. Similarly, the token “Trinkle’s” is a positive instance and the other tokens are negative instances in the speaker-end classifier. The annotated data is used to train two classifiers in advance. In the extracting stage, the two classifiers are applied to identify the start token and the end token of the speaker. In the example, the tokens “Professor”, “Steve”, and “Skiena” are identified as two start tokens by the start classifier and one end token by the end classifier. Then, we combine the identified results and view tokens between the start token and the end token as a speaker. (i.e. “Professor Steve Skiena” is outputted as a speaker) In the extracting stage, we apply the two classifiers to each token to identify whether the token is a “start”, “end”, neither, or both. After the extracting stage, we need to combine the starts and the ends predicted by the two classifiers. We need to decide which of the starts (if there exist more than one starts) to match with which of the ends (if there exist more than one ends). For the combination, a simple method is to search for an end from a start and then view the tokens between the two tokens as the target. If there exist two starts and only one end (as the example in Figure 5), then we start the search progress from the first start and view the tokens between the first token and the end token (i.e. “Professor Steve Skiena”) as the target. 
----------------------------------------

CHUNK 335
File: unknown
Size: 770 chars
Content:
----------------------------------------
However, in some applications, the simple combination method may not yield good results. Several works have been conducted to enhance the combination. For example, Finn et al propose a histogram model (Finn, 2004; Finn, 2006). In Figure 5, there are two possible extractions: “Professor Steve Skiena” and “Steve Skiena”. The histogram model estimates confidence as C * C * P(|e - s|). Here C is the confidence of the s e s start prediction and C is the confidence of the end prediction. (For example, with e Naïve Bayes, we can use the posterior probability as the confidence; with SVM, we can use the distance of the instance to the hyper-plane as the confidence.) P(|e - s|) is the probability of a text fragment of that length which we get from the training data. 11
----------------------------------------

CHUNK 336
File: unknown
Size: 424 chars
Content:
----------------------------------------
Finally, the method selects the text fragment with the highest confidence as output. To summarize, this IE classification approach simply learns to detect the start and the end of text fragments to be extracted. It treats IE as a standard classification task, augmented with a simple mechanism to combine the predicted start and end tags. Experiments indicate that this approach generally has high precision but low recall. 
----------------------------------------

CHUNK 337
File: unknown
Size: 764 chars
Content:
----------------------------------------
This approach can be viewed as that of one-level boundary classification (Finn, 2004). Many approaches can be used to training the classification models, for example, Support Vector Machines (Vapnik, 1998), Maximum Entropy (Berger, 1996), Adaboost (Shapire, 1999), and Voted Perceptron (Collins, 2002). Enhancing IE by a two-level boundary classification model Experiments on many data sets and in several real-world applications show that the one-level boundary classification approach can competitive with the start-of-the-art rule learning based IE systems. However, as the classifiers are built on a very large number of negative instances and a small number of positive instances, the prior probability that an arbitrary instance is a boundary is very small. 
----------------------------------------

CHUNK 338
File: unknown
Size: 2324 chars
Content:
----------------------------------------
This gives a model that has very high precision. Because the prior probability of predicting a tag is so low, and because the data is highly imbalanced, when we actually do prediction for a tag, it is very likely that the prediction is correct. The one-level model is therefore much more likely to produce false negatives than false positives (high precision). To overcome the problem, a two-level boundary classification approach has been proposed by (Finn, 2004). The intuition behind the two-level approach is as follows. At the first level, the start and end classifiers have high precision. To make a prediction, both the start classifier and the end classifier have to predict the start and end respectively. In many cases where we fail to extract a fragment, one of these classifiers made a prediction, but not the other. The second level assumes that these predictions by the first level are correct and is designed to identify the starts and ends that we failed to identify at the first level. The second-level models are learned from training data in which the prior probability that a given instance is a boundary is much higher than for the one-level learner. This “focused” training data is constructed as follows. When building the second-level start model, we take only the instances that occur a fixed distance before an end tag. Similarly, for the second-level end model, we use only instances that occur a fixed distance after a start tag. For example, an second-level window of size 10 means that the second-level start model is built using only 10 instances that occur before an end-tag in the training data, while the second-level end model is built using only those instances that occur in the 10 instances after a start tag in the training data. Note that these second-level instances are encoded in the same way as for the first-level; the difference is simply that the second-level learner is only allowed to look at a small subset of the available training data. Figure 6 shows an example of the IE using the two-level classification models. In the example of Figure 6, there are also two stages: learning and extracting. In learning, the tokens “Dr.” and “Trinkle’s” are the start and the end boundaries of a speaker respectively. For training the second-level start and end classifiers. We use 12
----------------------------------------

CHUNK 339
File: unknown
Size: 525 chars
Content:
----------------------------------------
window size as three and thus three instances after the start are used to train the end classifier and three instances before the end are used to train the start classifier. In the example, the three tokens “Trinkle’s”, “primary”, and “research” are instances of the second-level end classifier and the token “Dr.” is an instance of the second-level start classifier. Note, in this way, the instances used for training the second-level classifiers are only a subset of the instances for training the first-level classifiers. 
----------------------------------------

CHUNK 340
File: unknown
Size: 2641 chars
Content:
----------------------------------------
These second-level instances are encoded in the same way as for the first-level. When extracting, the second-level end classifier is only applied to the three tokens following the token which the first-level classifier predicted as a start and the token itself. Similarly the second-level start classifier is only applied to instances predicted as an end by the first-level classifier and the three preceding tokens. Instances of the end classifier The second-level start classifier Start Dr. Trinkle's primary research interests lie in the areas of robotic manipulation Not start The second-level End end classifier Instances of the Learning start classifier Not end Extracting Identified as start by the first-level start classifier Professor Steve Skiena will be at CMU Monday, January 13, and Tuesday, February 14. Predict using the second- level end classifier Figure 6. Example of Information Extraction by the two-level classification models In the exacting stage of the example, the token “Professor” is predicted as a start by the first-level start classifier and no token is predicted as the end in the first-level model. Then we can use the second-level end classifier to make prediction for the three following tokens. This second-level classification models are likely to have much higher recall but lower precision. If we were to blindly apply the second-level models to the entire document, it would generate a lot of false positives. Therefore, the reason we can use the second-level models to improve performance is that we only apply it to regions of documents where the first-level models have made a prediction. Specifically, during extraction, the second-level classifiers use the predictions of the first-level models to identify parts of the document that are predicted to contain targets. Figure 7 shows the extracting processing flow in the two-level classification approach. Given a set of documents that we want to extract from, we convert these documents into a set of instances and then apply the first-level models for start and end to the instances and generate a set of predictions for starts and ends. The first-level predictions are then used to guide which instances we need apply the second-level classifiers to. We use the predictions of the first-level end model to decide which instances to apply the second-level start model to, and we use the predictions of the first-level start model to decide which instances to apply the second-level end model to. Applying the second-level models to the selected instances gives us a set of predictions which we pass to the combination to output our extracted 13
----------------------------------------

CHUNK 341
File: unknown
Size: 2122 chars
Content:
----------------------------------------
results. The intuition behind the two-level approach is that we use the unmatched first-level predictions (i.e. when we identify either the start or the end but not the other) as a guide to areas of text that we should look more closely at. We use more focused classifiers that are more likely to make a prediction on areas of text where it is highly likely that an unidentified fragment exists. These classifiers are more likely to make predictions due to a much lower data imbalance so they are only applied to instances where we have high probability of a fragment existing. As the level of imbalance falls, the recall of the model rises while precision falls. We use the second-level classifiers to lookahead/lookback instances in a fixed windows size and obtain a subset of the instances in the first-level classifier. Extraction results Prediction results combination Start predictions of End predictions of second second level level Start Model of End Model of second level second level Start predictions of first End predictions of first level level Start Model of End Model of first first level level The first-level start The first-level end classifier classifier Test data Figure 7. Extracting processing flow in the two-level classification approach This enables us to improve recall without hurting precision by identifying the missing complementary tags for orphan predictions. If we have 100% precision at first-level prediction then we can improve recall without any corresponding drop in precision. In practice, the drop in precision is proportional to the number of incorrect prediction at the first-level classification. Enhancing IE by unbalance classification model Besides the two-level boundary classification approach, we introduce another approach to deal with the problem so as to improve performance of the classification based method. As the classifiers are built on a very large number of negative instances and a small number of positive instances, the prior probability that an arbitrary instance is a boundary is very small. This gives a model that has very high precision, but low recall. 
----------------------------------------

CHUNK 342
File: unknown
Size: 156 chars
Content:
----------------------------------------
In this section, we introduce an approach to the problem using an unbalanced classification model. The basic idea of the approach is to design a specific 14
----------------------------------------

CHUNK 343
File: unknown
Size: 2433 chars
Content:
----------------------------------------
classification method that is able to learn a better classifier on the unbalanced data. We have investigated the unbalanced classification model of SVMs (Support Vector Machines). Using the same notations in Section 2.2.1, we have the unbalanced classification model: (4) 1 n+ n− Minimize: wTw+C ∑ξ+C ∑ξ 2 1 i 2 i i=1 i=1 s.t.:y (wTx +b)≥1−ξ,i=1,2,...,n i i i here, C and C are two cost parameters used to control the training errors of positive 1 2 examples and negative examples respectively. For example, with a larger C and a 1 small C , we can obtain a classification model that attempts penalize false positive 2 examples more than false negative examples. The model can actually increase the probability of examples to be predicted as positive, so that we can improve the recall while likely hurting the precision, which is consistent with the method of two-level classification. Intuition shows that in this way we can control the trade-off between the problem of high precision and low recall and the training errors of the classification model. The model obtained by this formulation can perform better than the classical SVM model in the case of a large number of negative instances and a small number of positive instances. To distinguish this formulation from the classical SVM, we call the special formulation of SVM as Unbalanced-SVM. See also (Morik, 1999; Li, 2003) for details. Unbalanced-SVM enables us to improve the recall by adjusting the two parameters. We need to note that the special case of SVM might hurt the precision while improving the recall. The most advantage of the model is that it can achieve a better trade-off between precision and recall. Sequential Labeling based Extraction Methods Information extraction can be cast as a task of sequential labeling. In sequential labeling, a document is viewed as a sequence of tokens, and a sequence of labels are assigned to each token to indicate the property of the token. For example, consider the nature language processing task of labeling words of a sentence with their corresponding Part-Of-Speech (POS). In this task, each word is labeled with a tag indicating its appropriate POS. Thus the inputting sentence “Pierre Vinken will join the board as a nonexecutive director Nov. 29.” will result in an output as: [NNP Pierre] [NNP Vinken] [MD will] [VB join] [DT the] [NN board] [IN as] [DT a] [JJ nonexecutive] [NN director] [NNP Nov.] [CD 29] [. 
----------------------------------------

CHUNK 344
File: unknown
Size: 495 chars
Content:
----------------------------------------
.] Formally, given an observation sequence x = (x , x ,..., x ), the information 1 2 n extraction task as sequential labeling is to find a label sequence y* = (y , y ,..., y ) 1 2 n that maximizes the conditional probability p(y|x), i.e., y* = argmax p(y|x) (5) y Different from the rule learning and the classification based methods, sequential labeling enables describing the dependencies between target information. The dependencies can be utilized to improve the accuracy of the extraction. 
----------------------------------------

CHUNK 345
File: unknown
Size: 9 chars
Content:
----------------------------------------
Hidden 15
----------------------------------------

CHUNK 346
File: unknown
Size: 546 chars
Content:
----------------------------------------
Markov Model (Ghahramani, 1997), Maximum Entropy Markov Model (McCallum, 2000), and Conditional Random Field (Lafferty, 2001) are widely used sequential labeling models. For example, a discrete Hidden Markov Model is defined by a set of output symbols X (e.g. a set of words in the above example), a set of states Y (e.g. a set of POS in the above example), a set of probabilities for transitions between the states p(y|y), and a probability distribution on output symbols for each state p(x|y). An i j i i observed sampling of the process (i.e. 
----------------------------------------

CHUNK 347
File: unknown
Size: 1020 chars
Content:
----------------------------------------
the sequence of output symbols, e.g. “Pierre Vinken will join the board as a nonexecutive director Nov. 29.” in the above example) is produced by starting from some initial state, transitioning from it to another state, sampling from the output distribution at that state, and then repeating these latter two steps. The best label sequence can be found using Viterbi algorithm. Generative model Generative models define a joint probability distribution p(X, Y) where X and Y are random variables respectively ranging over observation sequences and their corresponding label sequences. In order to calculate the conditional probability p(y|x), Bayesian rule is employed: p(x,y) y*=argmax p(y|x)=argmax (6) y y p(x) Hidden Markov Models (HMMs) (Ghahramani, 1997) are one of the most common generative models currently used. In HMMs, each observation sequence is considered to have been generated by a sequence of state transitions, beginning in some start state and ending when some pre-designated final state is reached. 
----------------------------------------

CHUNK 348
File: unknown
Size: 1354 chars
Content:
----------------------------------------
At each state an element of the observation sequence is stochastically generated, before moving to the next state. In the case of POS tagging, each state of the HMM is associated with a POS tag. Although POS tags do not generate words, the tag associated with any given word can be considered to account for that word in some fashion. It is, therefore, possible to find the sequence of POS tags that best accounts for any given sentence by identifying the sequence of states most likely to have been traversed when “generating” that sequence of words. The states in an HMM are considered to be hidden because of the doubly stochastic nature of the process described by the model. For any observation sequence, the sequence of states that best accounts for that observation sequence is essentially hidden from an observer and can only be viewed through the set of stochastic processes that generate an observation sequence. The principle of identifying the most state sequence that best accounts for an observation sequence forms the foundation underlying the use of finite-state models for labeling sequential data. Formally, an HMM is fully defined by (cid:122) A finite set of states Y. (cid:122) A finite output alphabet X. (cid:122) A conditional distribution p(y’|y) representing the probability of moving from state y to state y’, where y, y’∈Y. 16
----------------------------------------

CHUNK 349
File: unknown
Size: 1311 chars
Content:
----------------------------------------
(cid:122) An observation probability distribution p(x|y) representing the probability of emitting observation x when in state y, where x∈X, y∈Y. (cid:122) An initial state distribution p(y), y∈Y. From the definition of HMMs, we can see that the probability of the state at time t depends only on the state at time t-1, and the observation generated at time t only depends on the state of the model at time t. Figure 8 shows the structure of a HMM. Figure 8. Graphic structure of first-order HMMs These conditional independence relations, combined with the probability chain rule, can be used to factorize the joint distribution over a state sequence y and observation sequence x into the product of a set of conditional probabilities: n p(y,x)= p(y )p(x | y )∏ p(y | y )p(x | y ) (7) 1 1 1 t t−1 t t t=2 In supervised learning, the conditional probability distribution p(y|y ) and t t-1 observation probability distribution p(x|y) can be gained with maximum likelihood. While in unsupervised learning, there is no analytic method to gain the distributions directly. Instead, Expectation Maximization (EM) algorithm is employed to estimate the distributions. Finding the optimal state sequence can be efficiently performed using a dynamic programming such as Viterbi algorithm. Limitations of generative models. 
----------------------------------------

CHUNK 350
File: unknown
Size: 1455 chars
Content:
----------------------------------------
Generative models define a joint probability distribution p(X, Y) over observation and label sequences. This is useful if the trained model is to be used to generate data. However, to define a joint probability over observation and label sequences, a generative model needs to enumerate all possible observation sequences, typically requiring a representation in which observations are task-appropriate atomic entities, such as words or nucleotides. In particular, it is not practical to represent multiple interacting features or long-range dependencies of the observations, since the inference problem for such models is intractable. Therefore, generative models must make strict independence assumptions in order to make inference tractable. In the case of an HMM, the observation at time t is assumed to depend only on the state at time t, ensuring that each observation element is treated as an isolated unit, independent from all other elements in the sequence (Wallach, 2002). In fact, most sequential data cannot be accurately represented as a set of isolated elements. Such data contain long-distance dependencies between observation elements and benefit from being represented in by a model that allows such dependencies and enables observation sequences to be represented by non-independent overlapping features. For example, in the POS task, when tagging a word, information such as the words surrounding the current word, the previous tag, 17
----------------------------------------

CHUNK 351
File: unknown
Size: 243 chars
Content:
----------------------------------------
whether the word begins with a capital character, can be used as complex features and help to improve the tagging performance. Discriminative models provide a convenient way to overcome the strong independence assumption of generative models. 
----------------------------------------

CHUNK 352
File: unknown
Size: 1811 chars
Content:
----------------------------------------
Discriminative models Instead of modeling joint probability distribution over observation and label sequences, discriminative models define a conditional distribution p(y|x) over observation and label sequences. This means that when identifying the most likely label sequence for a given observation sequence, discriminative models use the conditional distribution directly, without bothering to make any dependence assumption on observations or enumerate all the possible observation sequences to calculate the marginal probability p(x). Maximum Entropy Markov Models (MEMMs) MEMMs (McCallum, 2000) are a form of discriminative models for labeling sequential data. MEMMs consider observation sequences to be conditioned upon rather than generated by the label sequence. Therefore, instead of defining two types of distribution, a MEMM has only a single set of separately trained distributions of the form: p(y'| x)= p(y'| y,x) (8) which represent the probability of moving from state y to y’ on observation x. The fact the each of these functions is specific to a given state means that the choice of possible states at any given instant in time t+1 depends only on the state of the model at time t. Figure 9 show the graphic structure of MEMMs. Figure 9. Graphic structure of first-order MEMMs Given an observation sequence x, the conditional probability over label sequence y is given by n p(y| x)= p(y | x )∏ p(y | y ,x ) (9) 1 1 t t−1 t−1 t=2 Treating observations as events to be conditioned upon rather than generated means that the probability of each transition may depend on non-independent, interacting features of the observation sequence. Making use of maximum entropy frame work and defining each state-observation transition function to be a log-linear model, equation (8) can be calculated as 18
----------------------------------------

CHUNK 353
File: unknown
Size: 245 chars
Content:
----------------------------------------
1 p(y'|x)= exp(∑λ f (y',x)) (10) Z(y,x) k k k where Z(y, x) is a normalization factor; λ are parameters to be estimated and f are k k feature functions. The parameters can be estimated using Generalized Iterative Scaling (GIS) (McCallum, 2000). 
----------------------------------------

CHUNK 354
File: unknown
Size: 619 chars
Content:
----------------------------------------
Each feature function can be represented as a binary feature. For example: ⎧1 if b(x) is true and y = y' f(y',x)=⎨ (11) ⎩0 otherwise Despite the differences between MEMMs and HMMs, there is still an efficient dynamic programming solution to the classic problem of identifying the most likely label sequence given an observation sequence. A variant Viterbi algorithm is given by (McCallum, 2000). Label bias problem. Maximum Entropy Markov Models define a set of separately trained per-state probability distributions. This leads to an undesirable behavior in some situations, named label bias problem (Lafferty, 2001). 
----------------------------------------

CHUNK 355
File: unknown
Size: 1295 chars
Content:
----------------------------------------
Here we use an example to describe the label bias problem. The MEMM in Figure 10 is designed to shallow parse the sentences: (1) The robot wheels Fred round. (2) The robot wheels are round. Figure 10. MEMM designed for shallow parsing Consider when shallow parsing the sentence (1). Because there is only one outgoing transition from state 3 and 6, the per-state normalization requires that p(4|3, Fred) = p(7|6, are) = 1. Also it’s easy to obtain that p(8|7, round) = p(5|4, round) = p(2|1, robot) = p(1|0, The) = 1, etc. Now, given p(3|2, wheels) = p(6|2, wheels) = 0.5, by combining all these factors, we obtain p(0123459|The robot wheels Fred round.) = 0.5, p(0126789|The robot wheels Fred round.) = 0.5. Thus the MEMM ends up with two possible state sequences 0123459 and 0126789 with the same probability independently of the observation sequence. It’s impossible for the MEMM to tell which one is the most likely state sequence over the given sentence. Likewise, given p(3|2, wheels) < p(6|2, wheels), MEMM will always choose the bottom path despite what the preceding words and the following words are in the observation sequence. The label bias problem occurs because a MEMM uses per-state exponential model for the conditional probability of the next states given the current state. 19
----------------------------------------

CHUNK 356
File: unknown
Size: 1844 chars
Content:
----------------------------------------
Conditional Random Fields (CRFs) CRFs are undirected graphical model trained to maximize a conditional probability. CRFs can be defined as follows: CRF Definition. Let G = (V, E) be a graph such that Y=(Y ) , so that Y is indexed v v∈V by the vertices of G. Then (X, Y) is a conditional random field in case, when conditioned on X, the random variable Y obey the Markov property with respect to v the graph: p(Y |X, Y , w≠v) = p(Y |X, Y , w∽v), where w∽v means that w and v are v w v w neighbors in G. A CRF is a random field globally conditioned on the observation X. Linear-chain CRFs were first introduced by Lafferty et al (2001). Figure 11 shows the graphic structure of the linear-chain CRFs. Figure 11. Graphic structure of linear-chain CRFs By the fundamental theorem of random fields (Harmmersley, 1971), the conditional distribution of the labels y given the observations data x has the form 1 T p (y|x)= exp(∑∑λ ⋅ f (y ,y ,x,t)) (12) λ Z (x) k k t−1 t λ t=1 k where Z (x) is the normalization factor, also known as partition function, which has λ the form T Z (x)=∑exp(∑∑λ ⋅ f (y ,y ,x,t)) (13) λ k k t−1 t y t=1 k where f (y , y, x, t) is a feature function which can be both real-valued and k t-1 t binary-valued. The feature functions can measure any aspect of a state transition, y → y , and the observation sequence, x, centered at the current time step t. λ t−1 t k corresponds to the weight of the feature f . k The most probable labeling sequence for an input x y*=argmax p (y|x) (14) y λ can be efficiently calculated by dynamic programming using Viterbi algorithm. We can train the parameters λ=(λ , λ , ...) by maximizing the likelihood of a given 1 2 training set T ={(x ,y )}N : k k k=1 N T L = ∑(∑∑λ ⋅ f (y ,y ,x ,t)−logZ (x )) (15) λ k k t−1 t i λ i i=1 t=1 k Many methods can be used to do the parameter estimation. 
----------------------------------------

CHUNK 357
File: unknown
Size: 18 chars
Content:
----------------------------------------
The traditional 20
----------------------------------------

CHUNK 358
File: unknown
Size: 2934 chars
Content:
----------------------------------------
maximum entropy learning algorithms, such as GIS, IIS can be used to train CRFs (Darroch, 1972). In addition to the traditional methods, preconditioned conjugate-gradient (CG) (Shewchuk, 1994) or limited-memory quasi-Newton (L-BFGS) (Nocedal, 1999) have been found to perform better than the traditional methods (Sha, 2004). The voted perceptron algorithm (Collins, 2002) can also be utilized to train the models efficiently and effectively. To avoid overfitting1, log-likelihood is often penalized by some prior distribution over the parameters. Empirical distributions such as Gaussian prior, exponential prior, and hyperbolic-L prior can be used, and empirical experiments suggest that Gaussian 1 prior is a safer prior to use in practice (Chen, 1999). CRF avoids the label bias problem because it has a single exponential model for the conditional probability of the entire sequence of labels given the observation sequence. Therefore, the weights of different features at different states can be traded off against each other. Sequential labeling based extraction methods By casting information extraction as sequential labeling, a set of labels need to be defined first according to the extraction task. For example, in metadata extraction from research papers (Peng, 2004), labels such as TITLE, AUTHOR, EMAIL, and ABSTRACT are defined. A document is viewed as an observation sequence x. The observation unit can be a word, a text line, or any other unit. Then the task is to find a label sequence y that maximize the conditional probability p(y|x) using the models described above. In generative models, there is no other features can be utilized except the observation itself. Due to the conditional nature, discriminative models provide the flexibility of incorporating non-independent, arbitrary features as input to improve the performance. For example, in the task of metadata extraction from research papers, with CRFs we can use as features not only text content, but also layout and external lexicon. Empirical experiments show that the ability to incorporate non-independent, arbitrary features can significantly improve the performance. On the other hand, the ability to incorporate non-independent, arbitrary features of discriminative models may sometimes lead to too many features and some of the features are of little contributions to the model. A feature induction can be performed when training the model to obtain the features that are most useful for the model (McCallum, 2003). Non-linear Conditional Random Fields Conditional Random Fields (CRFs) are the state-of-the-art approaches in information extraction taking advantage of the dependencies to do better extraction, compared with HMMs (Ghahramani, 1997) and MEMMs (McCallum, 2000). However, the previous linear-chain CRFs only model the linear-dependencies in a sequence of information, and is not able to model the other kinds of dependencies (e.g. 
----------------------------------------

CHUNK 359
File: unknown
Size: 2 chars
Content:
----------------------------------------
21
----------------------------------------

CHUNK 360
File: unknown
Size: 1117 chars
Content:
----------------------------------------
non-linear dependencies) (Lafferty, 2001; Zhu, 2005). In this section, we will discuss several non-linear conditional random field models. Condition random fields for relational learning HMMs, MEMMs and linear-chain CRFs can only model dependencies between neighboring labels. But sometimes it is important to model certain kinds of long-range dependencies between entities. One important kind of dependency within information extraction occurs on repeated mentions of the same field. For example, when the same entity is mentioned more than once in a document, such as a person name Robert Booth, in many cases, all mentions have the same label, such as SEMINAR-SPEAKER. An IE system can take advantage of this fact by favoring labelings that treat repeated words identically, and by combining feature from all occurrences so that the extraction decision can be made based on global information. Furthermore, identifying all mentions of an entity can be useful in itself, because each mention might contain different useful information. The skip-chain CRF is proposed to address this (Sutton, 2005; Bunescu, 2005b). 
----------------------------------------

CHUNK 361
File: unknown
Size: 1448 chars
Content:
----------------------------------------
The skip-chain CRF is essentially a linear-chain CRF with additional long-distance edges between similar words. These additional edges are called skip edges. The features on skip edges can incorporate information from the context of both endpoints, so that strong evidence at one endpoint can influence the label at the other endpoint. Formally, the skip-chain CRF is defined as a general CRF with two clique templates: one for the linear-chain portion, and one for the skip edges. For an input x, let C ={(u,v)}be the set of all pairs of sequence positions for which there are skip edges. The probability of a label sequence y given an x is modeled as 1 T p (y|x)= exp(∑∑λ ⋅ f (y ,y ,x,t)+ ∑ ∑λ⋅ f (y ,y ,x,u,v)) (16) λ k k t−1 t l l u v Z(x) t=1 k (u,v)∈C l where Z(x) is the normalization factor, f is the feature function similar to that in k equation (12) and f is the feature function of the skip edges. λ and λ are weights of l k l the two kinds of feature functions. Because the loops in a skip-chain CRF can be long and overlapping, exact inference is intractable for the data considered. The running time required by exact inference is exponential in the size of the largest clique in the graph’s junction tree. Instead, approximate inference using loopy belief propagation is performed, such as TRP (Wainwright, 2001). Richer kinds of long-distance factor than just over pairs of words can be considered to augment the skip-chain model. 
----------------------------------------

CHUNK 362
File: unknown
Size: 326 chars
Content:
----------------------------------------
These factors are useful for modeling exceptions to the assumption that similar words tend to have similar labels. For example, in named entity recognition, the word China is as a place name when it appears alone, but when it occurs within the phrase The China Daily, it should be labeled as an organization (Finkel, 2005). 22
----------------------------------------

CHUNK 363
File: unknown
Size: 1681 chars
Content:
----------------------------------------
2D CRFs for web information extraction Zhu et al (2005) propose 2D Conditional Random Fields (2D CRFs). 2D CRFs are also a particular case of CRFs. They are aimed at extracting object information from two-dimensionally laid-out web pages. The graphic structure of a 2D CRF is a 2D grid, and it’s natural to model the 2D laid-out information. If viewing the state sequence on diagonal as a single state, a 2D CRF can be mapped to a linear-chain CRF, and thus the conditional distribution has the same form as a linear-chain CRF. Dynamic CRFs Sutton et al (2004) propose Dynamic Conditional Random Fields (DCRFs). As a particular case, a factorial CRF (FCRF) was used to jointly solve two NLP tasks (noun phrase chunking and Part-Of-Speech tagging) on the same observation sequence. Improved accuracy was obtained by modeling the dependencies between the two tasks. Tree-structure CRFs for information extraction We have investigated the problem of hierarchical information extraction and propose Tree-structured Conditional Random Fields (TCRFs). TCRFs can incorporate dependencies across the hierarchically laid-out information. We here use an example to introduce the problem of hierarchical information extraction. Figure 12 (a) give an example document, in which the underlined text are what we want to extract including two telephone numbers and two addresses. The information can be organized as a tree structure (ref. Figure 12 (b)). In this case, the existing linear-chain CRFs cannot model the hierarchical dependencies and thus cannot distinguish the office telephone number and the home telephone number from each other. Likewise for the office address and home address. 
----------------------------------------

CHUNK 364
File: unknown
Size: 118 chars
Content:
----------------------------------------
Contact Information: John Booth Office: Tel: 8765-4321 Addr: F2, A building Home: Tel: 1234-5678 Addr: No. 123, B St. 
----------------------------------------

CHUNK 365
File: unknown
Size: 695 chars
Content:
----------------------------------------
(a) Example document (b) Organized the document in tree-structure Figure 12. Example of tree-structured laid-out information To better incorporate dependencies across hierarchically laid-out information, we propose a Tree-structured Conditional Random Field (TCRF) model. We present the graphical structure of the TCRF model as a tree and reformulate the conditional distribution by defining three kinds of edge features respectively representing the parent-child dependency, child-parent dependency, and sibling dependency. As the tree structure can be cyclable, exact inference in TCRFs is expensive. We propose to use the Tree-based Reparameterization (TRP) algorithm (Wainwright, 2001) to 23
----------------------------------------

CHUNK 366
File: unknown
Size: 428 chars
Content:
----------------------------------------
compute the approximate marginal probabilities for edges and vertices. We conducted experiments on company annual reports collected from Shang Stock Exchange. On the annual reports we defined ten extraction tasks. Experimental results indicate that the TCRFs can significantly outperform the existing linear-chain CRF model (+7.67% in terms of F1-measure) for hierarchical information extraction. See (Tang, 2006b) for details. 
----------------------------------------

CHUNK 367
File: unknown
Size: 2050 chars
Content:
----------------------------------------
APPLICATIONS In this section, we introduce several extraction applications that we experienced. We will also introduce some well-known applications in this area. Information Extraction in Digital Libraries In digital libraries (DL), “metadata” is structured data for helping users find and process documents and images. With the metadata information, search engines can retrieve required documents more accurately. Scientists and librarians need use greatly manual efforts and lots of time to create metadata for the documents. To alleviate the hard labor, many efforts have been made toward the automatic metadata generation based on information extraction. Here we take Citeseer, a popular scientific literature digital library, as an example in our explanation. Citeseer is a public specialty scientific and academic DL that was created in NEC Labs, which is hosted on the World Wide Web at the College of Information Sciences and Technology, The Pennsylvania State University, and has over 700,000 documents, primarily in the fields of computer and information science and engineering (Lawrence, 1999; Han, 2003). Citeseer crawls and harvests documents on the web, extracts documents metadata automatically, and indexes the metadata to permit querying by metadata. By extending Dublin Core metadata standard, Citeseer defines 15 different meta-tags for the document header, including Title, Author, Affiliation, and so on. They view the task of automatic document metadata generation as that of labeling the text with the corresponding meta-tags. Each meta-tag corresponds to a metadata class. The extraction task is cast as a classification problem and SVM is employed to perform the classification. They show that classifying each text line into one or more classes is more efficient for meta-tagging than classifying each word, and decompose the metadata extraction problem into two sub-problems: (1) line classification and (2) chunk identification of multi-class lines. In line classification, both word and line-specific features are used. 
----------------------------------------

CHUNK 368
File: unknown
Size: 382 chars
Content:
----------------------------------------
Each line is represented by a set of word and line-specific features. A rule-based, context-dependent word clustering method is developed to overcome the problem of word sparseness. For example, an author line “Chungki Lee James E. Burns” is represented as “CapNonDictWord: :MayName: :MayName: : SingleCap: :MayName:”, after word clustering. The weight of a word-specific feature 24
----------------------------------------

CHUNK 369
File: unknown
Size: 2066 chars
Content:
----------------------------------------
is the number of times this feature appears in the line. And line-specific features are features such as “Number of the words in the line”, “The position of the line”, “The percentage of dictionary words in the line”, and so on. The classification process is performed in two steps, an independent line classification followed by an iterative contextual line classification. Independent line classification use the features described above to assign one or more classes to each text line. After that, by making use of the sequential information among lines output by the first step, an iterative contextual line classification is performed. In each iteration, each line uses the previous N and next N lines’ class information as features, concatenates them to the feature vector used in step one, and updates its class label. The procedure converges when the percentage of line with new class labels is lower than a threshold. The principle of the classification based method is the Two-level boundary classification approach as described in Section 2.2.3. After classifying each line into one or more classes, meta-tag can be assigned to lines that have only one class label. For those that have more than one class label, a further identification is employed to extract metadata from each line. The task is cast as a chunk identification task. Punctuation marks and spaces between words are considered candidate chunk boundaries. A two-class chunk identification algorithm for this task was developed and it yields an accuracy of 75.5%. For lines that have more than two class labels, they are simplified to two-class chunk identification tasks by detecting natural chunk boundary. For instance, using the positions of email and URL in the line, the three-class chunk identification can be simplified as two-class chunk identification task. The position of the email address in the following three-class line “International Computer Science Institute, Berkeley, CA94704. Email: aberer@icsi.berkeley.edu.” is a natural chunk boundary between the other two classes. 
----------------------------------------

CHUNK 370
File: unknown
Size: 171 chars
Content:
----------------------------------------
The method obtains an overall accuracy of 92.9%. It’s adopted in the DL Citeseer and EbizSearch for automatic metadata extraction. It can be also generalized to other DL. 
----------------------------------------

CHUNK 371
File: unknown
Size: 1097 chars
Content:
----------------------------------------
See (Lawrence, 1999; Han, 2003) for details. Information Extraction from Emails We also make use of information extraction methods to email data (Tang, 2005a). Email is one of the commonest means for communication via text. It is estimated that an average computer user receives 40 to 50 emails per day (Ducheneaut, 2001). Many text mining applications need take emails as inputs, for example, email analysis, email routing, email filtering, information extraction from email, and newsgroup analysis. Unfortunately, information extraction from email has received little attention in the research community. Email data can contain different types of information. Specifically, it may contain headers, signatures, quotations, and text content. Furthermore, the text content may have program codes, lists, and paragraphs; the header may have metadata information such as sender, receiver, subject, etc.; and the signature may have metadata information such as author name, author’s position, author’s address, etc. In this work, we formalize information extraction from email as that of text-block 25
----------------------------------------

CHUNK 372
File: unknown
Size: 893 chars
Content:
----------------------------------------
detection and block-metadata detection. Specifically, the problem is defined as a process of detection of different types of informative blocks (it includes header, signature, quotation, program code, list, and paragraph detections) and detection of block-metadata (it includes metadata detection of header and metadata detection of signature). We propose to conduct email extraction in a ‘cascaded’ fashion. In the approach, we perform the extraction on an email by running several passes of processing on it: first at email body level (text-block detection), next at text-content level (paragraph detection), and then at block levels (header-metadata detection and signature-metadata detection). We view the tasks as classification and propose a unified statistical learning approach to the tasks, based on SVMs (Support Vector Machines). Features used in the models have also been defined. 
----------------------------------------

CHUNK 373
File: unknown
Size: 31 chars
Content:
----------------------------------------
See (Tang, 2005a) for details. 
----------------------------------------

CHUNK 374
File: unknown
Size: 494 chars
Content:
----------------------------------------
1. From: SY <sandeep....@gmail.com> - Find messages by this author 2. Date: Mon, 4 Apr 2005 11:29:28 +0530 3. Subject: Re: ..How to do addition?? 4. Hi Ranger, 5. Your design of Matrix 6. class is not good. From: SY <sandeep....@gmail.com> - Find messages by this author Source 7. what are you doing with two Date: Mon, 4 Apr 2005 11:29:28 +0530 SentTime Header 8. matrices in a single class?make class Matrix as follows Subject: Re: ..How to do addition?? Subject Hi Ranger, Paragraph 9 10. . 
----------------------------------------

CHUNK 375
File: unknown
Size: 3125 chars
Content:
----------------------------------------
i m clp ao ssrt Mja av ta r. ii xo . {* ; Y wo itu hr t wde os i mgn a to rif c M esa it nr i ax sc il na gss le i s c ln ao sst ? m ag ko eo cd l. a w ssh Mat aa tr re ix y ao su fd oo lli on wg s Paragraph Text Content 11. public static int AnumberOfRows; 12. public static int AnumberOfColumns; i c pm l ua bp s lso i cr Mt sj a ta atv r tia i cx.i io { n. * t ; A numberOfRows; Email 13. public void inputArray() throws IOException public static int AnumberOfColumns; 1 14 5. . { InputStreamReader input = new InputStreamReader(System.in); public void inputArray() throws IOException Program Code 16. BufferedReader keyboardInput = new BufferedReader(input) { 17. } I Bn up fu ft eS retr de Ram eaR de ea r d ke er y i bn op au rt d = In n pe uw t = I n np eu wtS Btr ue fa fm erR ede Rad ee ar d( eS ry (is nte pm ut. )in); } 18. -- Sandeep Yadav 19. Tel: 011-243600808 -- Sandeep Yadav AuthorName 20. Homepage: http://www.it.com/~Sandeep/ Tel: 011-243600808 Telephone Signature Homepage: http://www.it.com/~Sandeep/ Homepage 21. On Apr 3, 2005 5:33 PM, ranger <asiri....@gmail.com> wrote: On Apr 3, 2005 5:33 PM, ranger <asiri....@gmail.com> wrote: Forwarded Message 22. > Hi... I want to perform the addtion in my Matrix class. I got the program to > Hi... I want to perform the addtion in my Matrix class. I got the program to 23. > enter 2 Matricx and diaplay them. Hear is the code of the Matrix class and > enter 2 Matricx and diaplay them. Hear is the code of the Matrix class and 24. > TestMatrix class. I'm glad If anyone can let me know how to do the addition Tnx > TestMatrix class. I'm glad If anyone can let me know how to do the addition Tnx Figure 13. Example of email Figure 14. Annotation results of the email message message Figure 13 shows an example of email that includes many typical information. Lines from 1 to 3 are a header; lines from 18 to 20 are a signature; and a forwarded message lies from line 21 to line 24. Lines from 4 to 8 are the actual text content, which should be two paragraphs, but is mistakenly separated by extra line breaks. Moreover, the header has a sender (line 1), a sent time (line 2), and a subject (line 3); the signature has an author name (line 18), a telephone (line 19), and a homepage (line 20). Figure 14 shows an ideal result of information extraction on the email in Figure 13. Within it, the text-blocks (the header, signature and the forwarded message) have been identified. The actual text content has been detected. In the text content, extra line breaks have been detected and the text has been annotated as two paragraphs. Metadata information is recognized in the identified header and the identified signature. We propose a cascaded approach for information extraction from email and cast the extraction tasks as detection tasks of different types of information blocks. We employ a unified machine learning approach in the detection tasks. The input is an email message. The implementation carries out extraction in the following steps. The identified text-blocks and other extraction results in each step will be saved for use in the later steps. 26
----------------------------------------

CHUNK 376
File: unknown
Size: 1624 chars
Content:
----------------------------------------
(1) Preprocessing. It uses patterns to recognize ‘special words’, including email address, IP address, URL, date, file directory, number (e.g. 5.42), money (e.g. $100), percentage (e.g. 92.86%), words containing special symbols (e.g. C#, .NET, .doc). (2) Forwarded message detection. It identifies forwarded messages using hard-coded rules. It views lines starting with special characters (e.g. >, |, >>) as forwarded messages. It then eliminates the identified forwarded messages for later processing. (3) Header and signature detection. It detects the header and signature (if there exist) in the email by using a classification model. It next eliminates the identified blocks (headers and signatures). (4) Metadata detection in header and signature. It uses the identified headers and signatures as input and then detects the metadata information from the headers and signatures, respectively. (5) List and program code detection. It detects list and program code (if there exist) in the email with the same approach as that in header and signature detection and removes them from the text content. After that, only natural language text remains. (6) Paragraph annotation. It identifies whether or not each line break is a paragraph ending by using a classification model. If not, it removes the line break. As a result, the text is segmented into paragraphs. The step is based on paragraph ending detection. We make use of Support Vector Machines (SVM) as the classification model (Vapnik, 1998). We use SVM-light, which is available at http://svmlight.joachims.org/. We obtain high performances in all detection tasks. 
----------------------------------------

CHUNK 377
File: unknown
Size: 1491 chars
Content:
----------------------------------------
(The F1-measuer scores range from 89.83% to 97.17%.) The extracted information from email is applied to applications of email data cleaning (Tang, 2005a) and email classification. In email data cleaning, we try to remove ‘noisy’ (irrelevant) blocks for a specific application (e.g. term extraction, a task in which base noun phrases are extracted from documents) and transform relevant text into a canonical form as that in a newspaper article. For term extraction, we identify and remove the header, signature, program code, and forwarded message. We view the remaining text as the relevant text. In the relevant text, we identify and remove extra line breaks, remove extra punctuations, and restore badly cased words. Experimental results show that the extraction based email cleaning can significantly improve the accuracy of term extraction. The improvements on precision range from +49.90% to +71.15%. See (Tang, 2005a) for details. In email classification, we are aimed at taking advantage of the extracted information to improve the performance of email classification. We evaluated the classification results on Enron Email Dataset, which is available at http://www.cs.umass.edu/~ronb/enron_dataset.html. Experimental results show that the classification performance can be significantly improved (averagely +49.02% in terms of F1-measure) by making use of the extraction results from emails. The related issues are what we are currently researching, and will be reported elsewhere. 
----------------------------------------

CHUNK 378
File: unknown
Size: 2 chars
Content:
----------------------------------------
27
----------------------------------------

CHUNK 379
File: unknown
Size: 1562 chars
Content:
----------------------------------------
Person Profile Extraction Person information management is an important topic in both research community and industrial community. A person can have different types of information: person profile (including portrait, homepage, position, affiliation, publications, and documents), contact information (including address, email, telephone, and fax number), and social network information (including person or professional relationships between persons, e.g. friend relationship). However, the information is usually hidden in heterogeneous and distributed web pages. We have investigated the problem of person information extraction. We have found that the person information is mainly hidden in person homepage, person introduction page (web page that introduces the person), person list (e.g. a faculty list), and email message (e.g. in signature). We employed the classification based method to extract the person information from the different types of web pages. More specifically, in extraction we convert a web page into a token sequence (the token can be word, punctuation, and space). Then we view each token as a candidate and define features for each candidate. Due to space limitation, we omit the details of the feature definition. After that, we use two classification models to respectively identify whether a token is the start position and whether the token is the end position for each type of information. We next view the tokens between the start token and the end token as the target. We can also use the text-line as candidate in extraction. 
----------------------------------------

CHUNK 380
File: unknown
Size: 445 chars
Content:
----------------------------------------
(a) (b) Figure 15. Personal Network Search system For learning the classification models, we have human annotators conduct annotation on the web pages. We also convert the web page into a token sequence and view each token as the candidate. Features are defined for each candidate. Finally, we learn two classification models respectively for the start position identification and the end position identification for each type of information. 28
----------------------------------------

CHUNK 381
File: unknown
Size: 1565 chars
Content:
----------------------------------------
As models, we use SVMs (Support Vector Machines) (Vapnik, 1998). Features are defined in the SVM models respectively for each type of the information. The average F1-measure obtained in extraction is 91.18%. We have developed a system based on the extracted person information, which is called ‘Personal Network Search’ (PNS shortly). In PNS, the user inputs a person name, and the system returns the information of the person. Given a person name, we first utilize Google API to get a list of relevant documents. Then a classification model is employed to identify whether or not a document in the list is really ‘related’ to the person. Next, we extract person information from the identified documents using the classification based method as described above. Figure 15 shows the snapshots of the PNS system. In Figure 15 (a), the user types a person name, and he gets a detailed description of the person. Figure 15 (b) shows the list of gathered persons in our current system. See (Tang, 2006a) for details. Table Extraction Using Conditional Random Fields Tables — textual tokens laid out in tabular form — are often used to compactly communicate information in fields and records. They have been described as “databases designed for human eyes”. Tables appear in the earliest writing on clay tablets, and in the most modern Web pages. Some make use of line-art, while others rely on white space only. They sometimes consist merely of two simple columns, other times of extremely baroque collections of headings, embedded subheadings, and varying cell sizes. 
----------------------------------------

CHUNK 382
File: unknown
Size: 992 chars
Content:
----------------------------------------
They are used in everything from government reports, to magazine articles, to academic publications. Pinto and McCallum (2003) propose a model of table extraction that richly integrates evidence from both content and layout by using Conditional Random Fields (CRFs). They describe a method that simultaneously locates tables in plain-text government statistical reports, and labels each of their constituent lines with tags such as header, sub-header, data, separator, etc. The features measure aspects of the input stream such as the percentage of alphabetic characters, the presence of regular expression matching months or years, and the degree to which white space in the current line aligns with white space in the previous line. In experiments on government reports, tables are located with 92% in terms of F1-measure, and lines are labeled with 94% accuracy — reducing error by 80% over a similarly configured hidden Markov model with the same features. See (Pinto, 2003) for details. 
----------------------------------------

CHUNK 383
File: unknown
Size: 483 chars
Content:
----------------------------------------
See also (Wang, 2002). Shallow Parsing with Conditional Random Fields Shallow parsing identifies the non-recursive cores of various phrase types in text, possibly as a precursor to full parsing or information extraction (Abney, 1991). The paradigmatic shallow parsing problem is NP chunking, which finds the non-recursive cores of noun phrases called base NPs. The pioneering work of (Ramshaw, 1995) introduced NP chunking as a machine-learning problem, with standard datasets and 29
----------------------------------------

CHUNK 384
File: unknown
Size: 2224 chars
Content:
----------------------------------------
evaluation metrics. The task was extended to additional phrase types for the CoNLL-2000 shared task (Tjong Kim Sang, 2000), which is now the standard evaluation task for shallow parsing. Sha et al (2003) employ Conditional Random Fields (CRFs) into shallow parsing. They carried out an empirical study on different sequential labeling approaches in shallow parsing. Their experimental results show that CRFs outperform all reported single-model NP chunking results on the standard evaluation dataset. They also compared different kinds of parameter estimation methods for training CRF models that confirm and strengthen previous results on shallow parsing and training methods for maximum entropy models. FUTURE RESEARCH DIRECTIONS There are a variety of promising directions for future research in applying supervised machine learning to information extraction. On the machine-learning side, it would be interesting to generalize the ideas of large-margin classification to sequence models, strengthening the results of (Collins, 2002) and leading to new optimal training algorithms with stronger guarantees against overfitting. For example, (Taskar, 2003) proposes a maximal Markov model for sequential labeling task using the maximal margin theory. In information extraction, in addition to identifying entities, an important problem is extracting specific types of relations between entities. For example, in newspaper text, one can identify that an organization is located in a particular city or that a person is affiliated with a specific organization (Zelenko, 2003); in biomedical text, one can identify that a protein interacts with another protein or that a protein is located in a particular part of the cell (Bunescu, 2005a; Craven, 1999). The entities may occur in different parts of a sentence or paragraph. New principled methods are needed to such problems to identify both the entities while identify their relations. Bunescu and Mooney (2005b) propose to use a Statistical Relational Learning (SRL) method for the complex problem. They are trying to integrate decision at different levels (e.g. different kinds of entity identification and different kinds of relations identification) into the SRL model. 
----------------------------------------

CHUNK 385
File: unknown
Size: 777 chars
Content:
----------------------------------------
Moreover, several recent projects have taken the first steps in this direction. For example, Sutton (2004) presents a dynamic version of CRF that integrates part-of-speech tagging and noun-phrase chunking into one coherent process. (Roth, 2004) presents an information extraction approach based on linear-programming that integrates recognition of entities with the identification of relations between these entities. As another future work, more applications, especially practical applications, need to be investigated. The new applications can provide rich data sources for conducting information extraction, at the same time bring big challenges to the field. This is because various applications have various characteristics, needing to use different methods to deal with. 
----------------------------------------

CHUNK 386
File: unknown
Size: 2 chars
Content:
----------------------------------------
30
----------------------------------------

CHUNK 387
File: unknown
Size: 1333 chars
Content:
----------------------------------------
CONCLUSIONS Aiming to apply methods and technologies from practical computer science such as compiler construction and artificial intelligence to the problem of processing unstructured textual data automatically, information extraction has become an important sub-discipline of language engineering, a branch of computer science. Nowadays, the significance of Information Extraction is determined by the growing amount of information available in unstructured (i.e. without metadata) form, for instance on the Internet. In this chapter, we have reviewed the information extraction methods. Specifically, we focus on the three state-of-the-art methods: rule learning based method, classification based method, and sequential labeling base method. We have explained the principle of the three methods by using several developed systems as examples. We have also introduced our research work on the information methods and their applications. We also introduced several practical application of information extraction, ranging from natural language processing to information extraction from web pages and plain texts. The rule learning based method try to exploit the regularity in language expressions of certain information to find common linguistic patterns that match these expressions. It is easy to understand by an average user. 
----------------------------------------

CHUNK 388
File: unknown
Size: 318 chars
Content:
----------------------------------------
The method can obtain good performance when processing some semi-structured documents (e.g. template-based web page). Its disadvantage lies on that its rudimentary learning mechanisms cannot provide enough generalization capabilities. This makes it difficult to obtain good performance in complicated situations (e.g. 
----------------------------------------

CHUNK 389
File: unknown
Size: 1613 chars
Content:
----------------------------------------
extraction from natural language text). The classification based method casts the IE task as a classification problem in terms of the statistical theory. It can incorporate different types of information (including words, syntax, a prior knowledge, etc.). Thus it has more generalization capabilities than the rule based method. In several real-world applications, it can outperform the rule based method. Its drawback is that its model is usually complex and it is difficult for the general user to understand (e.g. the feature definition). Thus the performances of extraction differ from application to application. The sequential labeling based method can make use of dependencies between information to improve the extraction performance. It is also based on the statistical theory and thus has strong generalization capabilities. In many applications, in particular natural language processing, it can outperform the rule based method and the classification based method. As for the disadvantage, similar to the classification based method, it is not easy to be understood by a general user. Information extraction suffers from uncertainty and implication of the natural language. Both of the two problems are difficult for machine to automatic extraction, sometimes even for human. For example, “It is likely that ...”. In such sentence, it is difficult to determine the reliability degree of the information. Consider another example “After a furious fight, enemy raised the white flag”, here the white flag means a defeat. However, it would of course difficult for computer to conclude the implication. 31
----------------------------------------

CHUNK 390
File: unknown
Size: 994 chars
Content:
----------------------------------------
Another interesting also important issue is how to make use of the prior knowledge in information extraction. So far, a usual method for incorporating the prior knowledge is to use some domain-specific dictionaries, thesauri in the extraction. The question is whether the simple method still works well when dealing with more complex extraction tasks. A further question is if we can incorporate the different types of prior knowledge into a unified model for extraction. In future work, research community has to face the rising challenges and focuses on how to enhance the practical usefulness of IE methods. ACKNOWLEDGE The work is funded by the Natural Science Foundation of China under Grant No. 90604025. Thanks to the anonymous reviewers for their constructive suggestions. REFERENCES Abney, S. (1991). Parsing by chunks. In R. Berwick, S. Abney, and C. Tenny (Eds.), Principle-based parsing. Boston: Kluwer Academic Publishers. Berger, A. L., Della Pietra, S. A., & Della Pietra, V. J. 
----------------------------------------

CHUNK 391
File: unknown
Size: 662 chars
Content:
----------------------------------------
(1996). A maximum entropy approach to natural language processing. In Computational Linguistics (Vol.22, pp.39-71). MA: MIT Press. Boser, B. E., Guyon, I. M., & Vapnik, V. N. (1992). A training algorithm for optimal margin classifiers. In D. Haussler (Eds.) 5th Annual ACM Workshop on COLT (pp.144-152). Pittsburgh, PA: ACM Press. Bunescu, R., Ge, R., Kate, R. J., Marcotte, E. M., Mooney, R. J., Ramani, A. K., et al. (2005). Comparative experiments on learning information extractors for proteins and their interactions. Artificial Intelligence in Medicine (special issue on Summarization and Information Extraction from Medical Documents). 33(2), pp.139-155. 
----------------------------------------

CHUNK 392
File: unknown
Size: 414 chars
Content:
----------------------------------------
Bunescu, R. & Mooney, R. J. (2005). Statistical relational learning for natural language information extraction. In Getoor, L., & Taskar, B. (Eds.), Statistical Relational Learning, forthcoming book Califf, M. E., & Mooney, R. J. (1998). Relational learning of pattern-match rules for information extraction. In Working Notes of AAAI Spring Symposium on Applying Machine Learning to Discourse Processing. pp.6-11. 
----------------------------------------

CHUNK 393
File: unknown
Size: 628 chars
Content:
----------------------------------------
Califf, M. E., & Mooney, R. J. (2003). Bottom-up relational learning of pattern matching rules for information extraction. Journal of Machine Learning Research. Vol.4, pp.177-210. Chen, S. F., & Rosenfeld, R. (1999). A Gaussian prior for smoothing maximum entropy models. Technical Report CMU-CS-99-108, Carnegie Mellon University. Ciravegna, F. (2001). (LP)2, an adaptive algorithm for information extraction from Web-related texts. In Proceedings of the IJCAI-2001 Workshop on Adaptive Text Extraction and Mining held in conjunction with 17th International Joint Conference on Artificial Intelligence (IJCAI), Seattle, USA. 32
----------------------------------------

CHUNK 394
File: unknown
Size: 440 chars
Content:
----------------------------------------
Collins, M. (2002). Discriminative training methods for Hidden Markov models: theory and experiments with Perceptron algorithms. In Proceedings of the Conference on Empirical Methods in NLP (EMNLP’02). Craven, M., & Kumlien, J. (1999). Constructing biological knowledge bases by extracting information from text sources. In Proceedings of the 7th International Conference on Intelligent Systems for Molecular Biology (ISMB-1999). pp.77-86. 
----------------------------------------

CHUNK 395
File: unknown
Size: 52 chars
Content:
----------------------------------------
Heidelberg, Germany. Darroch, J. N., & Ratcliff, D. 
----------------------------------------

CHUNK 396
File: unknown
Size: 703 chars
Content:
----------------------------------------
(1972). Generalized iterative scaling for log-linear models. The Annals of Mathematical Statistics, 43 (5), pp.1470-1480. Ducheneaut, N., & Bellotti, V. (2001). E-mail as Habitat: An exploration of embedded personal information management. Interactions, Vol.8, pp.30-38. Finkel, J. R., Grenager, T., & Manning, C. D. (2005). Incorporating non-local information into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics (ACL-2005). pp.363-370. Finn, A., & Kushmerick, N. (2004). Information extraction by convergent boundary classification. In AAAI-04 Workshop on Adaptive Text Extraction and Mining. San Jose, USA. 
----------------------------------------

CHUNK 397
File: unknown
Size: 142 chars
Content:
----------------------------------------
Finn, A. (2006). A multi-level boundary classification approach to information extraction. Phd thesis, University College Dublin. Freitag, D. 
----------------------------------------

CHUNK 398
File: unknown
Size: 1527 chars
Content:
----------------------------------------
(1998). Information extraction from HTML: Application of a general machine learning approach. In Proceedings of the 15th Conference on Artificial Intelligence (AAAI’98). pp.517-523 Freitag, D., & Kushmerick, N. (2000). Boosted wrapper induction. In Proceedings of 17th National Conference on Artificial Intelligence. pp.577-583 Ghahramani, Z., & Jordan, M. I. (1997). Factorial Hidden Markov Models. Machine Learning, Vol.29, pp.245-273 Hammersley, J., & Clifford, P. (1971). Markov fields on finite graphs and lattices. Unpublished manuscript. Han, H., Giles, L., Manavoglu, E., Zha, H., Zhang, Z., & Fox, E.A. (2003). Automatic document metadata extraction using support vector machines. In Proceedings of 2003 Joint Conference on Digital Libraries (JCDL’03). pp.37-48 Kauchak, D., Smarr, J., & Elkan, C. (2004). Sources of success for boosted wrapper induction. The Journal of Machine Learning Research, Vol.5, pp.499-527. MA: MIT Press. Kristjansson, T. T., Culotta, A., Viola, P. A., & McCallum, A. (2004). Interactive information extraction with constrained conditional random fields. In Proceedings of AAAI’04, pp.412-418 Kushmerick, N., Weld, D. S., & Doorenbos, R. (1997). Wrapper induction for information extraction. In Proceedings of the International Joint Conference on Artificial Intelligence(IJCAI’97). pp.729-737. Kushmerick, N. (2000). Wrapper induction: Efficiency and expressiveness. Artificial Intelligence, Vol.118, pp.15-68. Lafferty, J., McCallum, A., & Pereira, F. (2001). Conditional Random Fields: 33
----------------------------------------

CHUNK 399
File: unknown
Size: 314 chars
Content:
----------------------------------------
Probabilistic models for segmenting and labeling sequence data. In Proceedings of the 18th International Conference on Machine Learning (ICML’01). pp.282-289. Lawrence, S., Giles, C.L., & Bollacker K. (1999). Digital libraries and autonomous citation indexing. IEEE Computer, Vol.32(6), pp.67-71. Li, J., & Yu, Y. 
----------------------------------------

CHUNK 400
File: unknown
Size: 2015 chars
Content:
----------------------------------------
(2001). Learning to generate semantic annotation for domain specific sentences. In Proceedings of the Knowledge Markup and Semantic Annotation Workshop in K-CAP'2001, Victoria, BC. Li, X., & Liu, B. (2003). Learning to classify texts using positive and unlabeled data. In Proceedings of International Joint Conference on Artificial Intelligence (IJCAI'2003). pp.587-592 McCallum, A., Freitag, D., & Pereira, F. (2000). Maximum Entropy Markov Models for information extraction and segmentation. In Proceedings of the 17th International Conference on Machine Learning (ICML’00). pp.591-598. McCallum, A. (2003). Efficiently inducing features of Conditional Random Fields. In Proceedings of the 19th Conference in Uncertainty in Artificial Intelligence. pp.403-410. Morik, K., Brockhausen, P., & Joachims, T. (1999). Combining statistical learning with a knowledge-based approach - A case study in intensive care monitoring. In Proceedings of International Conference on Machine Learning (ICML’99). pp.268-277. Muslea, I., Minton, S., & Knoblock, C. (1998). STALKER: Learning extraction rules for semistructured, web-based information sources. In AAAI Workshop on AI and Information Integration. pp.74-81. Muslea, I., Minton, S., & Knoblock, C. (1999). Hierarchical wrapper induction for semistructured information sources. Autonomous Agents and Multi-Agent Systems. Vol.4, pp.93-114. Muslea, I. (1999). Extraction patterns for information extraction tasks: A survey. In Proceedings of AAAI-99: Workshop on Machine Learning for Information Extraction. Orlando. Muslea, I., Minton, S., & Knoblock, C. A. (2003). Active learning with strong and weak views: A case study on wrapper induction. In Proceedings of the International Joint Conference on Artificial Intelligence(IJCAI). Acapulco, Mexico. Ng, H. T., Lim, C. Y., Koo, J. L. T. (1999). Learning to Recognize Tables in Free Text. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics on Computational Linguistics (ACL’99). pp. 
----------------------------------------

CHUNK 401
File: unknown
Size: 101 chars
Content:
----------------------------------------
443-450. Nocedal, J., & Wright, S. J. (1999). Numerical optimization. New York, USA: Springer press. 
----------------------------------------

CHUNK 402
File: unknown
Size: 222 chars
Content:
----------------------------------------
Peng, F. (2001). Models for Information Extraction. Technique Report. Peng, F., & McCallum, A. (2004). Accurate information extraction from research papers using Conditional Random Fields. In Proceedings of HLT-NAACL. pp. 
----------------------------------------

CHUNK 403
File: unknown
Size: 252 chars
Content:
----------------------------------------
329-336. Pinto, D., McCallum, A., Wei, X., & Croft, W. B. (2003). Table Extraction Using Conditional Random Fields. In Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’03). 34
----------------------------------------

CHUNK 404
File: unknown
Size: 191 chars
Content:
----------------------------------------
pp. 235-242. Ramshaw, L. A., & Marcus, M. P. (1995). Text chunking using transformation-based learning. In Proceedings of Third Workshop on Very Large Corpora, ACL. pp.67-73. Ratnaparkhi, A. 
----------------------------------------

CHUNK 405
File: unknown
Size: 728 chars
Content:
----------------------------------------
(1998). Unsupervised Statistical Models for Prepositional Phrase Attachment. In Proceedings of COLING ACL’98. pp.1079-1085. Montreal, Canada. Riloff, E. (1993). Automatically Constructing a Dictionary for Information Extraction Tasks. In Proceedings of the Eleventh National Conference on Artificial Intelligence. pp.811-816. Riloff, E. (1996). Automatically Generating Extraction Patterns from Untagged Text. In Proceedings of the Thirteenth National Conference on Artificial Intelligence. pp.1044-1049. Riloff, E., & Jones, R. (1999). Learning Dictionaries for Information Extraction by Multi-Level Bootstrapping. In Proceedings of the Sixteenth National Conference on Artificial Intelligence. pp.474-479. Roth, D., & Yih, W. 
----------------------------------------

CHUNK 406
File: unknown
Size: 195 chars
Content:
----------------------------------------
(2004). A linear programming formulation for global inference in natural language tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) . pp.1-8. 
----------------------------------------

CHUNK 407
File: unknown
Size: 1013 chars
Content:
----------------------------------------
Boston, MA. Schölkopf B., Burges, C. J. C., & Smola A. J. (1999). Advances in kernel methods: Support vector learning. MA: MIT Press. Sha, F., & Pereira, F. (2003). Shallow parsing with Conditional Random Fields. In Proceedings of Human Language Technology, NAACL. pp.188-191. Shapire, R. E. (1999). A brief introduction to Boosting. In Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-1999). pp.1401-1405. Shewchuk, J. R. (1994). An introduction to the conjugate gradient method without the agonizing pain. from http://www-2.cs.cmu.edu/.jrs/jrspapers.html#cg. Siefkes, C., & Siniakov, P. (2005). An overview and classification of adaptive approaches to information extraction. Journal on Data Semantics IV. Berlin, Germany: Springer. Soderland, S., Fisher, D., Aseltine, J., & Lehnert, W. (1995). CRYSTAL: Inducing a conceptual dictionary. In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI’95). pp.1314-1319. Soderland, S. 
----------------------------------------

CHUNK 408
File: unknown
Size: 808 chars
Content:
----------------------------------------
(1999). Learning information extraction rules for semi-structured and free text. Machine Learning. Boston: Kluwer Academic Publishers Sutton, C., Rohanimanesh, K., & McCallum, A. (2004). Dynamic conditional random fields: factorized probabilistic models for labeling and segmenting sequence data. In Proceedings of ICML’2004. pp.783-790. Sutton, C., & McCallum, A. (2005). An introduction to Conditional Random Fields for relational learning. In Getoor, L., & Taskar, B. (Eds.), Statistical Relational Learning, forthcoming book. Tang, J., Li, H., Cao, Y., & Tang, Z. (2005). Email Data Cleaning. In Proceedings of SIGKDD’2005. pp.489-499. Chicago, Illinois, USA. Tang, J., Li, J., Lu, H., Liang, B., & Wang, K. (2005). iASA: Learning to Annotate the Semantic Web. Journal on Data Semantic IV (pp. 110-145). 
----------------------------------------

CHUNK 409
File: unknown
Size: 33 chars
Content:
----------------------------------------
New York, USA: Springer Press. 35
----------------------------------------

CHUNK 410
File: unknown
Size: 270 chars
Content:
----------------------------------------
Tang. J., Hong, M., Zhang, J., Liang, B., and Li, J. (2006). A New Approach to Personal Network Search based on Information Extraction. In Proceedings of the first International Conference of Asian Semantic Web (ASWC). To appear. Tang, J., Hong, M., Li, J., & Liang, B. 
----------------------------------------

CHUNK 411
File: unknown
Size: 934 chars
Content:
----------------------------------------
(2006). Tree-structured conditional random fields for semantic annotation. In Proceedings of 5th International Conference of Semantic Web (ISWC’2006), pp.640-653. Taskar, B., Guestrin, C., & Koller, D. (2003). Max-margin markov networks. In Neural Information Processing Systems 2003. Tetko, I.V., Livingstone, D.J., & Luik, A.I. (1995). Neural network studies. 1. Comparison of overfitting and overtraining. Journal of Chemical Information and Computer Sciences, Vol.35, pp.826-833. Tjong Kim Sang, E. F., & Buchholz, S. (2000). Introduction to the CoNLL-2000 shared task: Chunking. In Proceedings of CoNLL-2000, pp.127-132. Vapnik, V. (1998). Statistical Learning Theroy. Springer Verlage, New York, 1998 Vapnik V. (1999). The Nature of Statistical Learning Theory. Springer Verlag, New York, 1999. Wallach, H. (2002). Efficient training of Conditional Random Fields. Master thesis. University of Edinburgh, USA. Wang, Y., & Hu, J. 
----------------------------------------

CHUNK 412
File: unknown
Size: 231 chars
Content:
----------------------------------------
(2002). A Machine Learning based Approach for Table Detection on the Web. In Proceedings of the 11th International World Wide Web Conference (WWW’02). pp. 242-250. Honolulu, Hawaii, USA. Wainwright, M., Jaakkola, T., & Willsky, A. 
----------------------------------------

CHUNK 413
File: unknown
Size: 1009 chars
Content:
----------------------------------------
(2001). Tree-based reparameterization for approximate estimation on graphs with cycles. In Proceedings of Advances in Neural Information Processing Systems (NIPS'2001). pp.1001-1008. Zelenko, D., Aone, C., & Richardella, A. (2003). Kernel methods for relation extraction. Journal of Machine Learning Research, Vol.3, 1083-1106. Zhang, L., Pan, Y., & Zhang, T. (2004). Recognizing and Using Named Entities: Focused Named Entity Recognition Using Machine Learning. In Proceedings of the 27th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR’04). pp.281-288. Zhu, J., Nie, Z., Wen, J., Zhang, B., & Ma, W. (2005). 2D Conditional Random Fields for Web information extraction. In Proceedings of 22nd International Conference on Machine Learning (ICML2005). pp.1044-1051. Bonn, Germany. ADDITIONAL READING Adwait, R. (1996). Maximum Entropy Model for POS tagging. In Proceedings of the Conference on Empirical Methods in Natural Language Processing. pp.133-142. 
----------------------------------------

CHUNK 414
File: unknown
Size: 36 chars
Content:
----------------------------------------
Somerset, New Jersey, 1996. Ahn, D. 
----------------------------------------

CHUNK 415
File: unknown
Size: 319 chars
Content:
----------------------------------------
(2006). The Stages of Event Extraction. In Proceedings of the Workshop on Annotating and Reasoning about Time and Events. pp. 1–8. Sydney, July 2006. Allen, J. (1994). Natural Language Understanding (2nd Edition). Addison Wesley. 1994 Altun, Y., Tsochantaridis, I., & Hofmann, T. (2003). Hidden Markov Support Vector 36
----------------------------------------

CHUNK 416
File: unknown
Size: 652 chars
Content:
----------------------------------------
Machines. In Proceedings of the 20th International Conference on Machine Learning (ICML 2003). Appelt, D. & Israel, D. (1999). Introduction to Information Extraction Technology. In Proceedings of IJCAI’99 Tutorial. Baum, L. E. & Petrie, T. (1966). Statistical Inference for Probabilistic Functions of Finite State Markov Chains. Annual of Mathematical statistics, 37:1554-1563, 1966. Borthwick, A., Sterling, J., Agichtein, E., & Grishman, R. (1998). Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition. In Proceedings of the Sixth Workshop on Very Large Corpora New Brunswick, New Jersey. Bunescu, R.C. & Mooney, R.J. 
----------------------------------------

CHUNK 417
File: unknown
Size: 730 chars
Content:
----------------------------------------
(2004). Collective Information Extraction with Relational Markov Networks. In Proceedings of ACL’2004. Cafarella, M.J., Downey, D., Soderland, S., & Etzioni, O. (2005). KnowItNow: Fast, Scalable Information Extraction from the Web. In Proceedings of HLT/EMNLP’2005. Chieu, H.L. (2002). A Maximum Entropy Approach to Information Extraction from Semi-Structured and Free Text. In Proceedings of the Eighteenth National Conference on Artificial Intelligence (AAAI’2002). pp.786-791. Collins, M. (2002). Discriminative Training Methods for Hidden Markov Models: Theory and Experiments with Perceptron Algorithms. In Proceedings of the Conference on Empirical Methods in Natural Language Processing(EMNLP’2002). pp.1-8, July 06, 2002. 
----------------------------------------

CHUNK 418
File: unknown
Size: 435 chars
Content:
----------------------------------------
Dietterich, T. (2002). Machine Learning for Sequential Data: A Review. In Proceedings of the Joint IAPR International Workshop on Structural, Syntactic, and Statistical Pattern Recognition. pp. 15–30. 2002. Springer-Verlag. Downey, D., Etzioni, O., & Soderland, S. (2005). A Probabilistic Model of Redundancy in Information Extraction. In Proceedings of 22th International Joint Conference on Artificial Intelligence (IJCAI’2005). pp. 
----------------------------------------

CHUNK 419
File: unknown
Size: 270 chars
Content:
----------------------------------------
1034-1041. Durbin, R., Eddy, S., Krogh, A., & Mitchison, G. (1998). Biological sequence analysis: Probabilistic models of proteins and nucleic acids. Cambridge University Press, 1998. Eikvil, L. (1999). Information Extraction from World Wide Web - A Survey. Rapport Nr. 
----------------------------------------

CHUNK 420
File: unknown
Size: 904 chars
Content:
----------------------------------------
945, July, 1999. Embley, D.W. (2004). Toward Semantic Understanding - An Approach Based on Information Extraction. In Proceedings of the Fifteenth Australasian Database Conference, 2004. Freitag, D. (1998). Machine Learning for Information Extraction from Online Documents. PhD thesis, School of Computer Science. Carnegie Mellon University. Freitag, D. & McCallum, A. (2000). Information Extraction with HMM Structures Learned by Stochastic Optimization. In Proceedings of the Sixteenth National Conference on Artificial Intelligence (AAAI’2000). Grishman, R. & Sundheim, B. (1996). Message Understanding Conference –6: A Brief History. In Proceedings of the 16th International Conference on Computational Linguistics, Copenhagen, June 1996. Hu, Y., Li, H., Cao, Y., Meyerzon, D., Teng, L., & Zheng, Q. (2006). Automatic Extraction of Titles from General Documents using Machine Learning. Information 37
----------------------------------------

CHUNK 421
File: unknown
Size: 193 chars
Content:
----------------------------------------
Processing and Management. pp.1276-1293, 2006 Huffman, S.B. (1995). Learning Information Extraction Patterns from Examples. In Proceedings of Learning for Natural Language Processing’1995. pp. 
----------------------------------------

CHUNK 422
File: unknown
Size: 144 chars
Content:
----------------------------------------
246-260. Jackson, P. & Moulinier, I. (2002). Natural Language Processing for Online Applications. John Benjamins, 2002. Klein, D. & Manning, C. 
----------------------------------------

CHUNK 423
File: unknown
Size: 905 chars
Content:
----------------------------------------
(2002). Conditional Structure Versus Conditional Estimation in NLP Models. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP’2002), Philadelphia. Laender, A.H.F., Ribeiro-Neto, B.A., da Silva, A.S., & Teixeira, J.S. (2002). A Brief Survey of Web Data Extraction Tools . Journal of ACM SIGMOD Record, 2002. Leek, T.B. (1997). Information Extraction Using Hidden Markov Models. M.S. thesis. Moens, M. (2006). Information Extraction: Algorithms and Prospects in a Retrieval Context. Springer press Li, Y., Bontcheva, K., & Cunningham, H. (2005). Using Uneven-Margins SVM and Perceptron for Information Extraction. In Proceedings of Ninth Conference on Computational Natural Language Learning (CoNLL-2005). pp.72-79 Manning, C., & Schutze, H. (1999). Markov Models. In Book: Foundations of Statistical Natural Language Processing. The MIT Press. 1999. Pazienza, M.T. 
----------------------------------------

CHUNK 424
File: unknown
Size: 706 chars
Content:
----------------------------------------
(1999). Information Extraction : Towards Scalable, Adaptable Systems. Springer press. Punyakanok, V. & Roth, D. (2001). The Use of Classifiers in Sequential Inference. In Proceedings of NIPS’01. pp.995-1001. Rabiner, L. A. (1989). Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition. In Proceedings of the IEEE’1989. Shawe-Taylor, J. & Cristianini, N. (2000). Introduction to Support Vector Machines. Cambridge University Press, 2000 Skounakis, M., Craven, M., & Ray, S. (2003). Hierarchical Hidden Markov Models for Information Extraction. In Proceedings of the 18th International Joint Conference on Artificial Intelligence, Acapulco, Mexico. Morgan Kaufmann,2003. Z. Zhang. 
----------------------------------------

CHUNK 425
File: unknown
Size: 812 chars
Content:
----------------------------------------
(2004). Weakly-Supervised Relation Classification for Information Extraction. In Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management (CIKM’2004).pp581-588. 1 In machine learning, usually a learning algorithm is trained using some set of training examples. The learner is assumed to reach a state where it will also be able to predict the correct output for other examples. However, especially in cases where learning was performed too long or where training examples are rare, the learner may adjust to very specific random features of the training data, that have no causal relation to the target function. In this process of overfitting, the performance on the training examples still increases while the performance on unseen data becomes worse (Tetko, 1995). 38
----------------------------------------

CHUNK 426
File: unknown
Size: 89 chars
Content:
----------------------------------------
Knowledge Graph Construction, Evaluation, and Applications Haihua Chen March 1, 2022 1/85
----------------------------------------

CHUNK 427
File: unknown
Size: 236 chars
Content:
----------------------------------------
(cid:73) How to Construct a Large-scale Knowledge Graph (cid:73) Towards a Legal Knowledge Graph (cid:73) Knowledge Graph Qualify and Evaluation (cid:73) Knowledge Graph Applications Outline (cid:73) Introduction to Knowledge Graph 2/85
----------------------------------------

CHUNK 428
File: unknown
Size: 236 chars
Content:
----------------------------------------
(cid:73) Towards a Legal Knowledge Graph (cid:73) Knowledge Graph Qualify and Evaluation (cid:73) Knowledge Graph Applications Outline (cid:73) Introduction to Knowledge Graph (cid:73) How to Construct a Large-scale Knowledge Graph 3/85
----------------------------------------

CHUNK 429
File: unknown
Size: 236 chars
Content:
----------------------------------------
(cid:73) Knowledge Graph Qualify and Evaluation (cid:73) Knowledge Graph Applications Outline (cid:73) Introduction to Knowledge Graph (cid:73) How to Construct a Large-scale Knowledge Graph (cid:73) Towards a Legal Knowledge Graph 4/85
----------------------------------------

CHUNK 430
File: unknown
Size: 236 chars
Content:
----------------------------------------
(cid:73) Knowledge Graph Applications Outline (cid:73) Introduction to Knowledge Graph (cid:73) How to Construct a Large-scale Knowledge Graph (cid:73) Towards a Legal Knowledge Graph (cid:73) Knowledge Graph Qualify and Evaluation 5/85
----------------------------------------

CHUNK 431
File: unknown
Size: 236 chars
Content:
----------------------------------------
Outline (cid:73) Introduction to Knowledge Graph (cid:73) How to Construct a Large-scale Knowledge Graph (cid:73) Towards a Legal Knowledge Graph (cid:73) Knowledge Graph Qualify and Evaluation (cid:73) Knowledge Graph Applications 6/85
----------------------------------------

CHUNK 432
File: unknown
Size: 406 chars
Content:
----------------------------------------
(cid:73) KG Representation: (cid:104)head,relationship,tail(cid:105). Introduction to KG What is a Knowledge Graph (KG) (cid:73) Knowledge graph (KG) is a multi-relational graph composed of entities and relations which are regarded as nodes and different types of edges, respectively 1. 1 S.Ji,etal.”ASurveyonKnowledgeGraphs:Representation,AcquisitionandApplications.”arXivpreprint arXiv:2002.00388(2020). 
----------------------------------------

CHUNK 433
File: unknown
Size: 4 chars
Content:
----------------------------------------
7/85
----------------------------------------

CHUNK 434
File: unknown
Size: 287 chars
Content:
----------------------------------------
Introduction to KG What is a Knowledge Graph (KG) (cid:73) Knowledge graph (KG) is a multi-relational graph composed of entities and relations which are regarded as nodes and different types of edges, respectively 1. (cid:73) KG Representation: (cid:104)head,relationship,tail(cid:105). 
----------------------------------------

CHUNK 435
File: unknown
Size: 123 chars
Content:
----------------------------------------
1 S.Ji,etal.”ASurveyonKnowledgeGraphs:Representation,AcquisitionandApplications.”arXivpreprint arXiv:2002.00388(2020). 8/85
----------------------------------------

CHUNK 436
File: unknown
Size: 151 chars
Content:
----------------------------------------
Introduction to KG A KG Example in the Film Domain 2 2 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 9/85
----------------------------------------

CHUNK 437
File: unknown
Size: 420 chars
Content:
----------------------------------------
(cid:73) Concept: A fundamental category of existence; Representations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). 
----------------------------------------

CHUNK 438
File: unknown
Size: 5 chars
Content:
----------------------------------------
10/85
----------------------------------------

CHUNK 439
File: unknown
Size: 334 chars
Content:
----------------------------------------
(cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). 
----------------------------------------

CHUNK 440
File: unknown
Size: 91 chars
Content:
----------------------------------------
(cid:73) Concept: A fundamental category of existence; Representations of categories. 11/85
----------------------------------------

CHUNK 441
File: unknown
Size: 250 chars
Content:
----------------------------------------
(cid:73) Value: Data, String, Numerical, ect. Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). 
----------------------------------------

CHUNK 442
File: unknown
Size: 175 chars
Content:
----------------------------------------
(cid:73) Concept: A fundamental category of existence; Representations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. 12/85
----------------------------------------

CHUNK 443
File: unknown
Size: 204 chars
Content:
----------------------------------------
Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). 
----------------------------------------

CHUNK 444
File: unknown
Size: 221 chars
Content:
----------------------------------------
(cid:73) Concept: A fundamental category of existence; Representations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. 13/85
----------------------------------------

CHUNK 445
File: unknown
Size: 204 chars
Content:
----------------------------------------
Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). 
----------------------------------------

CHUNK 446
File: unknown
Size: 221 chars
Content:
----------------------------------------
(cid:73) Concept: A fundamental category of existence; Representations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. 14/85
----------------------------------------

CHUNK 447
File: unknown
Size: 204 chars
Content:
----------------------------------------
Introduction to KG Node (cid:73) Entity (Object/Instance): Something that exists as itself, as a subject or an object, actually or potentially, connectedly, or abstractly, physically or not. (Wikipedia). 
----------------------------------------

CHUNK 448
File: unknown
Size: 221 chars
Content:
----------------------------------------
(cid:73) Concept: A fundamental category of existence; Representations of categories. (cid:73) Category(Type, Class): Grouping of entities that have something in common. (cid:73) Value: Data, String, Numerical, ect. 15/85
----------------------------------------

CHUNK 449
File: unknown
Size: 161 chars
Content:
----------------------------------------
(cid:73) Attribute/Property/Quality: A characteristic/quality to define an entity. Introduction to KG Edge (cid:73) Relationship: The relation between entities. 
----------------------------------------

CHUNK 450
File: unknown
Size: 5 chars
Content:
----------------------------------------
16/85
----------------------------------------

CHUNK 451
File: unknown
Size: 161 chars
Content:
----------------------------------------
Introduction to KG Edge (cid:73) Relationship: The relation between entities. (cid:73) Attribute/Property/Quality: A characteristic/quality to define an entity. 
----------------------------------------

CHUNK 452
File: unknown
Size: 5 chars
Content:
----------------------------------------
17/85
----------------------------------------

CHUNK 453
File: unknown
Size: 101 chars
Content:
----------------------------------------
Introduction to KG Model of KG 3 3 Y.Xiao.”IntroductiontoKnowledgeGraphs”.FudanUniversity,2017. 18/85
----------------------------------------

CHUNK 454
File: unknown
Size: 345 chars
Content:
----------------------------------------
(cid:73) Scaling them is challenging. (cid:73) The quality of KGs is difficult to be guaranteed and evaluated. (cid:73) Knowledge Graphs: Introduction to KG Knowledge Graphs in the Wild 4 (cid:73) Building knowledge graphs are expensive. 4 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 
----------------------------------------

CHUNK 455
File: unknown
Size: 5 chars
Content:
----------------------------------------
19/85
----------------------------------------

CHUNK 456
File: unknown
Size: 238 chars
Content:
----------------------------------------
(cid:73) The quality of KGs is difficult to be guaranteed and evaluated. (cid:73) Knowledge Graphs: Introduction to KG Knowledge Graphs in the Wild 4 (cid:73) Building knowledge graphs are expensive. (cid:73) Scaling them is challenging. 
----------------------------------------

CHUNK 457
File: unknown
Size: 112 chars
Content:
----------------------------------------
4 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 20/85
----------------------------------------

CHUNK 458
File: unknown
Size: 345 chars
Content:
----------------------------------------
(cid:73) Knowledge Graphs: Introduction to KG Knowledge Graphs in the Wild 4 (cid:73) Building knowledge graphs are expensive. (cid:73) Scaling them is challenging. (cid:73) The quality of KGs is difficult to be guaranteed and evaluated. 4 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 
----------------------------------------

CHUNK 459
File: unknown
Size: 5 chars
Content:
----------------------------------------
21/85
----------------------------------------

CHUNK 460
File: unknown
Size: 345 chars
Content:
----------------------------------------
Introduction to KG Knowledge Graphs in the Wild 4 (cid:73) Building knowledge graphs are expensive. (cid:73) Scaling them is challenging. (cid:73) The quality of KGs is difficult to be guaranteed and evaluated. (cid:73) Knowledge Graphs: 4 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 
----------------------------------------

CHUNK 461
File: unknown
Size: 5 chars
Content:
----------------------------------------
22/85
----------------------------------------

CHUNK 462
File: unknown
Size: 102 chars
Content:
----------------------------------------
Introduction to KG Common Characteristics of KGs 5 5 https://queue.acm.org/detail.cfm?id=3332266 23/85
----------------------------------------

CHUNK 463
File: unknown
Size: 87 chars
Content:
----------------------------------------
(cid:73) High coverage over semantics relationships. (cid:73) Structured Organization. 
----------------------------------------

CHUNK 464
File: unknown
Size: 145 chars
Content:
----------------------------------------
(cid:73) Provide users with explainable results. Introduction to KG The Advantages of KG (cid:73) High coverage over entities and concepts. 24/85
----------------------------------------

CHUNK 465
File: unknown
Size: 34 chars
Content:
----------------------------------------
(cid:73) Structured Organization. 
----------------------------------------

CHUNK 466
File: unknown
Size: 198 chars
Content:
----------------------------------------
(cid:73) Provide users with explainable results. Introduction to KG The Advantages of KG (cid:73) High coverage over entities and concepts. (cid:73) High coverage over semantics relationships. 25/85
----------------------------------------

CHUNK 467
File: unknown
Size: 227 chars
Content:
----------------------------------------
(cid:73) Provide users with explainable results. Introduction to KG The Advantages of KG (cid:73) High coverage over entities and concepts. (cid:73) High coverage over semantics relationships. (cid:73) Structured Organization. 
----------------------------------------

CHUNK 468
File: unknown
Size: 5 chars
Content:
----------------------------------------
26/85
----------------------------------------

CHUNK 469
File: unknown
Size: 227 chars
Content:
----------------------------------------
Introduction to KG The Advantages of KG (cid:73) High coverage over entities and concepts. (cid:73) High coverage over semantics relationships. (cid:73) Structured Organization. (cid:73) Provide users with explainable results. 
----------------------------------------

CHUNK 470
File: unknown
Size: 5 chars
Content:
----------------------------------------
27/85
----------------------------------------

CHUNK 471
File: unknown
Size: 64 chars
Content:
----------------------------------------
Introduction to KG From KG Construction to KG Applications 28/85
----------------------------------------

CHUNK 472
File: unknown
Size: 164 chars
Content:
----------------------------------------
Construction of large-scale KG The Pipeline of KG Construction 6 6 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 29/85
----------------------------------------

CHUNK 473
File: unknown
Size: 281 chars
Content:
----------------------------------------
(cid:73) Identify which fields to be compared. (cid:73) Schema matching. Construction of large-scale KG Data Preparation 7 (cid:73) Storing the data in a uniform manner. (cid:73) Parsing: locate, identify, and separate data items. (cid:73) Data transformation and standardization. 
----------------------------------------

CHUNK 474
File: unknown
Size: 99 chars
Content:
----------------------------------------
7 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 30/85
----------------------------------------

CHUNK 475
File: unknown
Size: 184 chars
Content:
----------------------------------------
(cid:73) Schema matching. Construction of large-scale KG Data Preparation 7 (cid:73) Storing the data in a uniform manner. (cid:73) Parsing: locate, identify, and separate data items. 
----------------------------------------

CHUNK 476
File: unknown
Size: 196 chars
Content:
----------------------------------------
(cid:73) Data transformation and standardization. (cid:73) Identify which fields to be compared. 7 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 31/85
----------------------------------------

CHUNK 477
File: unknown
Size: 158 chars
Content:
----------------------------------------
Construction of large-scale KG Data Preparation 7 (cid:73) Storing the data in a uniform manner. (cid:73) Parsing: locate, identify, and separate data items. 
----------------------------------------

CHUNK 478
File: unknown
Size: 222 chars
Content:
----------------------------------------
(cid:73) Data transformation and standardization. (cid:73) Identify which fields to be compared. (cid:73) Schema matching. 7 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 32/85
----------------------------------------

CHUNK 479
File: unknown
Size: 158 chars
Content:
----------------------------------------
Construction of large-scale KG Data Preparation 7 (cid:73) Storing the data in a uniform manner. (cid:73) Parsing: locate, identify, and separate data items. 
----------------------------------------

CHUNK 480
File: unknown
Size: 222 chars
Content:
----------------------------------------
(cid:73) Data transformation and standardization. (cid:73) Identify which fields to be compared. (cid:73) Schema matching. 7 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 33/85
----------------------------------------

CHUNK 481
File: unknown
Size: 147 chars
Content:
----------------------------------------
Construction of large-scale KG Ingestion Flow 8 8 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 34/85
----------------------------------------

CHUNK 482
File: unknown
Size: 558 chars
Content:
----------------------------------------
(cid:73) Synonyms: Entity Linking, Entity Resolution, Reference Reconciliation, Deduplication, Match/Merge, Merge/Purge. Construction of large-scale KG Entity Mapping 9 Entity Mapping: To identify and discover instances referring to the same real-world entity. (cid:73) Objective: (cid:73) Data Enrichment. (cid:73) Improve Data Quality by identifying and removing duplicates. (cid:73) Supporting fact correctness by merging duplicate facts from multiple sources. 9 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 
----------------------------------------

CHUNK 483
File: unknown
Size: 5 chars
Content:
----------------------------------------
35/85
----------------------------------------

CHUNK 484
File: unknown
Size: 558 chars
Content:
----------------------------------------
Construction of large-scale KG Entity Mapping 9 Entity Mapping: To identify and discover instances referring to the same real-world entity. (cid:73) Objective: (cid:73) Data Enrichment. (cid:73) Improve Data Quality by identifying and removing duplicates. (cid:73) Supporting fact correctness by merging duplicate facts from multiple sources. (cid:73) Synonyms: Entity Linking, Entity Resolution, Reference Reconciliation, Deduplication, Match/Merge, Merge/Purge. 9 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 
----------------------------------------

CHUNK 485
File: unknown
Size: 5 chars
Content:
----------------------------------------
36/85
----------------------------------------

CHUNK 486
File: unknown
Size: 538 chars
Content:
----------------------------------------
Construction of large-scale KG Knowledge Fusion (Merging Entities) 10 After merging entity nodes in the graph, we end up with conflicting facts and connections. (cid:73) Resolving facts (and finding truth) (cid:73) Majority Voting (cid:73) Identify Authoritative Sources (cid:73) Fact Checker (cid:73) Gather evidence from different sources (cid:73) Evaluate evidences (cid:73) Model joint interactions (cid:73) Aggregate evidence and predict 10 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 
----------------------------------------

CHUNK 487
File: unknown
Size: 5 chars
Content:
----------------------------------------
37/85
----------------------------------------

CHUNK 488
File: unknown
Size: 461 chars
Content:
----------------------------------------
Construction of large-scale KG Error Detection 11 (cid:73) Data Quality Rules (cid:73) Functional Dependency and its conditional variation, e.g.; Zip→− City (cid:73) Inconsistency Entity cannot be a movie and book Date-of-birth < date-of-death (cid:73) Outliers detection (cid:73) External signals for relationship validation (e.g.; co-clicks) (cid:73) NLP features 11 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 
----------------------------------------

CHUNK 489
File: unknown
Size: 5 chars
Content:
----------------------------------------
38/85
----------------------------------------

CHUNK 490
File: unknown
Size: 308 chars
Content:
----------------------------------------
Construction of large-scale KG Fact Inference 12 (cid:73) Further data enrichment and data completion (cid:73) Internal: Dominant type and label (cid:73) External: Search engine method for enriching social links. 12 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 
----------------------------------------

CHUNK 491
File: unknown
Size: 5 chars
Content:
----------------------------------------
39/85
----------------------------------------

CHUNK 492
File: unknown
Size: 143 chars
Content:
----------------------------------------
Towards a Legal Knowledge Graph Our Vision of Legal KG 13 13 V.MirelesandM.Kaltenb ̈ock.”BuildingtheLegalKnowledgeGraph”.W3CWorkshop,2018 40/85
----------------------------------------

CHUNK 493
File: unknown
Size: 430 chars
Content:
----------------------------------------
(cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search 41/85
----------------------------------------

CHUNK 494
File: unknown
Size: 430 chars
Content:
----------------------------------------
(cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation 42/85
----------------------------------------

CHUNK 495
File: unknown
Size: 430 chars
Content:
----------------------------------------
(cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation and jurisdiction 43/85
----------------------------------------

CHUNK 496
File: unknown
Size: 430 chars
Content:
----------------------------------------
(cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases 44/85
----------------------------------------

CHUNK 497
File: unknown
Size: 430 chars
Content:
----------------------------------------
(cid:73) Question answering Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization 45/85
----------------------------------------

CHUNK 498
File: unknown
Size: 430 chars
Content:
----------------------------------------
Towards a Legal Knowledge Graph Why Do We Build a Legal KG (cid:73) Cross-jurisdictional search (cid:73) Support the comparative analyses of court decisions and different legal interpretations of legislation (cid:73) Enables the evolution of legislation and jurisdiction (cid:73) Interlink legal knowledge with external knowledge bases (cid:73) Legal argument mining, reasoning, and summarization (cid:73) Question answering 46/85
----------------------------------------

CHUNK 499
File: unknown
Size: 181 chars
Content:
----------------------------------------
Towards a Legal Knowledge Graph Build an intelligent legal system, for automated the information retrieval, evidence detection , and argument generation in legal applications. 47/85
----------------------------------------

CHUNK 500
File: unknown
Size: 711 chars
Content:
----------------------------------------
(cid:73) Legal text are often long, usually thousands of words. It may contain issues related to cross areas of laws, which makes information extraction difficult 14. (cid:73) legal cases in different categories may use the common descriptions of an event, which makes it difficult for semantic understanding 15. (cid:73) The lacking of automated tools in legal field. Towards a Legal Knowledge Graph Challenges (cid:73) The style and structure of legal text are complex, and high level domain knowledge are required 14. 14 B.Guido,L.Caro,andL.Humphreys.”Usingclassificationtosupportlegalknowledgeengineersinthe eunomoslegaldocumentmanagementsystem.”FifthinternationalworkshoponJuris-informatics(JURISIN).2011. 
----------------------------------------

CHUNK 501
File: unknown
Size: 174 chars
Content:
----------------------------------------
15 S.Pudaruthetal.”Aninnovativemulti-segmentstrategyfortheclassificationoflegaljudgmentsusingthe k-nearestneighbourclassifier.”Complex IntelligentSystems4.1(2018):1-10. 48/85
----------------------------------------

CHUNK 502
File: unknown
Size: 521 chars
Content:
----------------------------------------
(cid:73) legal cases in different categories may use the common descriptions of an event, which makes it difficult for semantic understanding 15. (cid:73) The lacking of automated tools in legal field. Towards a Legal Knowledge Graph Challenges (cid:73) The style and structure of legal text are complex, and high level domain knowledge are required 14. (cid:73) Legal text are often long, usually thousands of words. It may contain issues related to cross areas of laws, which makes information extraction difficult 14. 
----------------------------------------

CHUNK 503
File: unknown
Size: 364 chars
Content:
----------------------------------------
14 B.Guido,L.Caro,andL.Humphreys.”Usingclassificationtosupportlegalknowledgeengineersinthe eunomoslegaldocumentmanagementsystem.”FifthinternationalworkshoponJuris-informatics(JURISIN).2011. 15 S.Pudaruthetal.”Aninnovativemulti-segmentstrategyfortheclassificationoflegaljudgmentsusingthe k-nearestneighbourclassifier.”Complex IntelligentSystems4.1(2018):1-10. 49/85
----------------------------------------

CHUNK 504
File: unknown
Size: 711 chars
Content:
----------------------------------------
(cid:73) The lacking of automated tools in legal field. Towards a Legal Knowledge Graph Challenges (cid:73) The style and structure of legal text are complex, and high level domain knowledge are required 14. (cid:73) Legal text are often long, usually thousands of words. It may contain issues related to cross areas of laws, which makes information extraction difficult 14. (cid:73) legal cases in different categories may use the common descriptions of an event, which makes it difficult for semantic understanding 15. 14 B.Guido,L.Caro,andL.Humphreys.”Usingclassificationtosupportlegalknowledgeengineersinthe eunomoslegaldocumentmanagementsystem.”FifthinternationalworkshoponJuris-informatics(JURISIN).2011. 
----------------------------------------

CHUNK 505
File: unknown
Size: 174 chars
Content:
----------------------------------------
15 S.Pudaruthetal.”Aninnovativemulti-segmentstrategyfortheclassificationoflegaljudgmentsusingthe k-nearestneighbourclassifier.”Complex IntelligentSystems4.1(2018):1-10. 50/85
----------------------------------------

CHUNK 506
File: unknown
Size: 521 chars
Content:
----------------------------------------
Towards a Legal Knowledge Graph Challenges (cid:73) The style and structure of legal text are complex, and high level domain knowledge are required 14. (cid:73) Legal text are often long, usually thousands of words. It may contain issues related to cross areas of laws, which makes information extraction difficult 14. (cid:73) legal cases in different categories may use the common descriptions of an event, which makes it difficult for semantic understanding 15. (cid:73) The lacking of automated tools in legal field. 
----------------------------------------

CHUNK 507
File: unknown
Size: 364 chars
Content:
----------------------------------------
14 B.Guido,L.Caro,andL.Humphreys.”Usingclassificationtosupportlegalknowledgeengineersinthe eunomoslegaldocumentmanagementsystem.”FifthinternationalworkshoponJuris-informatics(JURISIN).2011. 15 S.Pudaruthetal.”Aninnovativemulti-segmentstrategyfortheclassificationoflegaljudgmentsusingthe k-nearestneighbourclassifier.”Complex IntelligentSystems4.1(2018):1-10. 51/85
----------------------------------------

CHUNK 508
File: unknown
Size: 234 chars
Content:
----------------------------------------
Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution 16 16 M.Dalvi,N.Tandon,andP.Clark.”Domain-targeted,highprecisionknowledgeextraction.”Transactionsof theAssociationforComputationalLinguistics5(2017):233-246. 52/85
----------------------------------------

CHUNK 509
File: unknown
Size: 286 chars
Content:
----------------------------------------
(cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. (cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. 
----------------------------------------

CHUNK 510
File: unknown
Size: 403 chars
Content:
----------------------------------------
(cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 53/85
----------------------------------------

CHUNK 511
File: unknown
Size: 580 chars
Content:
----------------------------------------
(cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. (cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. 
----------------------------------------

CHUNK 512
File: unknown
Size: 109 chars
Content:
----------------------------------------
17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 54/85
----------------------------------------

CHUNK 513
File: unknown
Size: 477 chars
Content:
----------------------------------------
(cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. (cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. 
----------------------------------------

CHUNK 514
File: unknown
Size: 212 chars
Content:
----------------------------------------
(cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 55/85
----------------------------------------

CHUNK 515
File: unknown
Size: 365 chars
Content:
----------------------------------------
(cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. 
----------------------------------------

CHUNK 516
File: unknown
Size: 324 chars
Content:
----------------------------------------
(cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 56/85
----------------------------------------

CHUNK 517
File: unknown
Size: 233 chars
Content:
----------------------------------------
(cid:73) Triple quality evaluation Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. 
----------------------------------------

CHUNK 518
File: unknown
Size: 456 chars
Content:
----------------------------------------
(cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. (cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 57/85
----------------------------------------

CHUNK 519
File: unknown
Size: 198 chars
Content:
----------------------------------------
Towards a Legal Knowledge Graph How to Build a Legal KG - Our Solution (cid:73) Corpus: 650 millions of US Case law documents. (cid:73) Sentence Selection: LexNLP 17 was used for sentence spliting. 
----------------------------------------

CHUNK 520
File: unknown
Size: 491 chars
Content:
----------------------------------------
(cid:73) Tuple Generation: OpenIE 18 was used to extract an initial set of triples from the sentences. (cid:73) Refinement and Scoring: Feature-based machine learning method based on partial tuples scored manually. (cid:73) Relation Canonicalization: Identify the equivalent or similar relations, and map them to a canonical generalized relation. (cid:73) Triple quality evaluation 17 https://lexpredict-lexnlp.readthedocs.io/en/latest/ 18 https://nlp.stanford.edu/software/openie.html 58/85
----------------------------------------

CHUNK 521
File: unknown
Size: 423 chars
Content:
----------------------------------------
(cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! Knowledge Graph Quality Evaluation Why does KG quality matter? 
----------------------------------------

CHUNK 522
File: unknown
Size: 144 chars
Content:
----------------------------------------
19 (cid:73) ”Garbage in, garbage out” 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 59/85
----------------------------------------

CHUNK 523
File: unknown
Size: 378 chars
Content:
----------------------------------------
(cid:73) Data quality and Information Quality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! Knowledge Graph Quality Evaluation Why does KG quality matter? 
----------------------------------------

CHUNK 524
File: unknown
Size: 189 chars
Content:
----------------------------------------
19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 60/85
----------------------------------------

CHUNK 525
File: unknown
Size: 332 chars
Content:
----------------------------------------
(cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! Knowledge Graph Quality Evaluation Why does KG quality matter? 
----------------------------------------

CHUNK 526
File: unknown
Size: 235 chars
Content:
----------------------------------------
19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 61/85
----------------------------------------

CHUNK 527
File: unknown
Size: 175 chars
Content:
----------------------------------------
(cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! 
----------------------------------------

CHUNK 528
File: unknown
Size: 392 chars
Content:
----------------------------------------
Knowledge Graph Quality Evaluation Why does KG quality matter? 19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 62/85
----------------------------------------

CHUNK 529
File: unknown
Size: 87 chars
Content:
----------------------------------------
(cid:73) A practical knowledge graph quality evaluation framework is therefore needed! 
----------------------------------------

CHUNK 530
File: unknown
Size: 480 chars
Content:
----------------------------------------
Knowledge Graph Quality Evaluation Why does KG quality matter? 19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications 19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 63/85
----------------------------------------

CHUNK 531
File: unknown
Size: 461 chars
Content:
----------------------------------------
Knowledge Graph Quality Evaluation Why does KG quality matter? 19 (cid:73) ”Garbage in, garbage out” (cid:73) Foundation of KG-based Applications (cid:73) Data quality and Information Quality (cid:73) Intrinsic: Syntactic validity, semantic accuracy, consistence, correct, completeness (cid:73) ”Fit for purpose”: Quality evaluation should be designed based on applications (cid:73) A practical knowledge graph quality evaluation framework is therefore needed! 
----------------------------------------

CHUNK 532
File: unknown
Size: 106 chars
Content:
----------------------------------------
19 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 64/85
----------------------------------------

CHUNK 533
File: unknown
Size: 76 chars
Content:
----------------------------------------
Knowledge Graph Quality Evaluation Basic Quality Requirements for a KG 65/85
----------------------------------------

CHUNK 534
File: unknown
Size: 676 chars
Content:
----------------------------------------
Knowledge Graph Quality Evaluation Comprehensive KG Quality Requirements 20 1. accessibility 2. accuracy 3. appropriate 4. believability (veracity) amount 5. complete- 6. concise rep- 7. consistent 8. cost- ness resentation representation effectiveness 9. easy of ma- 10. easy of op- 11. easy of un- 12. flexibility nipulating eration derstanding 13. free-of- 14. interoper- 15. objectivity 16. relevancy error ability 17. reputation 18. security 19. timeliness 20. traceability (velocity) 21. under- 22. value- 23. variety **fitness for standability added use** 20 EliasandUmutcan.”Buildingalarge-scale,accurateandfreshknowledgegraph.”SemanticsConference 2019,Tutorial. 66/85
----------------------------------------

CHUNK 535
File: unknown
Size: 189 chars
Content:
----------------------------------------
Knowledge Graph Quality Evaluation Knowledge graph quality evaluation framework 21 21 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 67/85
----------------------------------------

CHUNK 536
File: unknown
Size: 191 chars
Content:
----------------------------------------
Knowledge Graph Quality Evaluation KG quality requirements mapped to applications 22 22 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 68/85
----------------------------------------

CHUNK 537
File: unknown
Size: 197 chars
Content:
----------------------------------------
Knowledge Graph Quality Evaluation KG quality requirements mapped to quality dimensions 23 23 H.Chen,G.Cao,J.Chen,J.Ding,“APracticalFrameworkforEvaluatingtheQualityofKnowledge Graph”,CCKS2019 69/85
----------------------------------------

CHUNK 538
File: unknown
Size: 69 chars
Content:
----------------------------------------
KG Applications Significant Publications on Applications of KGs 70/85
----------------------------------------

CHUNK 539
File: unknown
Size: 307 chars
Content:
----------------------------------------
KG Applications Use Cases of Knowledge Graphs 24 (cid:73) Information Retrieval, Search (cid:73) Recommendation (cid:73) Questions Answering (cid:73) Medical Text Analysis (cid:73) Conversation (cid:73) ... 24 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 71/85
----------------------------------------

CHUNK 540
File: unknown
Size: 105 chars
Content:
----------------------------------------
KG Applications Use Case: Search 25 25 Y.Xiao.”IntroductiontoKnowledgeGraphs”.FudanUniversity,2017. 72/85
----------------------------------------

CHUNK 541
File: unknown
Size: 108 chars
Content:
----------------------------------------
KG Applications Use Case: Search (cid:73) Offer related knowledge that is related to searched results. 73/85
----------------------------------------

CHUNK 542
File: unknown
Size: 171 chars
Content:
----------------------------------------
KG Applications Use Case: Search (cid:73) Recommend semantically related content. (cid:73) Example: What a search engine should recommend if a user search for “flu shot”? 
----------------------------------------

CHUNK 543
File: unknown
Size: 138 chars
Content:
----------------------------------------
(cid:73) Recommend conceptually consistent content. (cid:73) Example: What Netflix should recommend if a user search for “Star War”? 74/85
----------------------------------------

CHUNK 544
File: unknown
Size: 65 chars
Content:
----------------------------------------
KG Applications Use Case: Question Answering Fact Answering 75/85
----------------------------------------

CHUNK 545
File: unknown
Size: 70 chars
Content:
----------------------------------------
KG Applications Use Case: Question Answering Knowledge-based Q&A 76/85
----------------------------------------

CHUNK 546
File: unknown
Size: 121 chars
Content:
----------------------------------------
KG Applications Use Case: Medical Text Analytics 26 26 Y.Xiao.”IntroductiontoKnowledgeGraphs”.FudanUniversity,2017. 77/85
----------------------------------------

CHUNK 547
File: unknown
Size: 156 chars
Content:
----------------------------------------
Summary KG for AI: ML + NLP + Conflation + Inference 27 27 Y.Gao,etal.”Buildingalarge-scale,accurateandfreshknowledgegraph.”KDD-2018,Tutorial39(2018). 78/85
----------------------------------------

CHUNK 548
File: unknown
Size: 67 chars
Content:
----------------------------------------
Summary Combine KG Construction, Evaluation, with Application 79/85
----------------------------------------

CHUNK 549
File: unknown
Size: 645 chars
Content:
----------------------------------------
(cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. (cid:73) Use language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic representation 29. GNN might be a good answer. (cid:73) ... Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on downstream tasks deserved attentions 28. 28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 
----------------------------------------

CHUNK 550
File: unknown
Size: 98 chars
Content:
----------------------------------------
29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 80/85
----------------------------------------

CHUNK 551
File: unknown
Size: 478 chars
Content:
----------------------------------------
(cid:73) Use language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic representation 29. GNN might be a good answer. (cid:73) ... Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on downstream tasks deserved attentions 28. (cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. 
----------------------------------------

CHUNK 552
File: unknown
Size: 265 chars
Content:
----------------------------------------
28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 81/85
----------------------------------------

CHUNK 553
File: unknown
Size: 478 chars
Content:
----------------------------------------
(cid:73) Use language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic representation 29. GNN might be a good answer. (cid:73) ... Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on downstream tasks deserved attentions 28. (cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. 
----------------------------------------

CHUNK 554
File: unknown
Size: 265 chars
Content:
----------------------------------------
28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 82/85
----------------------------------------

CHUNK 555
File: unknown
Size: 478 chars
Content:
----------------------------------------
(cid:73) ... Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on downstream tasks deserved attentions 28. (cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. (cid:73) Use language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic representation 29. GNN might be a good answer. 
----------------------------------------

CHUNK 556
File: unknown
Size: 265 chars
Content:
----------------------------------------
28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 83/85
----------------------------------------

CHUNK 557
File: unknown
Size: 465 chars
Content:
----------------------------------------
Summary Future Directions of KG (cid:73) All KG creation methods have their advantages and disadvantages, how to set the most appropriate method based on downstream tasks deserved attentions 28. (cid:73) A unified framework integrating different reasoning paradigms needs to be formalized 28. (cid:73) Use language models like BERT in conjunction with knowledge graph embeddings is the future work to enhance semantic representation 29. GNN might be a good answer. 
----------------------------------------

CHUNK 558
File: unknown
Size: 278 chars
Content:
----------------------------------------
(cid:73) ... 28 P.Bonatti,etal.”Knowledgegraphs:newdirectionsforknowledgerepresentationonthesemanticweb (Dagstuhlseminar18371).”SchlossDagstuhl-Leibniz-ZentrumfuerInformatik,2019. 29 L.Yao,etal.”KG-BERT:BERTforKnowledgeGraphCompletion.”arXivpreprintarXiv:1909.03193(2019). 84/85
----------------------------------------

CHUNK 559
File: unknown
Size: 48 chars
Content:
----------------------------------------
Questions, Comments and Suggestions? Thank you! 
----------------------------------------

CHUNK 560
File: unknown
Size: 5 chars
Content:
----------------------------------------
85/85
----------------------------------------

