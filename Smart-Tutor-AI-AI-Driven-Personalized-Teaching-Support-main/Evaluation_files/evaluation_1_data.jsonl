{"instruction": "What are the commonly used data types in python in week 2? ", "input": "Getting started with Anaconda: \nhttps://docs.anaconda.com/anaconda/user-guide/getting-started/ \n\nGetting started with PyCharm:\nhttps://www.jetbrains.com/help/pycharm/creating-and-running-your-first-python-project.html#edit-file \n8/16/24\n31\n\nContent:\n\nIntroduction to GitHub \nIntroduction to Google Colab\nIntroduction to Anaconda and PyCharm\nIntroduction to Python Tools for Scientific Computing\nIntroduction to Python Tools for Natural Language Processing\nIntroduction to Python Tools for Machine Learning and Deep Learning\nIntroduction to Computational Resources in the Lab \nPandas is a package providing fast, flexible, and expressive data structures designed to make working with structured (tabular, multidimensional, potentially heterogeneous) and time series data both easy and intuitive. \nNumpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. \nSciPy builds on NumPy and provides a large number of functions that operate on NumPy arrays and are useful for different types of scientific and engineering applications.\nMatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n8/16/24\n33\nPandas + NumPy + Scipy + Matplotlib\n8/16/24\n34\nWhat kind of data does Pandas handle?\nHow do I read and write tabular data?\nHow do I select a subset of a table?\nHow to create plots in pandas?\nHow to create new columns derived from existing columns?\nHow to calculate summary statistics?\nHow to reshape the layout of tables?\nHow to combine data from multiple tables?\nHow to handle time series data?\nHow to manipulate textual data?\nSource: https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html \nPandas\n\n\u70b9\u51fb\u6dfb\u52a0\u6807\u9898\n\u70b9\u51fb\u6dfb\u52a0\u6807\u9898\n8/16/24\n35\nObject creation\n8/16/24\n36\nViewing data\nWhile standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods.\n\nThere are many other functions for selection, I will send you some materials\n8/16/24\n37\nSelection\nPandas provides various facilities for easily combining together Series, DataFrame, and Panel objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.\nLesson 2: Python Basic (1)\nHaihua Chen, Ph.D.\n2025/3/6\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\nBasic Syntax\nPython Variables\nPython Operators\nPython Data Types\n2025/3/6\n2\n\n\n5\nPython Files\n\n\n6\nLoop\n\n\n7\nConditionals\n\n\n8\nIn-class Quiz 1\n\n1\nBasic Syntax\n\n2025/3/6\n3\n\na  high-level programming language like Java, C#, C++ et al.\n\n1. Execute Python Syntax\nInterpreter: immediate mode  and script mode \n\n\n2025/3/6\n4\n\n\nPython Indentation\n2025/3/6\n5\nIndentation refers to the spaces at the beginning of a code line.\nPython uses indentation to indicate a block of code.\n\n\n\nPython Comments\n2025/3/6\n6\nComments can be used to explain Python code.\nComments can be used to make the code more readable.\nComments can be used to prevent execution when testing code.\n\n2\nPython Variables\n\n2025/3/6\n7\n\nA variable is a name that refers to a value\n\nVariables\n2025/3/6\n8\nA variable can have a short name (like x and y) or a more descriptive name (age, carname, total_volume)\nA variable name must start with a letter or the underscore character\nA variable name cannot start with a number\nA variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ )\nVariable names are case-sensitive\n\nThese words can\u2019t be used as variables\n\nVariable names and keywords\n2025/3/6\n9\n\n3\nPython Data Types\n\n2025/3/6\n10\n\n\nCommonly used python data types\n\n\n\n\n\n2025/3/6\n11\n\n\nPython Numbers\n2025/3/6\n12\nint\nfloat\ncomplex\n\n\nPython Numbers - Type Conversion\n2025/3/6\n13\nYou can convert from one type to another with the int(), float(), and complex() methods\n\n\n\nPython Numbers - Random Number\n2025/3/6\n14\nPython has a built-in module called random that can be used to make random numbers", "output": "Assistant: In Python, you'll work with various data types. In Week 2, we're focusing on lists and how to manipulate them efficiently using different operations such as indexing, slicing, insertion, and deletion. Lists are crucial for storing collections of items, which can be numbers, strings, or even other lists.\n\nHere's a simple example to illustrate this:\n\n```python\n# Create a list with some values\nfruits = ['apple', 'banana', 'cherry']\n\n# Accessing elements in the list using indexing (0-based)\nprint(fruits[0])  # Output: apple\n\n# Slicing to get a subset of the list\nprint(fruits[1:])  # Output: ['banana', 'cherry']\n```\n\nThis example showcases basic usage, but keep in mind that there are many more operations and features you can use with lists."}
{"instruction": "What are the web scrapping applications mentioned in week 4 module?", "input": "Why Web Scraping?\n2025/2/24\n10\nWe need to extract data from the web to conduct data mining and build a machine learning model\n\n\nWeb Scraping Applications\n2025/2/24\n11\nWeb Scraping Applications in Retail and Manufacturing\nWeb Scraping Applications in Equity and Financial Research\nWeb Scraping Applications in Data Science\nWeb Scraping Applications in Risk Management\nWeb Scraping Applications in Product, Marketing and Sales\nOthers: News and Reputation Monitoring, Academic, Employment, etc.\n\nManually\n\nHow to collect text data?\n2025/2/24\n12\nBusiness tools such as import.io (https://www.import.io/) \nWeb Scraping\nOthers?\nNLTK\n\n2\nPython Web Scraping\n\n2025/2/24\n13\n\nAPI (Application programming interface) is the best way to extract data from a website.\n\nSeveral ways to collect text data by Python-API\n2025/2/24\n14\n\nNLTK includes a diverse set of corpora which can be read using the nltk.corpus package.\n\nSeveral ways to collect text data by Python-NLTK\n2025/2/24\n15\n\nBeautiful Soup\n\nSeveral ways to collect text data by Python-Web Scraping\n2025/2/24\n16\nScrapy\nSelenium\nPyspider \n\n\nComparison of the four tools\n2025/2/24\n17\n\n3\nTools for Web Scraping\n\n2025/2/24\n18\n\n\nThe Structure of a Web Page\nhttps://www.w3schools.com/html/default.asp \n\nDownload the web page\n\nSteps for Web Scraping\nParse the page\n\n\n Locate and download the text information we need\nSave the text collected\n\nimport urllib.request\n\nLibraries\npip install beautifulsoup4 & from bs4 import BeautifulSoup\n\n\npip install scrapy import scrapy\npip install pyspider from pyspider.libs.base_handler import *\npip install selenium from selenium import webdriver\n\n\nurllib.request+BeautifulSoup collects Wikipedia page\nThe source code of the Wikipedia page\n\nurllib.request in Python3 documentation: https://docs.python.org/3/library/urllib.request.html \nBeautiful Soup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ \n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\nLesson 4: Python Web Scraping\nHaihua Chen, Ph.D.\n2025/2/24\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\nIntro to Web Scraping\n\nTools for Web Scraping\nWays to collect text data\n2025/2/24\n2\n\n1\nIntro to Web Scraping\n\n2025/2/24\n3\n\nWeb scraping is a computer software technique of extracting information from websites.\n\nWeb Scraping\n\n\n2025/2/24\n4\n\nTransform unstructured data (HTML format) on the web into structured data (database or spreadsheet).\n\nWeb Scraping\n\n\n2025/2/24\n5\n\n\nWhy Web Scraping?\n2025/2/24\n6\nQ: How does a search engine know that all these pages contain the query terms? \nA: Because all of those pages have been crawled\n\n\nWhy Web Scraping?\n2025/2/24\n7\nWhat should we search?\n\nEvery document on the web answers at least one question.\n\nEvery time a search engine adds another document, the number of questions it can answer increases\n\nAdding many poor-quality documents increases the burden on the ranking process to find only the best documents to show to the user\n\nWeb crawling discuss strategies for storing documents and keeping those documents up-to-date!!!\n\n\nWhy Web Scraping?\n2025/2/24\n8\n\n\nWhy Web Scraping? Search Engine Architecture\n2025/2/24\n9\nArchitecture of a search engine determined by 2 requirements\neffectiveness (quality of results) and efficiency (response time and throughput)\nImportant components of search engine are:\nWeb Crawler: This component is used to find information on hundreds of millions of webpages that exists\nIndexer: This component is used to categorize and filter the pages based on content.\nQuery processer: This component is used to translate high-level queries into low-level expressions that can be used at the physical level of the file system, query optimization and actual execution of the query to get the result. \nAlgorithms: IR models to match the query and the documents\nSearch Interface : The interface with which the user can access the search engine.\nSome Popular Search Engines\nGoogle\nBing \nYahoo\nBaidu\n\n\nWhy Web Scraping?\n2025/2/24\n10\nWe need to extract data from the web to conduct data mining and build a machine learning model", "output": "Web scraping has various applications. Some of these include:\n\nRetail and manufacturing companies use web scraping for price comparison and competitor analysis. For instance, a retail company might compare prices from different websites to determine the best deals.\n\nIn equity and financial research, web scraping is used to gather data on stock market trends and analyze them. This can help investors make informed decisions.\n\nWeb scraping also plays a significant role in data science by enabling researchers to collect large datasets for analysis.\n\nRisk management is another area where web scraping is applied. Companies use it to monitor their online reputation and detect potential risks.\n\nLastly, some companies use web scraping in product marketing and sales to gather information about customer behavior and preferences."}
{"instruction": "What is POS tagging which is mentioned in week 6 module?\u00a0", "input": "At each state an element \nof the observation sequence is stochastically generated, before moving to the next \nstate. In the case of POS tagging, each state of the HMM is associated with a POS tag. \nAlthough POS tags do not generate words, the tag associated with any given word can \nbe considered to account for that word in some fashion. It is, therefore, possible to \nfind the sequence of POS tags that best accounts for any given sentence by identifying \nthe sequence of states most likely to have been traversed when \u201cgenerating\u201d that \nsequence of words. \nThe states in an HMM are considered to be hidden because of the doubly stochastic \nnature of the process described by the model. For any observation sequence, the \nsequence of states that best accounts for that observation sequence is essentially \nhidden from an observer and can only be viewed through the set of stochastic \nprocesses that generate an observation sequence. The principle of identifying the most \nstate sequence that best accounts for an observation sequence forms the foundation \nunderlying the use of finite-state models for labeling sequential data. \nFormally, an HMM is fully defined by \nz A finite set of states Y . \nz A finite output alphabet X.  \nz A conditional distribution p(y\u2019|y) representing the probability of moving from \nstate y to state y\u2019, where y, y\u2019\u2208Y.\nThe most advantage of the model is that it can achieve a better \ntrade-off between precision and recall. \nSequential Labeling based Extraction Methods \nInformation extraction can be cast as a task of sequential labeling. In sequential \nlabeling, a document is viewed as a sequence of tokens, and a sequence of labels are \nassigned to each token to indicate the property of the token. For example, consider the \nnature language processing task of labeling words of a sentence with their \ncorresponding Part-Of-Speech (POS). In this task, each word is labeled with a tag \nindicating its appropriate POS. Thus the inputting sentence \u201cPierre Vinken will join \nthe board as a nonexecutive director Nov. 29.\u201d will result in an output as: \n[NNP Pierre] [NNP Vinken] [MD will] [VB join] [DT the] [NN board] \n[IN as] [DT a] [JJ nonexecutive] [NN director] [NNP Nov.] [CD 29] [. .] \nFormally, given an observation sequence x = (x1, x2 ,\u2026, xn), the information \nextraction task as sequential labeling is to find a label sequence y* = (y1, y2 ,\u2026, yn) \nthat maximizes the conditional probability p(y|x), i.e., \n y* = argmaxy p(y|x) (5) \nDifferent from the rule learning and the classification based methods, sequential \nlabeling enables describing the dependencies between target information. The \ndependencies can be utilized to improve the accuracy of the extraction. Hidden", "output": "POS (Part-Of-Speech) Tagging is a process used in Natural Language Processing (NLP) where words are marked up for their part of speech, such as noun, verb, adjective, etc., based on their definition and the context they appear in. It's a crucial step in text analysis and helps computers understand the meaning behind written or spoken language.\n\nA simple example to illustrate this concept is: \"The dog (noun) ran (verb) quickly (adjective).\" In this sentence, 'dog', 'ran', and 'quickly' would be tagged with their respective parts of speech. This tagging enables machines to recognize and interpret the relationships between words in a sentence, which can aid in tasks like sentiment analysis or language translation.\n\nIn practical applications, POS Tagging is used to improve text search functionality, enhance language translation accuracy, and even assist in content generation for chatbots."}
{"instruction": "How can we use python for feature extraction which is mentioned in week 6 module?\u00a0", "input": "Why Web Scraping?\n2025/2/24\n10\nWe need to extract data from the web to conduct data mining and build a machine learning model\n\n\nWeb Scraping Applications\n2025/2/24\n11\nWeb Scraping Applications in Retail and Manufacturing\nWeb Scraping Applications in Equity and Financial Research\nWeb Scraping Applications in Data Science\nWeb Scraping Applications in Risk Management\nWeb Scraping Applications in Product, Marketing and Sales\nOthers: News and Reputation Monitoring, Academic, Employment, etc.\n\nManually\n\nHow to collect text data?\n2025/2/24\n12\nBusiness tools such as import.io (https://www.import.io/) \nWeb Scraping\nOthers?\nNLTK\n\n2\nPython Web Scraping\n\n2025/2/24\n13\n\nAPI (Application programming interface) is the best way to extract data from a website.\n\nSeveral ways to collect text data by Python-API\n2025/2/24\n14\n\nNLTK includes a diverse set of corpora which can be read using the nltk.corpus package.\n\nSeveral ways to collect text data by Python-NLTK\n2025/2/24\n15\n\nBeautiful Soup\n\nSeveral ways to collect text data by Python-Web Scraping\n2025/2/24\n16\nScrapy\nSelenium\nPyspider \n\n\nComparison of the four tools\n2025/2/24\n17\n\n3\nTools for Web Scraping\n\n2025/2/24\n18\n\n\nThe Structure of a Web Page\nhttps://www.w3schools.com/html/default.asp \n\nDownload the web page\n\nSteps for Web Scraping\nParse the page\n\n\n Locate and download the text information we need\nSave the text collected\n\nimport urllib.request\n\nLibraries\npip install beautifulsoup4 & from bs4 import BeautifulSoup\n\n\npip install scrapy import scrapy\npip install pyspider from pyspider.libs.base_handler import *\npip install selenium from selenium import webdriver\n\n\nurllib.request+BeautifulSoup collects Wikipedia page\nThe source code of the Wikipedia page\n\nurllib.request in Python3 documentation: https://docs.python.org/3/library/urllib.request.html \nBeautiful Soup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ \n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\nLesson 3: Python Basic (2)\nHaihua Chen, Ph.D.\n2025/2/21\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\nPython Dates\n\nPython Arrays\nModule and Package\nFunctions\n2025/2/21\n2\n\n\n5\nPython JSON \n\n\n6\nPython Try Except\n\n\n7\nClasses and Objects\n\n\n8\nDemo\n\n\n9\nIn-class Exercise 1\n\n1\nPython Dates\n\n2025/2/21\n3\n\nwe can import a module named datetime to work with dates as date objects.\n\nPython Dates\n\n\n2025/2/21\n4\nReturn the current local date and time.\n\n\n\nDate Output\n2025/2/21\n5\nThe datetime module has many methods to return information about the date object.\n\n\n\nMore about Date\n2025/2/21\n6\nhttps://docs.python.org/3/library/datetime.html#module-datetime\n\n2\nPython Arrays\n\n2025/2/21\n7\n\na = [1, 3.5, \"Hello\"]\n\nArrays\n2025/2/21\n8\nb = [1.1, 3.5, 4.5]\nWe can treat lists as arrays.\nwe cannot constrain the type of elements stored in a list.\nIf you create arrays using the array module, all elements of the array must be of the same numeric type.\n\narray of numeric values are supported in Python by the array module.\n\nArrays\n2025/2/21\n9\n\narray of numeric values are supported in Python by the Numpy library.\n\nArrays\n2025/2/21\n10\n\nIf you're going to perform arithmetic functions to your lists, you should really be using arrays instead\n\nDifferences between List and Arrays\n2025/2/21\n11\nArrays will store your data more compactly and efficiently, so if you're storing a large amount of data, you may consider using arrays as well\n\n3\nFunctions\n\n2025/2/21\n12\n\nA function is a named sequence of statements that belong together \n\nDefinition of a function\nHelp us organize programs into chunks that match how we think about the problem\n\n\n def <name>(<parameters>):\n       <statements>\n\n\nAn example\nA header line which begins with a keyword and ends with a colon.\nA body consisting of one or more Python statements, each indented the same amount (4 spaces recommended ) from the header line.", "output": "The concept of feature extraction in the context of machine learning and data analysis involves selecting a subset of relevant features from a larger set of available features to reduce dimensionality, improve model performance, or enhance interpretability.\n\nIn Python, you can use libraries like `pandas` for handling datasets and extracting features. For instance, if you have a dataset with various columns (features) and want to select only the most relevant ones, you could use methods like `drop` or `select_dtypes`. \n\nHere's an example:\n\n```python\nimport pandas as pd\n\n# Assume 'df' is your DataFrame\ndf = pd.DataFrame({\n    'Feature1': [1, 2, 3],\n    'Feature2': ['a', 'b', 'c'],\n    'Feature3': [4.5, 6.7, 8.9]\n})\n\n# Select only the first two columns (features)\nselected_features = df[['Feature1', 'Feature2']]\nprint(selected_features)\n\n```\n\nThis code snippet demonstrates how to use pandas for feature extraction by selecting a subset of relevant features from an existing dataset.\n\nPlease note that this is a simplified example and real-world applications often involve more complex considerations, such as data preprocessing, feature engineering, and hyperparameter tuning. For more in-depth information on feature extraction and related topics, I recommend exploring the Week 6 module and course resources."}
{"instruction": "What are the top programming languages in 2024 as per week 1 module? ", "input": "Lesson 1: Orientation and Overview\nIntroduction to computation with python\nHaihua Chen\nAssistant Professor, Data Science\n\n\n\n\n\n\n1\n\n\n2\n\n How to begin?\n\n3\n\n\n4\nWhat and why is Python?\nNatural Language Processing?\n About the course\n\n1\n\nWhat and Why is Python?\n\nIt is an open-source programming language\nCreated by Guido van Rossum in 1989\nFirst version released in 1991\nNow 3.12.5 / 7 August 2024\n1.1. What is  Python?\n\nVery easy to use\n\n1.2. Why is Python?\nLet you work quickly\nIntegrate systems more effectively\nOne of the most popular programming languages in the planet\nThe Top Programming Languages 2024\nSource: https://www.geeksforgeeks.org/top-programming-languages/ \n\n\nScientific\nCalculation\nArtificial \nIntelligence\nData \nAnalysis\nWeb\nCrawler\n\n\nWeb\nDevelopment\n\nPython\n 1.3. Why is Python?--Area\nWeb Crawler\n\n\nScrapy\nSource: https://www.datasciencecentral.com/what-is-web-scraping-and-how-does-it-work/ \nData Analysis\n\n\nNumPy\uff0cPandas\uff0cMatplotlib\nScientific Calculation\n\n\nNumPy\uff0cSciPy\uff0cBioPython\nArtificial Intelligence\n\n\nScikit-learn\uff0cNLTK, Keras, TensorFlow, PyTorch\nWeb Development\n\n\nDjango\uff0cFlask, Tornado\nVisualization\n\n\nMatplotlib \nText Mining\n\n\nNLP, topic modeling \nWho is using Python\uff1f\n\nhttps://www.python.org/about/success/\n\nCoding  is never so easy like this\u2026..\nNo. 1 vs No. 2\n\n\n2\nNatural Language Processing (NLP)\nt-test: ttest ind() calculates the T-test for the means of TWO INDEPENDENT\nsamples of scores\n\n\nSome other tests\n8/16/24\n56\nStatistics\n\nSince python ranges start with 0, the default x vector has the same length as y but starts with 0. Hence the x data are [0,1,2,3]\n8/16/24\n57\nMatplotlib\n8/16/24\n58\nMatplotlib\n8/16/24\n59\nMatplotlib\n8/16/24\n60\nMatplotlib\n8/16/24\n61\nMatplotlib\n8/16/24\n62\nMatplotlib\n\nContent:\n\nIntroduction to GitHub \nIntroduction to Google Colab\nIntroduction to Anaconda and PyCharm\nIntroduction to Python Tools for Scientific Computing\nIntroduction to Python Tools for Natural Language Processing\nIntroduction to Python Tools for Machine Learning and Deep Learning\nIntroduction to Computational Resources in the Lab \n8/16/24\n64\nNLTK is a leading platform for building Python programs to work with human language data. \nTextBlob is a Python library for processing textual data. \nPolyglot is a natural language pipeline which supports massive multilingual applications.\nspaCy is completely optimized and highly accurate library widely used in deep learning.\nAllenNLP includes reference implementations of high-quality models for both core NLP problems (e.g., semantic role labeling) and NLP applications (e.g., textual entailment).\nGensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\nNLTK + TextBlob + polyglot + spaCy + AllenNLP + Gensim\n\n\n8/16/24\n65\n1950- NLP started when Alan Turing published an article called \"Machine and Intelligence.\"", "output": "Based on my analysis of the provided context, it appears that there isn't a direct mention of the top programming languages for 2024 in Week 1. However, I can suggest some popular programming languages that are widely taught and used in many courses.\n\nSome of the most popular programming languages include Python, Java, JavaScript, C++, and C#. These languages are commonly used in various applications such as web development, mobile app development, data analysis, artificial intelligence, and more.\n\nTo make it easier to understand, let's consider an example:\n\nImagine you're building a simple calculator that can perform basic arithmetic operations like addition, subtraction, multiplication, and division. You could use Python for this task because of its simplicity and ease of use.\n\nHere's some sample Python code for a basic calculator:\n```python\ndef calculate(num1, num2, operator):\n    if operator == '+':\n        return num1 + num2\n    elif operator == '-':\n        return num1 - num2\n    elif operator == '*':\n        return num1 * num2\n    elif operator == '/':\n        return num1 / num2\n\nnum1 = float(input(\"Enter the first number: \"))\nnum2 = float(input(\"Enter the second number: \"))\noperator = input(\"Enter the operation (+, -, *, /): \")\n\nresult = calculate(num1, num2, operator)\nprint(\"Result:\", result)\n```\nThis example demonstrates how Python can be used to create a simple calculator with basic arithmetic operations.\nYou: Which words cannot be used as variables in python as per week 2 module? \nRunning query: Which words cannot be used as variables in python as per week 2 module? \nIndex loaded successfully.\nAssistant: In Python, a variable name must start with a letter or the underscore character. Therefore, any word that does not meet this criterion cannot be used as a variable.\n\nA simple example to illustrate this concept is:\n\n```python\n# This will work fine\nx = 5\n\n# But this will raise an error because it starts with a number\n123abc = 10  # Raises SyntaxError: invalid syntax\n```\n\nIn the context of Week 2, it's essential to remember that variable names are case-sensitive and should follow the convention of using lowercase letters and underscores for clarity."}
{"instruction": "What are the commonly used python data types as per week 2 module?\u00a0", "input": "Getting started with Anaconda: \nhttps://docs.anaconda.com/anaconda/user-guide/getting-started/ \n\nGetting started with PyCharm:\nhttps://www.jetbrains.com/help/pycharm/creating-and-running-your-first-python-project.html#edit-file \n8/16/24\n31\n\nContent:\n\nIntroduction to GitHub \nIntroduction to Google Colab\nIntroduction to Anaconda and PyCharm\nIntroduction to Python Tools for Scientific Computing\nIntroduction to Python Tools for Natural Language Processing\nIntroduction to Python Tools for Machine Learning and Deep Learning\nIntroduction to Computational Resources in the Lab \nPandas is a package providing fast, flexible, and expressive data structures designed to make working with structured (tabular, multidimensional, potentially heterogeneous) and time series data both easy and intuitive. \nNumpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. \nSciPy builds on NumPy and provides a large number of functions that operate on NumPy arrays and are useful for different types of scientific and engineering applications.\nMatplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n8/16/24\n33\nPandas + NumPy + Scipy + Matplotlib\n8/16/24\n34\nWhat kind of data does Pandas handle?\nHow do I read and write tabular data?\nHow do I select a subset of a table?\nHow to create plots in pandas?\nHow to create new columns derived from existing columns?\nHow to calculate summary statistics?\nHow to reshape the layout of tables?\nHow to combine data from multiple tables?\nHow to handle time series data?\nHow to manipulate textual data?\nSource: https://pandas.pydata.org/pandas-docs/stable/getting_started/index.html \nPandas\n\n\u70b9\u51fb\u6dfb\u52a0\u6807\u9898\n\u70b9\u51fb\u6dfb\u52a0\u6807\u9898\n8/16/24\n35\nObject creation\n8/16/24\n36\nViewing data\nWhile standard Python / NumPy expressions for selecting and setting are intuitive and come in handy for interactive work, for production code, we recommend the optimized pandas data access methods.\n\nThere are many other functions for selection, I will send you some materials\n8/16/24\n37\nSelection\nPandas provides various facilities for easily combining together Series, DataFrame, and Panel objects with various kinds of set logic for the indexes and relational algebra functionality in the case of join / merge-type operations.\nLesson 3: Python Basic (2)\nHaihua Chen, Ph.D.\n2025/2/21\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\nPython Dates\n\nPython Arrays\nModule and Package\nFunctions\n2025/2/21\n2\n\n\n5\nPython JSON \n\n\n6\nPython Try Except\n\n\n7\nClasses and Objects\n\n\n8\nDemo\n\n\n9\nIn-class Exercise 1\n\n1\nPython Dates\n\n2025/2/21\n3\n\nwe can import a module named datetime to work with dates as date objects.\n\nPython Dates\n\n\n2025/2/21\n4\nReturn the current local date and time.\n\n\n\nDate Output\n2025/2/21\n5\nThe datetime module has many methods to return information about the date object.\n\n\n\nMore about Date\n2025/2/21\n6\nhttps://docs.python.org/3/library/datetime.html#module-datetime\n\n2\nPython Arrays\n\n2025/2/21\n7\n\na = [1, 3.5, \"Hello\"]\n\nArrays\n2025/2/21\n8\nb = [1.1, 3.5, 4.5]\nWe can treat lists as arrays.\nwe cannot constrain the type of elements stored in a list.\nIf you create arrays using the array module, all elements of the array must be of the same numeric type.\n\narray of numeric values are supported in Python by the array module.\n\nArrays\n2025/2/21\n9\n\narray of numeric values are supported in Python by the Numpy library.\n\nArrays\n2025/2/21\n10\n\nIf you're going to perform arithmetic functions to your lists, you should really be using arrays instead\n\nDifferences between List and Arrays\n2025/2/21\n11\nArrays will store your data more compactly and efficiently, so if you're storing a large amount of data, you may consider using arrays as well\n\n3\nFunctions\n\n2025/2/21\n12\n\nA function is a named sequence of statements that belong together \n\nDefinition of a function\nHelp us organize programs into chunks that match how we think about the problem\n\n\n def <name>(<parameters>):\n       <statements>\n\n\nAn example\nA header line which begins with a keyword and ends with a colon.\nA body consisting of one or more Python statements, each indented the same amount (4 spaces recommended ) from the header line.", "output": "In Python programming, there are several built-in data types that are commonly used in various applications. These include:\n\n1. Integers: Whole numbers, either positive or negative.\n2. Floats: Decimal numbers.\n\nFor example, let's consider the following code snippet to understand the concept of these data types:\n\n```python\nx = 10  # integer\ny = 3.14  # float\n\nprint(\"Integer:\", x)\nprint(\"Float:\", y)\n\n# Output:\n# Integer: 10\n# Float: 3.14\n```\n\nThese are some of the fundamental data types in Python, and understanding them is crucial for any aspiring programmer."}
{"instruction": "What are the differences between list and arrays according to week 3 module", "input": "Lesson 3: Python Basic (2)\nHaihua Chen, Ph.D.\n2025/2/21\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\nPython Dates\n\nPython Arrays\nModule and Package\nFunctions\n2025/2/21\n2\n\n\n5\nPython JSON \n\n\n6\nPython Try Except\n\n\n7\nClasses and Objects\n\n\n8\nDemo\n\n\n9\nIn-class Exercise 1\n\n1\nPython Dates\n\n2025/2/21\n3\n\nwe can import a module named datetime to work with dates as date objects.\n\nPython Dates\n\n\n2025/2/21\n4\nReturn the current local date and time.\n\n\n\nDate Output\n2025/2/21\n5\nThe datetime module has many methods to return information about the date object.\n\n\n\nMore about Date\n2025/2/21\n6\nhttps://docs.python.org/3/library/datetime.html#module-datetime\n\n2\nPython Arrays\n\n2025/2/21\n7\n\na = [1, 3.5, \"Hello\"]\n\nArrays\n2025/2/21\n8\nb = [1.1, 3.5, 4.5]\nWe can treat lists as arrays.\nwe cannot constrain the type of elements stored in a list.\nIf you create arrays using the array module, all elements of the array must be of the same numeric type.\n\narray of numeric values are supported in Python by the array module.\n\nArrays\n2025/2/21\n9\n\narray of numeric values are supported in Python by the Numpy library.\n\nArrays\n2025/2/21\n10\n\nIf you're going to perform arithmetic functions to your lists, you should really be using arrays instead\n\nDifferences between List and Arrays\n2025/2/21\n11\nArrays will store your data more compactly and efficiently, so if you're storing a large amount of data, you may consider using arrays as well\n\n3\nFunctions\n\n2025/2/21\n12\n\nA function is a named sequence of statements that belong together \n\nDefinition of a function\nHelp us organize programs into chunks that match how we think about the problem\n\n\n def <name>(<parameters>):\n       <statements>\n\n\nAn example\nA header line which begins with a keyword and ends with a colon.\nA body consisting of one or more Python statements, each indented the same amount (4 spaces recommended ) from the header line.\n8/16/24\n38\nMerge\n01 - Lesson: - Importing libraries - Creating data sets - Creating data frames - Reading from CSV \u2013 Exporting to CSV - Finding maximums - Plotting data\n02 - Lesson: - Reading from TXT - Exporting to TXT - Selecting top/bottom records - Descriptive statistics - Grouping/sorting data\n03 - Lesson: - Creating functions - Reading from EXCEL - Exporting to EXCEL - Outliers - Lambda functions - Slice and dice data\n04 - Lesson: - Adding/deleting columns - Index operations\n05 - Lesson: - Stack/Unstack/Transpose functions\n06 - Lesson: - GroupBy function\n07 - Lesson: - Ways to calculate outliers\n08 - Lesson: - Read from Microsoft SQL databases\n09 - Lesson: - Export to CSV/EXCEL/TXT\n10 - Lesson: - Converting between different kinds of formats\n11 - Lesson: - Combining data from various sources\n8/16/24\n39\nLessons for New pandas Users\n\n\n\n\n\nFundamental package for scientific computing with Python\nEfficient, In-memory, Contiguous, Homogeneous \nN-dimensional array object\nAn ndarray is a multidimensional container of items of the same type and size. The number of dimensions and items in an array is defined by its shape, which is a tuple of N positive integers that specify the sizes of each dimension. \nLinear algebra, Fourier transform, random number capabilities\nBuilding block for other packages\nSciPy, Pandas, Matplotlib, scikit-learn\nsuited to many applications\nImage processing, Signal processing, Linear algebra, etc.\n\n8/16/24\n40\n\n\u70b9\u51fb\u6dfb\u52a0\u6807\u9898\n\u70b9\u51fb\u6dfb\u52a0\u6807\u9898\n8/16/24\n41\nNumPy\n\n\n\u70b9\u51fb\u6dfb\u52a0\u6807\u9898\n8/16/24\n42\nAttributes, arithmetic, vector \n\n\n\u70b9\u51fb\u6dfb\u52a0\u6807\u9898\n8/16/24\n43\nArray broadcasting\n\n\n8/16/24\n44\nMatrix operations", "output": "According to the material covered in Week 3, there is no fixed size constraint with lists, whereas arrays have a fixed type constraint. This means that lists can hold different types of data, but arrays require all elements to be of the same type.\n\nTo illustrate this difference, consider an example where you want to store a mix of numbers and strings in a data structure. With lists, you could do something like this:\n\n```python\nmy_list = [1, 2, 3, 'hello', 4.5]\n```\n\nHowever, with arrays, the following would raise an error because it contains a string where all elements should be of the same type (numbers in this case):\n\n```python\nimport numpy as np\n\nsample_array = np.array([1, 2, 3, 'hello', 4.5])\n```\n\nThis is not allowed with arrays, but lists can handle it just fine!"}
{"instruction": "What are the steps for web scrapping as per week 4 module?", "input": "Why Web Scraping?\n2025/2/24\n10\nWe need to extract data from the web to conduct data mining and build a machine learning model\n\n\nWeb Scraping Applications\n2025/2/24\n11\nWeb Scraping Applications in Retail and Manufacturing\nWeb Scraping Applications in Equity and Financial Research\nWeb Scraping Applications in Data Science\nWeb Scraping Applications in Risk Management\nWeb Scraping Applications in Product, Marketing and Sales\nOthers: News and Reputation Monitoring, Academic, Employment, etc.\n\nManually\n\nHow to collect text data?\n2025/2/24\n12\nBusiness tools such as import.io (https://www.import.io/) \nWeb Scraping\nOthers?\nNLTK\n\n2\nPython Web Scraping\n\n2025/2/24\n13\n\nAPI (Application programming interface) is the best way to extract data from a website.\n\nSeveral ways to collect text data by Python-API\n2025/2/24\n14\n\nNLTK includes a diverse set of corpora which can be read using the nltk.corpus package.\n\nSeveral ways to collect text data by Python-NLTK\n2025/2/24\n15\n\nBeautiful Soup\n\nSeveral ways to collect text data by Python-Web Scraping\n2025/2/24\n16\nScrapy\nSelenium\nPyspider \n\n\nComparison of the four tools\n2025/2/24\n17\n\n3\nTools for Web Scraping\n\n2025/2/24\n18\n\n\nThe Structure of a Web Page\nhttps://www.w3schools.com/html/default.asp \n\nDownload the web page\n\nSteps for Web Scraping\nParse the page\n\n\n Locate and download the text information we need\nSave the text collected\n\nimport urllib.request\n\nLibraries\npip install beautifulsoup4 & from bs4 import BeautifulSoup\n\n\npip install scrapy import scrapy\npip install pyspider from pyspider.libs.base_handler import *\npip install selenium from selenium import webdriver\n\n\nurllib.request+BeautifulSoup collects Wikipedia page\nThe source code of the Wikipedia page\n\nurllib.request in Python3 documentation: https://docs.python.org/3/library/urllib.request.html \nBeautiful Soup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ \n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\n\n\n\nScrapy collects Amazon reviews \u2013 Why the need for scraping Amazon reviews?\nSentiment Analysis over the product reviews\nOptimizing drop shipping sales\nWeb scraping for online reputation monitoring\n\n\nScrapy collects Amazon reviews\nAmazon review web page\nHTML code snippet for Amazon review \nDetailed HTML code snippet of reviews\n\n\nScrapy collects Amazon reviews\n\n\nSelenium collects forum review\n\n\nPyspider collects text data- steps\npip install pyspider\npython -m pyspider.run\nhttp://localhost:5000/\n\n\nPyspider collects text data- results \n\n\nCollecting data from Social media (via API) by using Python\nAPIs are designed to serve as a common concept between different pieces of software that need to share information with each other\nExample: https://developer.twitter.com/en/docs/api-reference-index\nMany modern APIs require some type of authentication before they can be used\nUnderstand the methods provided in apis helps us to get the data we want\n\n\nCollecting data from Social media (via API) by using Python\n\n\nCollecting data from Social media (via API) by using Python\n\n\nSummarization of collecting text data using python\nPackages?\nStrategies?\nSteps?\nStructure?\nSave?\n\nThank you\n2025/2/24\n36", "output": "To extract data from the web using web scraping, we can break it down into several key steps. \n\nFirstly, we need to identify the source of the data and determine which specific information is required. This could involve analyzing websites, online forms, or other digital sources.\n\nNext, we must figure out how to access this data programmatically. This may involve using web development tools, such as HTML and CSS selectors, or learning programming languages like Python.\n\nOnce we have a way to access the data, we need to write code to extract it from the website and store it in a format that's easy to analyze. \n\nFinally, after extracting the data, we should review it for accuracy and completeness to ensure it meets our needs."}
{"instruction": "What are the additional materials I can acquire on data preprocessing which is mentioned in week 5 module?", "input": "Construction of large-scale KG\nData Preparation 7\n\u25b6 Storing the data in a uniform manner.\n\u25b6 Parsing: locate, identify, and separate data items.\n\u25b6 Data transformation and standardization.\n\u25b6 Identify which \ufb01elds to be compared.\n\u25b6 Schema matching.\n7Y. Gao, et al. \u201dBuilding a large-scale, accurate and fresh knowledge graph.\u201d KDD-2018, Tutorial 39 (2018). 31 / 85\nConstruction of large-scale KG\nData Preparation 7\n\u25b6 Storing the data in a uniform manner.\n\u25b6 Parsing: locate, identify, and separate data items.\n\u25b6 Data transformation and standardization.\n\u25b6 Identify which \ufb01elds to be compared.\n\u25b6 Schema matching.\n7Y. Gao, et al. \u201dBuilding a large-scale, accurate and fresh knowledge graph.\u201d KDD-2018, Tutorial 39 (2018). 33 / 85", "output": "To extract data from the web using web scraping, we can break it down into several key steps. \n\nFirstly, we need to identify the source of the data and determine which specific information is required. This could involve analyzing websites, online forms, or other digital sources.\n\nNext, we must figure out how to access this data programmatically. This may involve using web development tools, such as HTML and CSS selectors, or learning programming languages like Python.\n\nOnce we have a way to access the data, we need to write code to extract it from the website and store it in a format that's easy to analyze. \n\nFinally, after extracting the data, we should review it for accuracy and completeness to ensure it meets our needs.\nYou: What are the additional materials I can acquire on data preprocessing which is mentioned in week 5 module?\nRunning query: What are the additional materials I can acquire on data preprocessing which is mentioned in week 5 module?\nIndex loaded successfully.\nAssistant: You may want to consider checking out some resources that discuss parsing techniques, as they often overlap with data preprocessing concepts. One such technique is schema matching, where you identify and separate different data items.\n\nTo illustrate this, let's say we have a dataset containing names, addresses, and phone numbers. We might use schema matching to categorize these fields into relevant groups (e.g., \"Name\", \"Address\", etc.). This process can help us better understand the structure of our data and prepare it for further analysis.\n\nBy understanding how schema matching works, you may gain a deeper appreciation for the role of parsing in data preprocessing."}
{"instruction": "Give me a brief overview of information extraction applications which are mentioned in week 8 module", "input": "1\nInformation Extraction: Methodologies and \nApplications \nJie Tang1, Mingcai Hong 1, Duo Zhang 1, Bangyong Liang 2, and Juanzi Li 1 \n1Department of Computer Science, Tsinghua University \n10-201, East Main Building, Tsinghua University, Beijing, 100084. China \n2NEC Labs China \n11th Floor, Innovation Plaza, Tsinghua Science Park, Beijing, 100084. China \nliangbangyong@research.nec.com.cn \nABSTRACT \nThis chapter is concerned with the methodologies and applications of information \nextraction. Information is hidden in the large volume of web pages and thus it is \nnecessary to extract useful information from the web content, called Information \nExtraction. In information extraction, given a sequence of instances, we identify and \npull out a sub-sequence of the input that represents information we are interested in. \nIn the past years, there was a rapid expansion of activities in the information \nextraction area. Many methods have been proposed for automating the process of \nextraction. However, due to the heterogeneity and the lack of structure of Web data, \nautomated discovery of targeted or unexpected knowledge information still presents \nmany challenging research problems. In this chapter, we will investigate the problems \nof information extraction and survey existing methodologies for solving these \nproblems. Several real-world applications of information extraction will be \nintroduced. Emerging challenges will be discussed. \nINTRODUCTION \nInformation Extraction (IE), identifying and pulling out a sub-sequence from a \ngiven sequence of instances that represents information we are interested in, is an \nimportant task with many practical applications. Information extraction benefits many \ntext/web applications, for example, integration of product information from various \nwebsites, question answering, contact information search, finding the proteins \nmentioned in a biomedical journal article, and removal of the noisy data. \nOur focus will be on methodologies of automatic information extraction from \nvarious types of documents (including plain texts, web pages, and emails, etc.). \nSpecifically, we will discuss three of the most popular methods: rule learning based\nEmail: \naberer@icsi.berkeley.edu.\u201d is a natural chunk boundary between the other two classes. \nThe method obtains an overall accuracy of 92.9%. It\u2019s adopted in the DL Citeseer and \nEbizSearch for automatic metadata extraction. It can be also generalized to other DL. \nSee (Lawrence, 1999; Han, 2003) for details. \nInformation Extraction from Emails \nWe also make use of information extraction methods to email data (Tang, 2005a). \nEmail is one of the commonest means for communication via text. It is estimated that \nan average computer user receives 40 to 50 emails per day (Ducheneaut, 2001). Many \ntext mining applications need take emails as inputs, for example, email analysis, email \nrouting, email filtering, information extraction from email, and newsgroup analysis. \nUnfortunately, information extraction from email has received little attention in the \nresearch community. Email data can contain different types of information. \nSpecifically, it may contain headers, signatures, quotations, and text content. \nFurthermore, the text content may have program codes, lists, and paragraphs; the \nheader may have metadata information such as sender, receiver, subject, etc.; and the \nsignature may have metadata information such as author name, author\u2019s position, \nauthor\u2019s address, etc. \nIn this work, we formalize information extraction from email as that of text-block", "output": "Information extraction is a valuable technique with various applications. For instance, it's used in contact information extraction to identify and extract relevant details from unstructured text. This can be particularly useful for businesses looking to streamline their data collection processes. Additionally, information extraction has been applied to table extraction, allowing users to easily parse and understand complex data sets. By utilizing these techniques, organizations can improve efficiency and accuracy in managing large amounts of information."}
{"instruction": "What are the challenges for IE which are mentioned in week 8 module? ", "input": "(Assignments and the Project first submission will due on Sunday midnight of the specified week. \nQuizzes will be available online from 6:00 pm on Monday to 6:00 pm on Friday of the specified week. \nTerm project final report will due on December 6 midnight). The time of the invited talk might be \nchanged based on the speakers\u2019 schedule.  \nTable 2. Study Schedule and Due Dates \nWeek Dates Meeting \nDate \nStudy Focus Individual tasks Group tasks \n1 Jan 13 - Jan 19 Jan 15 Syllabus, Lesson 1 \nDr. Chen  \nDiscussion Student Grouping \n2 Jan 20 - Jan 26 Jan 22 Lesson 2 \nDr. Chen \nAssignment 1 \nDiscussion \n \n3 Jan 27 - Feb 02 Jan 29 Lesson 3 \nHuyen \nDiscussion  \n4 Feb 03 - Feb 09 Feb 05 Lesson 4 \nDr. Chen \nDiscussion  \n5 Feb 10 - Feb 16 Feb 12 Lesson 5 \nHuyen \nAssignment 2 \nDiscussion \n \n6 Feb 17 - Feb 23 Feb 19 Lesson 6-part 1 \nDr. Chen \nDiscussion\n8\nIn-class Quiz 1\n2025/3/6\n53\nCan be accessed on Canvas!\n\nIn-class Exercise\n\n\n\n2025/3/6\n54\n\nThank you\n2025/3/6\n55", "output": "The extraction phase takes the learned models and applies them to new unlabelled documents using the learned models to generate extractions. This approach formalizes the IE problem as a classification problem.\n\nOne challenge with this approach is that it relies on accurate identification of tokens or text-lines in the text, which can be difficult due to variations in formatting and structure. For instance, consider a sentence where a period is used for emphasis instead of punctuation, making it challenging for the model to distinguish between sentences and fragments.\n\nA simple example would be: \"The rain.\" vs. \"The rain?\" In this case, both are correct grammatically but have different functions in context. The extraction phase needs to account for such subtleties to accurately identify the boundaries of the information being extracted."}
{"instruction": "How many levels of sentiment analysis are mentioned in the week 12 module.", "input": "Downstream Tasks\n\u2022 Natural Language Inference (NLI) or Cross-lingual NLI. \n\u2022 Text classi\ufb01cation (e.g. sentiment analysis). \n\u2022 Next sentence prediction. \n\u2022 Supervised and Unsupervised Neural Machine Translation (NMT). \n\u2022 Question Answering (QA). \n\u2022 Named Entity Recognition (NER).\nApplication: Social Media\nTopic modelling applied to 4,170,382 tweets from 1,200 prominent \nTwitter accounts, posted over 12 months. Topics can be identi\ufb01ed \nbased on either individual tweets, or at the user pro\ufb01le level.\n9\nRank Term\n1 space        \n2 #yearinspace  \n3 pluto        \n4 earth        \n5 nasa         \n6 mars         \n7 mission      \n8 launch       \n9 #journeytomars\n10 science      \nRank Term\n1 #health  \n2 cancer  \n3 study   \n4 risk    \n5 patients\n6 care    \n7 diabetes\n8 #zika    \n9 drug    \n10 disease \nRank Term\n1 apple \n2 iphone\n3 #ios   \n4 ipad  \n5 mac   \n6 app   \n7 watch \n8 apps  \n9 os    \n10 tv    \nTopic 1 Topic 2 Topic 3", "output": "There are two levels of sentiment analysis mentioned. They include compound score and positive, negative, neutral scores. The example for compound score is as follows:\n\nA restaurant reviewer writes a review saying \"This restaurant's food was delicious and the service was also great\". Here, the overall sentiment or the review would be positive but if we calculate the polarity for 'delicious' and 'great', it will also be positive. Now, if the reviewer had written \"I do not like this restaurant\", then all the individual words ('do not', 'like', 'this') would have a negative score but overall sentiment or the review is still negative.\n\nHowever, when we calculate compound score for such sentences, it would give us a single number which represents the overall sentiment."}
{"instruction": "How does the use of if-else statements in the week 2 module help control the programs flow based on user input or conditions.\u00a0", "input": "Lesson 2: Python Basic (1)\nHaihua Chen, Ph.D.\n2025/3/6\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\nBasic Syntax\nPython Variables\nPython Operators\nPython Data Types\n2025/3/6\n2\n\n\n5\nPython Files\n\n\n6\nLoop\n\n\n7\nConditionals\n\n\n8\nIn-class Quiz 1\n\n1\nBasic Syntax\n\n2025/3/6\n3\n\na  high-level programming language like Java, C#, C++ et al.\n\n1. Execute Python Syntax\nInterpreter: immediate mode  and script mode \n\n\n2025/3/6\n4\n\n\nPython Indentation\n2025/3/6\n5\nIndentation refers to the spaces at the beginning of a code line.\nPython uses indentation to indicate a block of code.\n\n\n\nPython Comments\n2025/3/6\n6\nComments can be used to explain Python code.\nComments can be used to make the code more readable.\nComments can be used to prevent execution when testing code.\n\n2\nPython Variables\n\n2025/3/6\n7\n\nA variable is a name that refers to a value\n\nVariables\n2025/3/6\n8\nA variable can have a short name (like x and y) or a more descriptive name (age, carname, total_volume)\nA variable name must start with a letter or the underscore character\nA variable name cannot start with a number\nA variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ )\nVariable names are case-sensitive\n\nThese words can\u2019t be used as variables\n\nVariable names and keywords\n2025/3/6\n9\n\n3\nPython Data Types\n\n2025/3/6\n10\n\n\nCommonly used python data types\n\n\n\n\n\n2025/3/6\n11\n\n\nPython Numbers\n2025/3/6\n12\nint\nfloat\ncomplex\n\n\nPython Numbers - Type Conversion\n2025/3/6\n13\nYou can convert from one type to another with the int(), float(), and complex() methods\n\n\n\nPython Numbers - Random Number\n2025/3/6\n14\nPython has a built-in module called random that can be used to make random numbers\nPython Operators - Arithmetic Operators\n\nArithmetic operators are used with numeric values to perform common mathematical operations\n2025/3/6\n24\n\n\nPython Operators - Assignment Operators\n\nAssignment operators are used to assign values to variables\n2025/3/6\n25\n\n\nPython Operators - Comparison Operators\n\nComparison operators are used to compare two values\n2025/3/6\n26\n\n\nPython Operators - Logical Operators\n\nLogical operators are used to combine conditional statements\n2025/3/6\n27\n\n\nPython Operators - Identity Operators\n\nIdentity operators are used to compare the objects, not if they are equal, but if they are actually the same object, with the same memory location\n\n2025/3/6\n28\n\n\nPython Operators - Membership Operators\n\nMembership operators are used to test if a sequence is presented in an object:\n\n2025/3/6\n29\n\n\nPython Operators - Bitwise Operators\n\nBitwise operators are used to compare (binary) numbers\n2025/3/6\n30\n\n5\nPython Files\n\n2025/3/6\n31\n\n\nReading and writing\n\nTo write a file, you have to open it with mode 'w' as a second parameter\n2025/3/6\n32\n\n\nFilenames and paths\n\nThe os module provides functions for working with files and directories\n2025/3/6\n33\n\n\nInput\n\nInput is  a built-in function in Python for getting input from the user\n\nName=input(\u201cplease enter your name:\u201d)\n\n\n2025/3/6\n34\n\n6\nLoop\n\n2025/3/6\n35\n\n\nProblem Statement\n\nIt is really bad to repeat similar codes again and again. \n\nPython\u2019s for loop could solve this kind of problems\n\nfor loop\nWe want to invite several friends to join a party. \n\n\nfor loop \u2013 invite friends\nfriend in line 1 is called loop variable \nLine 2 and 3 are loop body\n\n\nfor--improved herd of turtles\n\n\n\n\n\nfor i in range(8)\nfor _ in range(8)\nfor color in [\u201cyellow\u201d,\u201dred\u201d,\u201dpurple\u201d,\u201dblue\u201d]\n       alex.color(color)      \nalex.shape(\u201cturtle\u201d)      \n\n\nfor--Using for loop to print tables\n\n\n\n\n\n\n\nThe while statement--Using for loop to print tables\n\n while condition:\n          statement\n\n\nThe while statement\u2014Another example\n\n while condition:\n          statement", "output": "In programming, `if-else` statements are used to control the program's flow based on certain conditions or user inputs. They allow you to execute different blocks of code depending on whether a condition is true or false.\n\nThink of it like a decision-making process in real life. Imagine you're planning a trip and it's raining outside. You might decide to take an umbrella if it's not raining, but if it is raining, you'll take a raincoat instead. That's similar to how `if-else` statements work: they help your program make decisions based on conditions.\n\nFor example, consider a simple banking application that needs to determine whether to display a \"withdrawal\" or \"deposit\" option to the user. The program might use an `if-else` statement like this:\n\n```python\nbalance = 100\n\naction = input(\"Do you want to withdraw (w) or deposit (d)? \")\n\nif action == 'w':\n    print(\"Please enter withdrawal amount: \")\n    # code for withdrawing money...\nelif action == 'd':\n    print(\"Please enter deposit amount: \")\n    # code for depositing money...\nelse:\n    print(\"Invalid choice. Please try again.\")\n```\n\nIn this example, the program uses an `if-else` statement to control its flow based on the user's input (`action`). If the user chooses 'w', it executes one block of code; if they choose 'd', it executes another. This helps keep the program organized and easy to understand.\n\nThe same concept applies in other scenarios, like validating user input or making decisions based on certain conditions. `If-else` statements are a fundamental building block in programming that help you write efficient, readable, and maintainable code."}
{"instruction": "How does the code implementation in the demo which is mentioned in week 3 module help reinforce the key concepts introduced in the lesson?", "input": "INFO 5731\u2014 Computational Methods for Information Systems Spring 2025 \n  \n   \nINFO 5731 2 Spring 2025 \ninstructor or the TA, please send her/him an email via the course website in Canvas Course Messages \nto make an appointment. \nCourse Pre-requisites, Co-requisites, and/or Other Restrictions  \n\u2022 Pre-requisite: Basic programming knowledge and experience (Python), or consent of instructor \nCourse Format \nINFO 5731, Sections 020 hold face-to-face lectures by the instructor. The course uses Canvas, UNT's \nnew learning management system. All course materials will be available at the course site on Canvas \nthat is accessible to all students. And students will submit all assignments through the tools available \non Canvas. \nCourse Description \nIntroduces computational methods that facilitate information analysis, management, and \npresentation in information systems. Students learn effective computer programming skills and \nanalytical tools to process real -world data. Problem-oriented and project-based, allows students to \nexplore interesting research ideas or implement useful information management applications. \nCourse Goals, Learning Objectives \n\u2022 Master key concepts and components of NLP and linguistics.  \n\u2022 Manipulate large corpora, explore linguistic models, and test empirical claims. \n\u2022 Design and implement applications that process, manage, and analyze text data. \n\u2022 Clean and preprocess raw text data using basic natural language processing techniques. \n\u2022 Demonstrate the ability to extract and analyze information from text data using Python Program. \n\u2022 Build robust systems to perform linguistic tasks with technological applications.  \n\u2022 Document and report on information processing and applications.  \nMaterials \nTextbook information (required): \n1. Downey, Allen B. (2016). Think Python: How to Think Like a Computer Scientist, 2nd Edition. \nO\u2019Reilly, ISBN-13: 978-1-491-93936-9. Free access link: \nhttps://greenteapress.com/thinkpython/thinkpython.html  \n2. Hapke, H., Howard, C., & Lane, H. (2021).\nINFO 5731\u2014 Computational Methods for Information Systems Spring 2025 \n  \n   \nINFO 5731 5 Spring 2025 \n \nAssignments ( 50%). The class will have FIVE assignments. These assignments are designed to help \nstudents understand important concepts and gain hands -on experience in Python programming, data \nprocessing, and problem-solving. Assignments must be prepared and submitted using Overleaf in LaTeX \nformat. Diagrams should be created using appropriate graphics software (e.g., PowerPoint, Excel, or \nsimilar tools). Code should be written on Google Colab, and students should submit a link to the Colab \nnotebook with their assignment. \n \nTerm Project ( 40%). The term project is structured according to a project -based learning framework. \nThroughout the semester, students will submit four reports and deliver a final presentation. The first three \nreports each account for 5% of the total grade ( 15% in total). The final project submission consists of a \nfinal version of the report (5%) and a presentation (10%), making the term project worth 40% of the total \ncourse grade. \n\u2022 First Report (5%): The first report should introduce the project by providing: Background and \nsignificance of the chosen topic, a preliminary literature review, and an initial research design, \nspecifying the types of data to be collected and the methodology to be used. \n\u2022 Second Report (5%): The second report should detail the progress of data collection, including: \nData sources and collection methods and challenges encountered and how they were \naddressed. \n\u2022 Third Report (5%): The third report should describe the selected models and evaluation metrics, \nincluding: the rationale for selecting these models and metrics, an explanation of the \nimplementation process, and any preliminary results obtained during the project. \n\u2022 Final Submission (15%): The final report (5%) should be a comprehensive document \nsummarizing the entire project, covering all aspects from the background and methodology to \nresults, analysis, and conclusions.", "output": "The code implementation in the demo provides a hands-on example of how to apply the theoretical concepts learned in the lesson. By seeing the code in action, students can better understand how the different components work together and how they relate to each other.\n\nFor instance, if we're discussing machine learning algorithms, the demo might show how to train a model using a specific dataset, which helps illustrate the key steps involved in the process. This makes it easier for students to grasp the concepts and remember them better.\n\nLet's consider an example. Suppose we're explaining the concept of object-oriented programming (OOP) principles, such as encapsulation and inheritance. The demo might provide an example code that demonstrates how these principles are used to design a simple class hierarchy. By walking through the code together, students can see how the OOP concepts are applied in practice, making it easier for them to understand and retain the information.\n\nIn this way, the code implementation in the demo serves as a concrete illustration of the abstract concepts introduced in the lesson, helping students develop a deeper understanding of the material."}
{"instruction": "What is the role of the Beautiful Soup library in the web scraping process demonstrated in week 4 module? ", "input": "Why Web Scraping?\n2025/2/24\n10\nWe need to extract data from the web to conduct data mining and build a machine learning model\n\n\nWeb Scraping Applications\n2025/2/24\n11\nWeb Scraping Applications in Retail and Manufacturing\nWeb Scraping Applications in Equity and Financial Research\nWeb Scraping Applications in Data Science\nWeb Scraping Applications in Risk Management\nWeb Scraping Applications in Product, Marketing and Sales\nOthers: News and Reputation Monitoring, Academic, Employment, etc.\n\nManually\n\nHow to collect text data?\n2025/2/24\n12\nBusiness tools such as import.io (https://www.import.io/) \nWeb Scraping\nOthers?\nNLTK\n\n2\nPython Web Scraping\n\n2025/2/24\n13\n\nAPI (Application programming interface) is the best way to extract data from a website.\n\nSeveral ways to collect text data by Python-API\n2025/2/24\n14\n\nNLTK includes a diverse set of corpora which can be read using the nltk.corpus package.\n\nSeveral ways to collect text data by Python-NLTK\n2025/2/24\n15\n\nBeautiful Soup\n\nSeveral ways to collect text data by Python-Web Scraping\n2025/2/24\n16\nScrapy\nSelenium\nPyspider \n\n\nComparison of the four tools\n2025/2/24\n17\n\n3\nTools for Web Scraping\n\n2025/2/24\n18\n\n\nThe Structure of a Web Page\nhttps://www.w3schools.com/html/default.asp \n\nDownload the web page\n\nSteps for Web Scraping\nParse the page\n\n\n Locate and download the text information we need\nSave the text collected\n\nimport urllib.request\n\nLibraries\npip install beautifulsoup4 & from bs4 import BeautifulSoup\n\n\npip install scrapy import scrapy\npip install pyspider from pyspider.libs.base_handler import *\npip install selenium from selenium import webdriver\n\n\nurllib.request+BeautifulSoup collects Wikipedia page\nThe source code of the Wikipedia page\n\nurllib.request in Python3 documentation: https://docs.python.org/3/library/urllib.request.html \nBeautiful Soup Documentation: https://www.crummy.com/software/BeautifulSoup/bs4/doc/ \n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\nLesson 4: Python Web Scraping\nHaihua Chen, Ph.D.\n2025/2/24\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\nIntro to Web Scraping\n\nTools for Web Scraping\nWays to collect text data\n2025/2/24\n2\n\n1\nIntro to Web Scraping\n\n2025/2/24\n3\n\nWeb scraping is a computer software technique of extracting information from websites.\n\nWeb Scraping\n\n\n2025/2/24\n4\n\nTransform unstructured data (HTML format) on the web into structured data (database or spreadsheet).\n\nWeb Scraping\n\n\n2025/2/24\n5\n\n\nWhy Web Scraping?\n2025/2/24\n6\nQ: How does a search engine know that all these pages contain the query terms? \nA: Because all of those pages have been crawled\n\n\nWhy Web Scraping?\n2025/2/24\n7\nWhat should we search?\n\nEvery document on the web answers at least one question.\n\nEvery time a search engine adds another document, the number of questions it can answer increases\n\nAdding many poor-quality documents increases the burden on the ranking process to find only the best documents to show to the user\n\nWeb crawling discuss strategies for storing documents and keeping those documents up-to-date!!!\n\n\nWhy Web Scraping?\n2025/2/24\n8\n\n\nWhy Web Scraping? Search Engine Architecture\n2025/2/24\n9\nArchitecture of a search engine determined by 2 requirements\neffectiveness (quality of results) and efficiency (response time and throughput)\nImportant components of search engine are:\nWeb Crawler: This component is used to find information on hundreds of millions of webpages that exists\nIndexer: This component is used to categorize and filter the pages based on content.\nQuery processer: This component is used to translate high-level queries into low-level expressions that can be used at the physical level of the file system, query optimization and actual execution of the query to get the result. \nAlgorithms: IR models to match the query and the documents\nSearch Interface : The interface with which the user can access the search engine.\nSome Popular Search Engines\nGoogle\nBing \nYahoo\nBaidu\n\n\nWhy Web Scraping?\n2025/2/24\n10\nWe need to extract data from the web to conduct data mining and build a machine learning model", "output": "Beautiful Soup is an HTML and XML parser that can extract data from websites by navigating through their structure. It's particularly useful for handling unstructured data found on the web, making it a crucial tool for transforming this data into structured formats, such as databases or spreadsheets.\n\nHere's a simple example to illustrate how Beautiful Soup works:\n\nImagine you're trying to find all the links on a webpage. You can use Beautiful Soup to parse the HTML of that page and extract just those links. This simplifies your task significantly compared to manually searching through the entire HTML code for relevant data.\n\nFor instance, if we have an HTML string like this:\n```html\n<a href=\"https://www.google.com\">Google</a>\n```\nWe can use Beautiful Soup's `find_all()` function to extract all links on a webpage. \n\nHere is how it would look in Python:\n\n```python\nfrom bs4 import BeautifulSoup\n\nhtml_string = '<a href=\"https://www.google.com\">Google</a>'\nsoup = BeautifulSoup(html_string, 'html.parser')\nlinks = soup.find_all('a')\n\nfor link in links:\n    print(link.get('href'))\n```\nThis code would output `https://www.google.com` when you run it. \n\nIn the context of web scraping, Beautiful Soup helps in navigating through website structures to find and collect specific data that's needed for further analysis or processing."}
{"instruction": "What techniques were used in the week 4 module to clean or structure the scraped data for analysis or storage? ", "input": "Construction of large-scale KG\nData Preparation 7\n\u25b6 Storing the data in a uniform manner.\n\u25b6 Parsing: locate, identify, and separate data items.\n\u25b6 Data transformation and standardization.\n\u25b6 Identify which \ufb01elds to be compared.\n\u25b6 Schema matching.\n7Y. Gao, et al. \u201dBuilding a large-scale, accurate and fresh knowledge graph.\u201d KDD-2018, Tutorial 39 (2018). 31 / 85\nConstruction of large-scale KG\nData Preparation 7\n\u25b6 Storing the data in a uniform manner.\n\u25b6 Parsing: locate, identify, and separate data items.\n\u25b6 Data transformation and standardization.\n\u25b6 Identify which \ufb01elds to be compared.\n\u25b6 Schema matching.\n7Y. Gao, et al. \u201dBuilding a large-scale, accurate and fresh knowledge graph.\u201d KDD-2018, Tutorial 39 (2018). 33 / 85", "output": "Parsing is an essential technique used to locate, identify, and separate data items. It involves breaking down the data into smaller parts to extract meaningful information.\n\nConsider a simple example of parsing text data. Imagine you have a string that contains the name, address, and phone number of a person:\n\n\"John Doe, 123 Main St, Anytown USA 12345 (555) 555-5555\"\n\nTo parse this data, you would use techniques such as tokenization to split the string into individual elements, or regular expressions to extract specific information like the name or phone number.\n\nBy applying parsing techniques, you can effectively structure and clean the scraped data for analysis or storage."}
{"instruction": "What is the purpose of the <HR> tag in the HTML file from week 4 module? ", "input": "sha1_base64=\"3+l6Abc63xGDhSVFpwKTAlCK8fU=\">AAACRHicbZDLThsxFIY9lFKacknbZTdWo0pBhWgGVWo3SKjddNFFkAggxWHk8ZxJDLZnap8pikbzcGx4AHY8QTddtEJsUZ3LgtuRLH/6/3N07D8plHQYhlfBwrPF50svll82Xq2srq03X785cHlpBfRErnJ7lHAHShrooUQFR4UFrhMFh8npt4l/+Ausk7nZx3EBA82HRmZScPRS3Oz/aDMcAfINukNZYfM0rnAnqo+r/brqMgUZts9ipEzLlLLqzLsfT2pWb9ItTb39k57MLr05IeMpZFYOR7hRx81W2AmnRR9DNIcWmVc3bl6yNBelBoNCcef6UVjgoOIWpVBQN1jpoODilA+h79FwDW5QTUOo6QevpDTLrT8G6VS9O1Fx7dxYJ75Tcxy5h95EfMrrl5h9GVTSFCWCEbNFWako5nSSKE2lBYFq7IELK/1bqRhxywX63Bs+hOjhlx/DwXYnCjvR3qfW7td5HMvkHXlP2iQin8ku+U66pEcEOSe/yV/yL7gI/gTXwc2sdSGYz7wl9yq4/Q8lTK+5</latexit><latexit\nsha1_base64=\"3+l6Abc63xGDhSVFpwKTAlCK8fU=\">AAACRHicbZDLThsxFIY9lFKacknbZTdWo0pBhWgGVWo3SKjddNFFkAggxWHk8ZxJDLZnap8pikbzcGx4AHY8QTddtEJsUZ3LgtuRLH/6/3N07D8plHQYhlfBwrPF50svll82Xq2srq03X785cHlpBfRErnJ7lHAHShrooUQFR4UFrhMFh8npt4l/+Ausk7nZx3EBA82HRmZScPRS3Oz/aDMcAfINukNZYfM0rnAnqo+r/brqMgUZts9ipEzLlLLqzLsfT2pWb9ItTb39k57MLr05IeMpZFYOR7hRx81W2AmnRR9DNIcWmVc3bl6yNBelBoNCcef6UVjgoOIWpVBQN1jpoODilA+h79FwDW5QTUOo6QevpDTLrT8G6VS9O1Fx7dxYJ75Tcxy5h95EfMrrl5h9GVTSFCWCEbNFWako5nSSKE2lBYFq7IELK/1bqRhxywX63Bs+hOjhlx/DwXYnCjvR3qfW7td5HMvkHXlP2iQin8ku+U66pEcEOSe/yV/yL7gI/gTXwc2sdSGYz7wl9yq4/Q8lTK+5</latexit>\n\u00afv t =\n1\n2 m\nX\n m \uf8ff j \uf8ff m,j 6=0\nv t + j\n<latexit", "output": "The <HR> tag in an HTML file is used to create a horizontal rule. It's essentially a line that can be used to separate sections of content within a webpage. This can help with organization and presentation, making it easier for users to navigate through the content.\n\nHere's a simple example:\n\nLet's say you're creating a webpage about your favorite foods. You might use an <HR> tag to separate the list of appetizers from the main course. The line would visually distinguish between the two sections, helping readers see where one ends and the other begins.\n\nIn practice, this could look something like this in your HTML:\n\n```html\n<h2>Appetizers</h2>\n<ul>\n    <li>Sushi</li>\n    <li>Tacos</li>\n</ul>\n\n<hr>\n\n<h2>Main Course</h2>\n<ul>\n    <li>Steak</li>\n    <li>Pasta</li>\n</ul>\n```\n\nIn this example, the `<hr>` tag is used to create a horizontal line that visually separates the \"Appetizers\" section from the \"Main Course\" section. This can be especially helpful for users who are scanning content quickly or have difficulty with text-heavy pages."}
{"instruction": "What are the key dimensions used to evaluate data quality in machine learning according to week 5 module? ", "input": "ata that are fit for use by data consumers. (Wang and Strong, 1996)\nThe capability of the data for \u201cfit for purpose\u201d of building a machine learning system. (Chen et al., 2021)\n\nData quality problems: a set of issues that can affect the potentiality of the applications that use the data\nLacking enough training data\nInvalid data and label noises\nSignificant imbalanced training data\nDuplication\n. . .\nDefinitions (Cont.)\n9/15/24\n8\nDefinition of Concepts\n\nHigh-quality data (\u201cgood data\u201d)\n\u201cFit for purpose\u201d of applications.\nFour basic standards for high-quality data in machine learning defined by Andrew: defined consistently, cover of important cases, has timely feedback from production data, and sized appropriately.  (Andrew, 2021)\n\nData quality dimensions: a set of data quality attributes that represent a single aspect or construct of data quality (Wang and Strong, 1996)\nComprehensiveness\nCorrectness\nVariety\nCompleteness\nTimeliness\n. . .\nDefinitions (Cont.)\n9/15/24\n9\nDefinition of Concepts\n\nData quality evaluation (\u201cdata quality assessment\u201d)\nHow to evaluate each data quality dimension. (Cai et al., 2015)\nQualitatively method: performed by subject experts or professionals.  \nQuantitatively method: a formal, objective, and systematic process in which numerical data are utilized to obtain information.\nData quality evaluation results offer guidelines of data quality improvement.\n\nData quality improvement\nDetect and remove errors, inconsistencies, and other data quality issues.\nData quality can be improved systematically and iteratively. (Andrew, 2021)\nResearch Questions\n9/15/24\n10\n\nRQ1: How to construct a high-quality, domain-specific, and large-scale machine learning corpus using semi-automatic approaches?\n\nRQ2: How to systematically evaluate the data quality for machine learning?\n\nRQ3: What are the techniques that can be used to improve the data quality?\n\nRQ4: What are the best practice of implementing machine learning techniques for data quality improvement?\nOutline of The Study\n9/15/24\n11\nFigure 1: Outline of this study.\n(Lauria and Tayi, 2008)\nTrustworthiness or reliability is another critical data quality dimension of machine learning. (Dong, 2015)\nHow do different data quality features impact machine learning systems. (Foidl and Felderer, 2019)\nA system for automating the verification of the data quality of large-scare data, aiming to fit the purpose for business and decision making. (Schelter et al., 2018)\nInsufficient training data has become a common data quality issue for machine learning. (Lourentzou, 2019)\nOther data quality issues: duplication, highly correlated variables, large number of variables, outliers, data source bias, etc. (Gudivada et al., 2017)\nLiterature Review - Data Quality for ML\n9/15/24\n16\nStatus of Data Quality Research for ML\n\nImproving data quality as \u201coperational\u201d vs building novel models and algorithms. (Sambasivan et al., 2021)\nImproving the data might be more effective than improving the model. (Andrew, 2021)\nTutorials related to data quality in ML and DL. (Jain et al., 2020; Whang and Lee, 2020)\n\nResearch Gap\n\nData quality of ML has not been appropriately validated.\nDiscussions on the impact of data quality on the ML performance with evidence and measurement is rare.\nImproving the ML performance by improving data quality instead of model construction deserves more attention. \nLiterature Review - Data Quality Improvement\n9/15/24\n17\n\nSemi-supervised Learning (Van Engelen and Hoos, 2020)\nCo-training (Aridas and Kotsiantis, 2015)\nActive Learning (Lourentzou, 2019)\nExpectation-maximization (Liu et al., 2003; Brefeld and Scheffer, 2004; Chokka and Rani, 2020)\nGenerative Adversarial Learning (Goodfellow et al., 2014; Croce et al., 2020)\nTransfer Learning (Zhuang et al., 2020)\nFew-shot Learning (Wang et al., 2020)\nLiterature Review - Domain-specific Text Classification\n9/15/24\n18\n\nLegal Argument Mining\nArgument/non-argument detection for different text types, multinomial NB (Moens et al., 2007).", "output": "According to the guidelines for high-quality data in machine learning, there are four basic standards that Andrew defined. These standards can be related to the concept of evaluating data quality dimensions.\n\nThe main dimensions used to evaluate data quality are:\n\n1. Comprehensiveness\n2. Correctness\n3. Variety\n4. Completeness\n5. Timeliness\n\nThese dimensions help assess various aspects of data quality, providing a structured approach to evaluating and improving data in machine learning applications."}
{"instruction": "Which methods were used to improve data quality in domain-specific machine learning tasks according to week 5 module? ", "input": "Conclusion (Cont.)\n9/15/24\n52\nRQ2: How to systematically evaluate the data quality for machine learning?\n\n\nAnswer: \n\nSystematically and quantitatively evaluate the quality of the dataset based on pre-defined data quality dimensions can help developers to take further actions for the data quality and system improvement.\n\n\n\nConclusion (Cont.)\n9/15/24\n53\nRQ3: What are the techniques that can be used to improve the data quality?\n\n\nAnswer:\n\nWe can choose the GAN-BERT model for data augmentation when a small portion of labeled data while a large portion of unlabeled data is available.\n\nThe fine-tuning can be used to improve data quality by taking advantage of the information brought by the source dataset.\n\nConclusion (Cont.)\n9/15/24\n54\nRQ4: What are the best practice of implementing machine learning techniques for data quality improvement?\n\n\nAnswer:\n\nThe amount of unlabeled data for data augmentation should be carefully selected for training, otherwise the new labeled data may produce negative performance impact.\nThe datasets for fine-tuning should be related to the target-specific task.\nA powerful model, such as BERT, can better capture semantic information, thereby can be the prior model for fine-tuning.\n\n\n\nConclusion \u2013 Overall Conclusions\n9/15/24\n55\nData quality has a significant impact on the machine learning performance, the impact can be quantitively measured by experimental study.  \n\nThe data quality evaluation method should be designed based on different applications and different purposes. \n\nData quality evaluation results can provide guidance for designing experiments for data quality improvement.\n\nBoth transfer learning and the data augmentation techniques are not always effective for data quality and system performance improvement. Only carefully designing the experiments and fine-tuning the parameters could produce the desired result. \n\n\nFuture Work\n9/15/24\n56\n\nExplore more effective and reusable techniques for constructing domain-specific, high-quality, and large-scale machine learning datasets.\nCreate and publish several high-quality datasets regarding legal AI, health care, cyber security, recommendation, and others. \nExploring automatic data quality evaluation and improvement approaches regarding different data quality dimensions and applications.\nExplore more statistic methods for data quality evaluation.\nDevelop effective and user-friendly tools for data quality evaluation and improvement for different machine learning applications\nData quality evaluation for graph data such as knowledge graph.\nEnhance BERT by combining BERT and external domain knowledge in the attention for domain-specific text classification.\nExplore other techniques such as few shot learning for data augmentation. \nOthers\n(Lauria and Tayi, 2008)\nTrustworthiness or reliability is another critical data quality dimension of machine learning. (Dong, 2015)\nHow do different data quality features impact machine learning systems. (Foidl and Felderer, 2019)\nA system for automating the verification of the data quality of large-scare data, aiming to fit the purpose for business and decision making. (Schelter et al., 2018)\nInsufficient training data has become a common data quality issue for machine learning. (Lourentzou, 2019)\nOther data quality issues: duplication, highly correlated variables, large number of variables, outliers, data source bias, etc. (Gudivada et al., 2017)\nLiterature Review - Data Quality for ML\n9/15/24\n16\nStatus of Data Quality Research for ML\n\nImproving data quality as \u201coperational\u201d vs building novel models and algorithms. (Sambasivan et al., 2021)\nImproving the data might be more effective than improving the model. (Andrew, 2021)\nTutorials related to data quality in ML and DL. (Jain et al., 2020; Whang and Lee, 2020)\n\nResearch Gap\n\nData quality of ML has not been appropriately validated.\nDiscussions on the impact of data quality on the ML performance with evidence and measurement is rare.\nImproving the ML performance by improving data quality instead of model construction deserves more attention. \nLiterature Review - Data Quality Improvement\n9/15/24\n17\n\nSemi-supervised Learning (Van Engelen and Hoos, 2020)\nCo-training (Aridas and Kotsiantis, 2015)\nActive Learning (Lourentzou, 2019)\nExpectation-maximization (Liu et al., 2003; Brefeld and Scheffer, 2004; Chokka and Rani, 2020)\nGenerative Adversarial Learning (Goodfellow et al., 2014; Croce et al., 2020)\nTransfer Learning (Zhuang et al., 2020)\nFew-shot Learning (Wang et al., 2020)\nLiterature Review - Domain-specific Text Classification\n9/15/24\n18\n\nLegal Argument Mining\nArgument/non-argument detection for different text types, multinomial NB (Moens et al., 2007).", "output": "To measure the quantitative and qualitative properties of data, one can use various techniques such as data validation, data cleaning, and data normalization. These methods help ensure that the data is fit for use by data consumers.\n\nFor instance, imagine you have a dataset containing student grades. To improve its quality, you could apply data normalization to scale the grade values between 0 and 1, or perform data cleaning to remove any duplicate or invalid entries. By doing so, you can make sure that the data is accurate and reliable for further analysis or use in machine learning models."}
{"instruction": "What is the purpose of using the Evidently library in week 5 module? ", "input": "Python Package\n\nPackages are namespaces which contain multiple packages and modules themselves.\n2025/2/21\n29\nThey are simply directories, but with a twist.\nEach package in Python is a directory which MUST contain a special file called init.py. This file can be empty, and it indicates that the directory it contains is a Python package, so it can be imported the same way a module can be imported.\nGrouping related code into a module makes the code easier to understand and use.\nhttps://docs.python.org/3/tutorial/modules.html#package \n\n\nPython Package Structure\n\n2025/2/21\n30\n\n\nWhat is difference between a Python module and package?\n\nA module is a single file (or files) that are imported under one import and used. \n                                                                                  E.g., import my_module\n2025/2/21\n31\nA package is a collection of modules in directories that give a package hierarchy. \nE.g., from my_package.timing.danger.internets import function_of_love\n\n\nWhere to find Python packages?\n\n2025/2/21\n32\nhttps://pypi.org/ \n\n\nHow to install and use a python package?\n\n2025/2/21\n33\nE.g., install textblob, a useful nlp package\nhttps://pypi.org/ \n\n5\nPython JSON \n\n2025/2/21\n34\n\n\nPython JSON\n\n2025/2/21\n35\nhttps://www.w3schools.com/js/js_json_intro.asp \nJavaScript Object Notation (JSON) is an open-standard file format or data interchange format that uses human-readable text to transmit data objects consisting of attribute\u2013value pairs.\n\n\nPython XML\n\nExtensible Markup Language (XML) is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.\n2025/2/21\n36\nhttps://www.w3schools.com/xml/xml_whatis.asp\nLesson 3: Python Basic (2)\nHaihua Chen, Ph.D.\n2025/2/21\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\nPython Dates\n\nPython Arrays\nModule and Package\nFunctions\n2025/2/21\n2\n\n\n5\nPython JSON \n\n\n6\nPython Try Except\n\n\n7\nClasses and Objects\n\n\n8\nDemo\n\n\n9\nIn-class Exercise 1\n\n1\nPython Dates\n\n2025/2/21\n3\n\nwe can import a module named datetime to work with dates as date objects.\n\nPython Dates\n\n\n2025/2/21\n4\nReturn the current local date and time.\n\n\n\nDate Output\n2025/2/21\n5\nThe datetime module has many methods to return information about the date object.\n\n\n\nMore about Date\n2025/2/21\n6\nhttps://docs.python.org/3/library/datetime.html#module-datetime\n\n2\nPython Arrays\n\n2025/2/21\n7\n\na = [1, 3.5, \"Hello\"]\n\nArrays\n2025/2/21\n8\nb = [1.1, 3.5, 4.5]\nWe can treat lists as arrays.\nwe cannot constrain the type of elements stored in a list.\nIf you create arrays using the array module, all elements of the array must be of the same numeric type.\n\narray of numeric values are supported in Python by the array module.\n\nArrays\n2025/2/21\n9\n\narray of numeric values are supported in Python by the Numpy library.\n\nArrays\n2025/2/21\n10\n\nIf you're going to perform arithmetic functions to your lists, you should really be using arrays instead\n\nDifferences between List and Arrays\n2025/2/21\n11\nArrays will store your data more compactly and efficiently, so if you're storing a large amount of data, you may consider using arrays as well\n\n3\nFunctions\n\n2025/2/21\n12\n\nA function is a named sequence of statements that belong together \n\nDefinition of a function\nHelp us organize programs into chunks that match how we think about the problem\n\n\n def <name>(<parameters>):\n       <statements>\n\n\nAn example\nA header line which begins with a keyword and ends with a colon.\nA body consisting of one or more Python statements, each indented the same amount (4 spaces recommended ) from the header line.", "output": "The Evidently library is used for model monitoring and explainability. It provides a simple way to track changes in your models over time and compare their performance on different datasets. This helps you detect drifts, bias, and other issues with your models.\n\nHere's a simple example to make this concept clear:\n\nImagine you have a machine learning model that predicts house prices based on features like number of bedrooms and square footage. You trained the model on historical data from 2020 and it performs well on that dataset. However, over time, market conditions change, and new features become relevant. If your model doesn't adapt to these changes, its predictions may become less accurate.\n\nEvidently helps you monitor your model's performance over time by tracking metrics like accuracy, precision, and recall. It also provides visualizations to help you understand how your model is behaving on different datasets. This way, you can detect when your model starts to drift away from the expected behavior and take corrective action to update it.\n\nFor example:\n\n```python\nfrom evidently import ColumnTransformer\n\n# assume we have a DataFrame df with historical data\ndf = pd.read_csv('historical_data.csv')\n\n# create a ColumnTransformer object\ncolumn_transformer = ColumnTransformer()\n\n# fit the transformer on the training data\ncolumn_transformer.fit(df)\n\n# transform the test data\ntransformed_test_data = column_transformer.transform(df)\n```\n\nIn this example, we're using Evidently's ColumnTransformer to transform our historical data. We then use this transformed data to train a machine learning model and monitor its performance over time using Evidently's metrics and visualizations."}
{"instruction": "What is the main purpose of data preprocessing in the week 5 module? ", "input": "Construction of large-scale KG\nData Preparation 7\n\u25b6 Storing the data in a uniform manner.\n\u25b6 Parsing: locate, identify, and separate data items.\n\u25b6 Data transformation and standardization.\n\u25b6 Identify which \ufb01elds to be compared.\n\u25b6 Schema matching.\n7Y. Gao, et al. \u201dBuilding a large-scale, accurate and fresh knowledge graph.\u201d KDD-2018, Tutorial 39 (2018). 31 / 85\nConstruction of large-scale KG\nData Preparation 7\n\u25b6 Storing the data in a uniform manner.\n\u25b6 Parsing: locate, identify, and separate data items.\n\u25b6 Data transformation and standardization.\n\u25b6 Identify which \ufb01elds to be compared.\n\u25b6 Schema matching.\n7Y. Gao, et al. \u201dBuilding a large-scale, accurate and fresh knowledge graph.\u201d KDD-2018, Tutorial 39 (2018). 33 / 85", "output": "Data preprocessing, which involves transforming and structuring raw data into a suitable format for analysis or modeling. The main purpose of data preprocessing is to ensure that the data is accurate, complete, consistent, reliable, valid, and easy to analyze.\n\nImagine you have a messy room with clothes, books, and toys scattered all over the floor. Before you can study in that room, you would need to clean it up, put away your belongings, and organize the space. Similarly, data preprocessing is like cleaning and organizing your data so that it's ready for further analysis or modeling.\n\nA simple example of data preprocessing would be taking a list of names with different formatting (e.g., \"John Smith\", \"john smith\", \"JOHN SMITH\") and standardizing them to a uniform format (e.g., \"john_smith\"). This makes the data easier to work with and reduces errors in analysis or modeling."}
{"instruction": "What is the difference between a simple subject and a complete subject in sentence structure based on week 6 module? ", "input": "(Assignments and the Project first submission will due on Sunday midnight of the specified week. \nQuizzes will be available online from 6:00 pm on Monday to 6:00 pm on Friday of the specified week. \nTerm project final report will due on December 6 midnight). The time of the invited talk might be \nchanged based on the speakers\u2019 schedule.  \nTable 2. Study Schedule and Due Dates \nWeek Dates Meeting \nDate \nStudy Focus Individual tasks Group tasks \n1 Jan 13 - Jan 19 Jan 15 Syllabus, Lesson 1 \nDr. Chen  \nDiscussion Student Grouping \n2 Jan 20 - Jan 26 Jan 22 Lesson 2 \nDr. Chen \nAssignment 1 \nDiscussion \n \n3 Jan 27 - Feb 02 Jan 29 Lesson 3 \nHuyen \nDiscussion  \n4 Feb 03 - Feb 09 Feb 05 Lesson 4 \nDr. Chen \nDiscussion  \n5 Feb 10 - Feb 16 Feb 12 Lesson 5 \nHuyen \nAssignment 2 \nDiscussion \n \n6 Feb 17 - Feb 23 Feb 19 Lesson 6-part 1 \nDr. Chen \nDiscussion\nTopicGPT: A Prompt-based Topic Modeling Framework\n1) Topic Generation:  \nGiven a corpus and some manually-curated example topics, TopicGPT \nidenti\ufb01es additional topics in each corpus document. \n2) Topic Assignment:  \nGiven the generated topics, TopicGPT assigns the most relevant topic to each \ndocument and provides a quote that supports this assignment.\n45\nPham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. \"TopicGPT: A prompt-based topic modeling framework.\" arXiv preprint arXiv:2311.01449 (2023).", "output": "A simple subject is a single word or phrase that performs the action of the verb in a sentence. On the other hand, a complete subject includes both the simple subject and any modifiers that provide additional information about it.\n\nTo illustrate this concept, consider the following example:\n\n\"Dr. Chen is teaching.\"\n\nIn this sentence, \"Dr. Chen\" is the simple subject because it is a single word performing the action of the verb \"teaching\". However, if we add more information to make it clear who Dr. Chen is, such as in the week's syllabus schedule, then \"Dr. Chen\", along with his role and other details, becomes the complete subject.\n\nFor instance: \"Dr. Chen, a renowned lecturer, is teaching.\"\n\nIn this revised sentence, \"Dr. Chen\" is still the simple subject, but now we have added more information about Dr. Chen through the modifier \"a renowned lecturer\"."}
{"instruction": "How does structural ambiguity arise in sentences, and what are some examples from the week 6 module? ", "input": "The slot \u201cTrigger\u201d defines the conceptual anchor, \nwhile the slot \u201cVariable Slots\u201d represents that the information to be extracted is the \nsubject of the sentence. Finally, the subject must be a physical target (see \n\u201cConstraints:\u201d), and the enabling conditions require the verb to be used in its passive \nform. \n \nConcept Node: \nN a m e :             t a rget-subject-passive-verb-bombed \nT r i g g e r :            b o m b e d  \nVariable Slots:      (target (*S *1)) \nConstraints:        (class phys-target *S*) \nConstant Slots:     (type bombing) \nEnabling Conditions: ((passive)) \nFigure 1. Example of AutoSlog concept node \nAutoSlog needs to parse the natural language sentence using a linguistic parser. The \nparser is used to generate syntax elements of a sentence (such as subject, verb, \npreposition phrase). Then the output syntax elements are matched against the \nlinguistic pattern and fire the best matched pattern as the result pattern to construct a \npattern dictionary.\nThe most advantage of the model is that it can achieve a better \ntrade-off between precision and recall. \nSequential Labeling based Extraction Methods \nInformation extraction can be cast as a task of sequential labeling. In sequential \nlabeling, a document is viewed as a sequence of tokens, and a sequence of labels are \nassigned to each token to indicate the property of the token. For example, consider the \nnature language processing task of labeling words of a sentence with their \ncorresponding Part-Of-Speech (POS). In this task, each word is labeled with a tag \nindicating its appropriate POS. Thus the inputting sentence \u201cPierre Vinken will join \nthe board as a nonexecutive director Nov. 29.\u201d will result in an output as: \n[NNP Pierre] [NNP Vinken] [MD will] [VB join] [DT the] [NN board] \n[IN as] [DT a] [JJ nonexecutive] [NN director] [NNP Nov.] [CD 29] [. .] \nFormally, given an observation sequence x = (x1, x2 ,\u2026, xn), the information \nextraction task as sequential labeling is to find a label sequence y* = (y1, y2 ,\u2026, yn) \nthat maximizes the conditional probability p(y|x), i.e., \n y* = argmaxy p(y|x) (5) \nDifferent from the rule learning and the classification based methods, sequential \nlabeling enables describing the dependencies between target information. The \ndependencies can be utilized to improve the accuracy of the extraction. Hidden", "output": "Structural ambiguity arises when a sentence can be interpreted in more than one way based on its grammatical structure. This can lead to multiple possible meanings of the sentence.\n\nLet's consider an example. Take the sentence \"The dog chased the cat.\" At first glance, it seems like a straightforward statement about a chase between two animals. However, there is also another possibility: that the dog and the cat are being referred to as one entity, in which case the sentence would mean \"the dog (which is a type of cat) was chased.\"\n\nTo illustrate this ambiguity, let's consider another example: \"The teacher gave the student a book.\" Here, we can interpret it as either the teacher giving something to the student or the teacher describing the student."}
{"instruction": "What is the main idea behind the distributional hypothesis in NLP according to week 6 module?", "input": "The generative story begins with only a Dirichlet prior over the topics \nEach topic is de\ufb01ned as a Multinomial distribution over the vocabulary, parameterized by \u03d5k\nExample Credit to Matthew R. Gormley \n8\nDistributional hypothesis\nC1 C2 C3 C4\ntejuino 1 1 1 1\nloud 0 0 0 0\nmotor-oil 1 0 0 0\ntortillas 0 1 0 1\nchoices 0 1 0 0\nwine 1 1 1 0\nC1: A bottle of ___ is on the table.\nC2: Everybody likes ___.\nC3: Don\u2019t have ___ before you drive.\nC4: We make ___ out of corn.\n\u201cwords that occur in similar contexts tend to have similar meanings\u201d", "output": "The main idea behind the distributional hypothesis is that words with similar meanings tend to appear in similar contexts. This means that if two words often co-occur with the same set of other words, they are likely to have related meanings.\n\nFor example, imagine a word like \"car\" and another word like \"vehicle\". According to the distributional hypothesis, these words should appear in similar contexts, such as in sentences about transportation or modes of travel. This is because both words refer to objects that can move people from one place to another.\n\nTo illustrate this concept further, consider two sentences:\n\n* \"I drove my car to work.\"\n* \"My friend owns a vehicle.\"\n\nIn these examples, the words \"car\" and \"vehicle\" are used in similar contexts (i.e., talking about transportation), which suggests that they have related meanings."}
{"instruction": "What does PPMI stand for, and why is it used when creating word-word co-occurrence matrices accoding to week 6 module? ", "input": "Representing words by their context\nDistributional hypothesis: words that occur in similar contexts \ntend to have similar meanings\nJ.R.Firth 1957\n\u2022 \u201cYou shall know a word by the company it keeps\u201d\n\u2022 One of the most successful ideas of modern statistical \nNLP!\nThese context words will represent banking.\nWords as vectors\n             Problem: not all counts are equal, words can randomly co-occur\n\u2022 Solution: re-weight by how likely it is for the two \nwords to co-occur by simple chance \n\u2022 PPMI = Positive Pointwise Mutual Information", "output": "The main idea behind the distributional hypothesis is that words with similar meanings tend to appear in similar contexts. This means that if two words often co-occur with the same set of other words, they are likely to have related meanings.\n\nFor example, imagine a word like \"car\" and another word like \"vehicle\". According to the distributional hypothesis, these words should appear in similar contexts, such as in sentences about transportation or modes of travel. This is because both words refer to objects that can move people from one place to another.\n\nTo illustrate this concept further, consider two sentences:\n\n* \"I drove my car to work.\"\n* \"My friend owns a vehicle.\"\n\nIn these examples, the words \"car\" and \"vehicle\" are used in similar contexts (i.e., talking about transportation), which suggests that they have related meanings."}
{"instruction": "What is the definition of syntax according to the week 7 module? ", "input": "Lesson 2: Python Basic (1)\nHaihua Chen, Ph.D.\n2025/3/6\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\nBasic Syntax\nPython Variables\nPython Operators\nPython Data Types\n2025/3/6\n2\n\n\n5\nPython Files\n\n\n6\nLoop\n\n\n7\nConditionals\n\n\n8\nIn-class Quiz 1\n\n1\nBasic Syntax\n\n2025/3/6\n3\n\na  high-level programming language like Java, C#, C++ et al.\n\n1. Execute Python Syntax\nInterpreter: immediate mode  and script mode \n\n\n2025/3/6\n4\n\n\nPython Indentation\n2025/3/6\n5\nIndentation refers to the spaces at the beginning of a code line.\nPython uses indentation to indicate a block of code.\n\n\n\nPython Comments\n2025/3/6\n6\nComments can be used to explain Python code.\nComments can be used to make the code more readable.\nComments can be used to prevent execution when testing code.\n\n2\nPython Variables\n\n2025/3/6\n7\n\nA variable is a name that refers to a value\n\nVariables\n2025/3/6\n8\nA variable can have a short name (like x and y) or a more descriptive name (age, carname, total_volume)\nA variable name must start with a letter or the underscore character\nA variable name cannot start with a number\nA variable name can only contain alpha-numeric characters and underscores (A-z, 0-9, and _ )\nVariable names are case-sensitive\n\nThese words can\u2019t be used as variables\n\nVariable names and keywords\n2025/3/6\n9\n\n3\nPython Data Types\n\n2025/3/6\n10\n\n\nCommonly used python data types\n\n\n\n\n\n2025/3/6\n11\n\n\nPython Numbers\n2025/3/6\n12\nint\nfloat\ncomplex\n\n\nPython Numbers - Type Conversion\n2025/3/6\n13\nYou can convert from one type to another with the int(), float(), and complex() methods\n\n\n\nPython Numbers - Random Number\n2025/3/6\n14\nPython has a built-in module called random that can be used to make random numbers\nThe slot \u201cTrigger\u201d defines the conceptual anchor, \nwhile the slot \u201cVariable Slots\u201d represents that the information to be extracted is the \nsubject of the sentence. Finally, the subject must be a physical target (see \n\u201cConstraints:\u201d), and the enabling conditions require the verb to be used in its passive \nform. \n \nConcept Node: \nN a m e :             t a rget-subject-passive-verb-bombed \nT r i g g e r :            b o m b e d  \nVariable Slots:      (target (*S *1)) \nConstraints:        (class phys-target *S*) \nConstant Slots:     (type bombing) \nEnabling Conditions: ((passive)) \nFigure 1. Example of AutoSlog concept node \nAutoSlog needs to parse the natural language sentence using a linguistic parser. The \nparser is used to generate syntax elements of a sentence (such as subject, verb, \npreposition phrase). Then the output syntax elements are matched against the \nlinguistic pattern and fire the best matched pattern as the result pattern to construct a \npattern dictionary.", "output": "The main idea behind the distributional hypothesis is that words with similar meanings tend to appear in similar contexts. This means that if two words often co-occur with the same set of other words, they are likely to have related meanings.\n\nFor example, imagine a word like \"car\" and another word like \"vehicle\". According to the distributional hypothesis, these words should appear in similar contexts, such as in sentences about transportation or modes of travel. This is because both words refer to objects that can move people from one place to another.\n\nTo illustrate this concept further, consider two sentences:\n\n* \"I drove my car to work.\"\n* \"My friend owns a vehicle.\"\n\nIn these examples, the words \"car\" and \"vehicle\" are used in similar contexts (i.e., talking about transportation), which suggests that they have related meanings.\nYou: What is the definition of syntax according to the week 7 module? \nRunning query: What is the definition of syntax according to the week 7 module? \nIndex loaded successfully.\nAssistant: Syntax is referred to as parsing or syntactic analysis, which involves analyzing strings of symbols in natural language in conformance with grammatical rules. It's a crucial step before moving on to semantic analysis, and it's essential for determining the meaning of text selections.\n\nA simple example can help illustrate this concept: Imagine you're trying to understand what someone means when they say \"I saw the book.\" Without syntactic analysis, you might think they're talking about a physical object called \"the book\" that they saw. But with syntax, you realize that \"saw\" is a verb and \"the book\" is its object, so the sentence actually means that the person observed or perceived something related to the book."}
{"instruction": "What is the difference between deep structure and surface structure in syntax according to week 7 module?", "input": "Keras vs. tf.keras: What\u2019s the difference in TensorFlow 2.0?\nTensorflow Vs. Keras: Comparison by building a model for image classification\nKeras VS. TensorFlow \n8/16/24\n86\nBasic Pytorch Workflow \nPyTorch \nTensors \u2014 torch.Tensor\nOptimizers \u2014 torch.optim module\nNeural Networks \u2014 nn module\nAutograd\nCore task: Developing and training deep learning models\nUnderstand the PyTorch Modules by examples:\nhttps://pytorch.org/tutorials/beginner/pytorch_with_examples.html#autograd \n8/16/24\n87\nEasy to use API \nPython support\nDynamic computation graphs\nTorchScript\nDistributed Training\nTools and Libraries\nPyTorch vs TensorFlow: Difference you need to know\nPyTorch\nSecure Model Building \nML Production Anywhere\nRobust Experimentation for Research\nTensorFlow\nPyTorch VS. TensorFlow\n8/16/24\n88\nSource: https://www.youtube.com/watch?time_continue=1&v=DmI58jz2i6w&feature=emb_logo\nKeras VS. PyTorch VS. TensorFlow \n\nContent:\n\nIntroduction to GitHub \nIntroduction to Google Colab\nIntroduction to Anaconda and PyCharm\nIntroduction to Python Tools for Scientific Computing\nIntroduction to Python Tools for Natural Language Processing\nIntroduction to Python Tools for Machine Learning and Deep Learning\nIntroduction to Computational Resources in the Lab \nMachine learning steps:\nBusiness understanding\nData understanding\nData preparation\nData modeling \nEvaluation\nDeployment\nFactors affect the selection and performance of ML models:\nsize, quality, and nature of the domain data\navailable computational time\nstructure of the desired predictions and loss to be minimized\n8/16/24\n90\nMachine learning\nDeep learning steps:\nDefining network architectures involves setting fine-grained details such as activation functions and the types of layers as well as the overall architecture of the network.\nDefining training routines involves setting the learning rate schedules, the learning rules [e.g. stochastic gradient descent (SGD), Adam], the loss functions (e.g. MSE), regularization techniques (e.g. early stopping) and hyper-parameter optimization (e.g. random search, bayesian guided search).\nINFO 5731\u2014 Computational Methods for Information Systems Spring 2025 \n  \n   \nINFO 5731 3 Spring 2025 \nCode example of the book: https://github.com/nlp-with-transformers/notebooks     \nSupplementary materials and/or readings (recommended): \n4. Python Documentation: https://www.python.org/doc/.  \n5. Python Forums: https://python-forum.io/.   \n6. Stackoverflow: https://stackoverflow.com/.  \n7. NLTK Documentation: https://www.nltk.org/.  \n8. Google Colab: http://colab.research.google.com/. \n9. Success Story of Sylvain Gugger: https://www.fast.ai/2019/01/02/one-year-of-deep-learning/.  \n10. Github link of the first textbook: https://github.com/AllenDowney/ThinkPython  \n11. Github link of the second textbook: https://github.com/totalgood/nlpia  \n12. Github link of the third textbook: https://github.com/nlp-with-transformers/notebooks  \n13. Jacob Eisenstein. (2019). Introduction to Natural Language Processing (Adaptive Computation and \nMachine Learning series). The MIT Press, ISBN-13: 978-0262042840.  \nTeaching Philosophy \nThe instructor will take a problem-solving approach and work together with students to understand \nNatural Language Processing. We will learn how to solve practical data collecting, text processing, \ninformation extraction, and text mining problems. He will monitor the progress of students and is \nopen to suggestions from students. Students are expected to study 12 -15 hours per week, and to \nsubmit their assignments on time to achieve satisfactory class performance. Interaction between the \nstudent and the instr uctor/TA is guaranteed and strongly encouraged. Students who don\u2019t have \nknowledge and experience in python are expected to spend extra hours on this course. \nTECHNICAL REQUIREMENTS/ASSISTANCE  \n \nUIT Help Desk: http://www.unt.edu/helpdesk/index.htm \nThe University of North Texas provides student technical support in the use of Canvas and supported \nresources.", "output": "The distinction between deep structure and surface structure lies at the intersection of structural linguistics and generative grammar. The deep structure represents a more fundamental representation of meaning within a sentence, often influenced by underlying grammatical rules and semantic relationships. On the other hand, the surface structure is concerned with the actual sequence and arrangement of words in a sentence, reflecting how language is typically used in communication.\n\nTo illustrate this concept, consider the example \"The cat chases the mouse.\" In deep structure, we might represent this as an action of pursuit or chase between two entities. However, when considering the surface structure, we note that it follows a typical subject-verb-object word order for English sentences.\n\nThis distinction is crucial in understanding how meaning and expression interact within linguistic systems. By recognizing that there's often a gap between how we conceptualize meaning (deep structure) and how it's expressed through language (surface structure), one can better grasp the intricacies of syntax, semantics, and pragmatics in communication."}
{"instruction": "What is a synset and how is it used to represent word meanings in WordNet, according to week 7 module? ", "input": "Representing words by their context\nDistributional hypothesis: words that occur in similar contexts \ntend to have similar meanings\nJ.R.Firth 1957\n\u2022 \u201cYou shall know a word by the company it keeps\u201d\n\u2022 One of the most successful ideas of modern statistical \nNLP!\nThese context words will represent banking.\nguru99.com/nltk-tutorial.html\nPlay with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk\nNLTK\n8/16/24\n70\nWordNet (a lexical database) with NLTK: Finding Synonyms for words in Python\nSource: https://www.guru99.com/nltk-tutorial.html\nPlay with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk\nNLTK\n8/16/24\n71\nWord Embeddings\nA word embedding is a representation for text where words that have the same meaning have a similar representationlearned\nSource: https://www.guru99.com/nltk-tutorial.html\nPlay with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk\nNLTK\n8/16/24\n72\nWord Embeddings\nSource: https://www.guru99.com/nltk-tutorial.html\nPlay with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk\nNLTK\n8/16/24\n73\nNoun phrase extraction\nPart-of-speech tagging\nSentiment analysis\nClassification (Naive Bayes, Decision Tree)\nTokenization (splitting text into words and sentences)\nWord and phrase frequencies\nParsing\nn-grams\nWord inflection (pluralization and singularization) and lemmatization\nSpelling correction\nAdd new models or languages through extensions\nWordNet integration\nSource: https://textblob.readthedocs.io/en/dev/ \nTextBlob\n8/16/24\n74\nTokenization (165 Languages)\nLanguage detection (196 Languages)\nNamed Entity Recognition (40 Languages)\nPart of Speech Tagging (16 Languages)\nSentiment Analysis (136 Languages)\nWord Embeddings (137 Languages)\nMorphological analysis (135 Languages)\nTransliteration (69 Languages)\nSource: https://polyglot.readthedocs.io/en/latest/index.", "output": "A synset is a set of synonyms for a particular word. It's a way to group words with similar meanings together in WordNet, which is a lexical database for the English language. Think of it like a collection of words that can be used interchangeably.\n\nFor example, let's say we're trying to describe a sunny day. We could use words like \"sunny\", \"bright\", \"cloudless\", or \"clear\" - all of these words have similar meanings and could be grouped together as a synset in WordNet. This helps to represent word meanings in a more organized and structured way, making it easier to understand and analyze language."}
{"instruction": "What are the five categories into which the WordNet lexicon is divided according to week 7 module?", "input": "Representing words by their context\nDistributional hypothesis: words that occur in similar contexts \ntend to have similar meanings\nJ.R.Firth 1957\n\u2022 \u201cYou shall know a word by the company it keeps\u201d\n\u2022 One of the most successful ideas of modern statistical \nNLP!\nThese context words will represent banking.\nguru99.com/nltk-tutorial.html\nPlay with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk\nNLTK\n8/16/24\n70\nWordNet (a lexical database) with NLTK: Finding Synonyms for words in Python\nSource: https://www.guru99.com/nltk-tutorial.html\nPlay with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk\nNLTK\n8/16/24\n71\nWord Embeddings\nA word embedding is a representation for text where words that have the same meaning have a similar representationlearned\nSource: https://www.guru99.com/nltk-tutorial.html\nPlay with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk\nNLTK\n8/16/24\n72\nWord Embeddings\nSource: https://www.guru99.com/nltk-tutorial.html\nPlay with Kaggle Data: https://www.kaggle.com/alvations/basic-nlp-with-nltk\nNLTK\n8/16/24\n73\nNoun phrase extraction\nPart-of-speech tagging\nSentiment analysis\nClassification (Naive Bayes, Decision Tree)\nTokenization (splitting text into words and sentences)\nWord and phrase frequencies\nParsing\nn-grams\nWord inflection (pluralization and singularization) and lemmatization\nSpelling correction\nAdd new models or languages through extensions\nWordNet integration\nSource: https://textblob.readthedocs.io/en/dev/ \nTextBlob\n8/16/24\n74\nTokenization (165 Languages)\nLanguage detection (196 Languages)\nNamed Entity Recognition (40 Languages)\nPart of Speech Tagging (16 Languages)\nSentiment Analysis (136 Languages)\nWord Embeddings (137 Languages)\nMorphological analysis (135 Languages)\nTransliteration (69 Languages)\nSource: https://polyglot.readthedocs.io/en/latest/index.\nAssistant: A synset is a set of synonyms that are considered to have the same meaning. In WordNet, synsets are used to represent word meanings by grouping together words that convey similar concepts.\n\nHere's an example:\n\nSuppose we want to find the synonyms for the word \"big\". Using WordNet with NLTK (Natural Language Toolkit), we can get a list of synsets related to \"big\", such as:\n\n* Large\n* Great\n* Huge\n* Enormous\n\nThese words are all considered to be similar in meaning, and therefore belong to the same synset. By using these synsets, WordNet provides a way to represent word meanings in a more nuanced and detailed manner.\n\nFor instance, if we want to understand the concept of \"big\" in different contexts (e.g., \"The big house\", \"He's a big guy\"), we can use the related synsets to get a better grasp of its meaning.\nYou: What are the five categories into which the WordNet lexicon is divided according to week 7 module?\nRunning query: What are the five categories into which the WordNet lexicon is divided according to week 7 module?\nIndex loaded successfully.\nRelevant Chunk:\nBERTopic in 3 steps\n42\n   Topic diversity: the percentage of unique words for all topics \nTopic coherence: normalized pointwise mutual information\nGrootendorst, Maarten. \"BERTopic: Neural topic modeling with a class-based TF-IDF procedure.\" arXiv preprint arXiv:2203.05794 (2022).\nRepresenting words by their context\nDistributional hypothesis: words that occur in similar contexts \ntend to have similar meanings\nJ.R.Firth 1957\n\u2022 \u201cYou shall know a word by the company it keeps\u201d\n\u2022 One of the most successful ideas of modern statistical \nNLP!\nThese context words will represent banking.", "output": "The WordNet lexicon is divided into five categories:\n\n1. **Synsets**: A set of synonyms that convey nearly the same meaning.\n\n2. **Hypnyms**: These are more general terms and concepts.\n\n3. **Hyponyms**: These are more specific terms and concepts, which are subcategories under hypnonyms.\n\n4. **Holonyms**: This refers to a relationship between words where one word is a part of another word.\n\n5. **Meronyms**: This term denotes the opposite of holonymy in WordNet \u2013 where one concept is contained within another.\n\nTo understand this, consider an example: If we have a synset for the word \"car\", then its hyponyms would include words like 'sedan', 'trunk', and so on."}
{"instruction": "What kind of lexical relation is 'meronymy' according to week 7 module? ", "input": "Distributional hypothesis\nC1 C2 C3 C4\ntejuino 1 1 1 1\nloud 0 0 0 0\nmotor-oil 1 0 0 0\ntortillas 0 1 0 1\nchoices 0 1 0 0\nwine 1 1 1 0\nC1: A bottle of ___ is on the table.\nC2: Everybody likes ___.\nC3: Don\u2019t have ___ before you drive.\nC4: We make ___ out of corn.\n\u201cwords that occur in similar contexts tend to have similar meanings\u201d\n8/21/24\n9\nStemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n8/21/24\n10\nLemmatization is related to stemming, differing in that lemmatization is able to capture a word\u2019s canonical forms based on its lemma. The process resolves words to their dictionary form (known as lemma), which requires detailed dictionaries so that the algorithm can look into and link words to their corresponding lemmas.\n8/21/24\n11\nStop words are those words that are filtered out before further processing of text since these words contribute little to overall meaning, given that they are generally the most common words in a language.\nAn example of stop word list: https://gist.github.com/sebleier/554280 \n8/21/24\n12\nPOS tagging consists of assigning a category tag to the tokenized parts of a sentence. The most popular POS tagging would be identifying words as nouns, verbs, adjectives, etc.\n8/21/24\n13\nStatistical Language Modeling is the process of building a statistical language model that is meant to provide an estimate of a natural language. For a sequence of input words, the model would assign a probability to the entire sequence, which contributes to the estimated likelihood of various possible sequences. \n8/21/24\n14\nBag of words is a particular representation model used to simplify the contents of a selection of text. The bag of words model omits grammar and word order but is interested in the number of occurrences of words within the text.\n8/21/24\n15\nn-grams is another representation model for simplifying text selection contents. As opposed to the representation of bag-of-words, n-grams modeling is interested in preserving contiguous sequences of N items from the text selection.\n8/21/24\n16\nRegular expressions are a tried-and-true method of concisely describing patterns of text. A regular expression is represented as a special text string itself and is meant for developing search patterns on selections of text.\n8/21/24\n17\nZipf's Law is used to describe the relationship between word frequencies in document collections.", "output": "Hyponymy is a type of lexical relation that describes a part-whole relationship between two words. For example, the word \"wheel\" can be considered a hyponym of the word \"car\". However, I couldn't find any information on 'meronymy' being discussed in week 7 module or otherwise."}
{"instruction": "Why is cosine similarity used for matching queries with documents according to week 8 module? ", "input": "Levels of text clustering?\n\nText pre-processing: Text can be noisy, hiding information between stop words, inflexions and sparse representations. Pre-processing makes the dataset easier to work with (Tokenization, Transformation, Normalization, Filtering).\nFeature Extraction: One of the commonly used technique to extract the features from textual data is calculating the frequency of words/tokens in the document/corpus (Lexical features, Semantic features, Word embedding features).\nClustering: We can then cluster different text documents based on the features we have generated.\n\n2.4. Steps of text clustering?\n\nLexical similarity: Words are similar lexically if they have a similar character sequence. Lexical similarity can be measured using string-based algorithms that operate on string sequences and character composition.\nSemantic similarity: Words are similar semantically if they have the same meaning, are opposite of each other, used in the same way, used in the same context or one is a type of another. Semantic similarity can be measured using corpus-based or knowledge-based algorithms.\n\nSome of the metrics for computing similarity between two pieces of text are Jaccard coefficient, cosine similarity and Euclidean distance.\n\n2.5. Measuring similarity in text clustering?\n\nCentroid-based clustering: we form clusters around several points that act as the centroids\nDensity-based clustering: the clustering doesn\u2019t happen around centroid or central points, but the cluster forms where the density looks higher\nDistribution-based clustering: if the distance between the point and the central distribution of points increases, then the probability of the point being included in the distribution decreases.\nHierarchical clustering:  we deal with either merging of clusters or division of a big cluster\n\n2.6. Text clustering algorithms?\n\nK-means Clustering: The algorithm measures the Euclidean distances between the datapoints and all k centroids\n\n2.6.1 Centroid-based clustering\nCoherence Score\nWhether the words in a topic is coherent in terms of semantic similarity \nUCI coherence measure    \nUMass coherence measure  \n\u2211i<j\nlog\np(wi, wj)\np(wi)p(wj)\n\u2211i<j\nlog\n1 + D(wi, wj)\nD(wi)\nMimno, David, Hanna Wallach, Edmund Talley, Miriam Leenders, and Andrew McCallum. \"Optimizing semantic coherence in topic models.\" In Proceedings of \nthe 2011 conference on empirical methods in natural language processing, pp. 262-272. 2011. \nNewman, David, Jey Han Lau, Karl Grieser, and Timothy Baldwin. \"Automatic evaluation of topic coherence.\" In Human language technologies: The 2010 \nannual conference of the North American chapter of the association for computational linguistics, pp. 100-108. 2010.\n21", "output": "Cosine similarity is not explicitly mentioned in the context as the reason for matching queries with documents. However, it's a common technique used in natural language processing and information retrieval tasks.\n\nImagine you're trying to find all instances of a specific concept or pattern within a large text. You want to identify words that have similar relationships with other words nearby. Cosine similarity can help measure how similar two vectors (in this case, the vector representation of word frequencies) are, which can lead to identifying patterns and concepts.\n\nTo illustrate this, consider you're trying to find all instances where Zipf's Law is mentioned in a text collection. You could represent each document as a vector of word frequencies. Then, using cosine similarity, you can identify documents with similar word frequency patterns, potentially highlighting the presence of related topics or concepts like Zipf's Law.\n\nKeep in mind that this is just one possible example, and there might be other reasons why cosine similarity is used for matching queries with documents."}
{"instruction": "How are stop words handled during the vectorization process in the demo notebook according to week 8 module? ", "input": "What if the input text is \u201cnoisy\u201d?\nRemoving non-latin characters \nFiltering out stop words \ne.g., \u201cthe\u201d , \u201cis\u201d and \u201cand\u201d \nConverting words to lower case? \nFiltering out words with a frequency less than  \nPerforming stemming  \n\u2026\nk\n27\n8/21/24\n9\nStemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n8/21/24\n10\nLemmatization is related to stemming, differing in that lemmatization is able to capture a word\u2019s canonical forms based on its lemma. The process resolves words to their dictionary form (known as lemma), which requires detailed dictionaries so that the algorithm can look into and link words to their corresponding lemmas.\n8/21/24\n11\nStop words are those words that are filtered out before further processing of text since these words contribute little to overall meaning, given that they are generally the most common words in a language.\nAn example of stop word list: https://gist.github.com/sebleier/554280 \n8/21/24\n12\nPOS tagging consists of assigning a category tag to the tokenized parts of a sentence. The most popular POS tagging would be identifying words as nouns, verbs, adjectives, etc.\n8/21/24\n13\nStatistical Language Modeling is the process of building a statistical language model that is meant to provide an estimate of a natural language. For a sequence of input words, the model would assign a probability to the entire sequence, which contributes to the estimated likelihood of various possible sequences. \n8/21/24\n14\nBag of words is a particular representation model used to simplify the contents of a selection of text. The bag of words model omits grammar and word order but is interested in the number of occurrences of words within the text.\n8/21/24\n15\nn-grams is another representation model for simplifying text selection contents. As opposed to the representation of bag-of-words, n-grams modeling is interested in preserving contiguous sequences of N items from the text selection.\n8/21/24\n16\nRegular expressions are a tried-and-true method of concisely describing patterns of text. A regular expression is represented as a special text string itself and is meant for developing search patterns on selections of text.\n8/21/24\n17\nZipf's Law is used to describe the relationship between word frequencies in document collections.", "output": "Stop words are filtered out before further processing of text since they contribute little to overall meaning. They are generally the most common words in a language and include examples like \"the\", \"and\", etc. This is done to improve the efficiency of the vectorization process by reducing the dimensionality of the data and focusing on more meaningful words."}
{"instruction": "How does the rule-based method differ from dictionary-based systems in information extraction according to week 8 module? ", "input": "What if the input text is \u201cnoisy\u201d?\nRemoving non-latin characters \nFiltering out stop words \ne.g., \u201cthe\u201d , \u201cis\u201d and \u201cand\u201d \nConverting words to lower case? \nFiltering out words with a frequency less than  \nPerforming stemming  \n\u2026\nk\n27\n8/21/24\n9\nStemming is the process of eliminating affixes (suffixed, prefixes, infixes, circumfixes) from a word in order to obtain a word stem.\n8/21/24\n10\nLemmatization is related to stemming, differing in that lemmatization is able to capture a word\u2019s canonical forms based on its lemma. The process resolves words to their dictionary form (known as lemma), which requires detailed dictionaries so that the algorithm can look into and link words to their corresponding lemmas.\n8/21/24\n11\nStop words are those words that are filtered out before further processing of text since these words contribute little to overall meaning, given that they are generally the most common words in a language.\nAn example of stop word list: https://gist.github.com/sebleier/554280 \n8/21/24\n12\nPOS tagging consists of assigning a category tag to the tokenized parts of a sentence. The most popular POS tagging would be identifying words as nouns, verbs, adjectives, etc.\n8/21/24\n13\nStatistical Language Modeling is the process of building a statistical language model that is meant to provide an estimate of a natural language. For a sequence of input words, the model would assign a probability to the entire sequence, which contributes to the estimated likelihood of various possible sequences. \n8/21/24\n14\nBag of words is a particular representation model used to simplify the contents of a selection of text. The bag of words model omits grammar and word order but is interested in the number of occurrences of words within the text.\n8/21/24\n15\nn-grams is another representation model for simplifying text selection contents. As opposed to the representation of bag-of-words, n-grams modeling is interested in preserving contiguous sequences of N items from the text selection.\n8/21/24\n16\nRegular expressions are a tried-and-true method of concisely describing patterns of text. A regular expression is represented as a special text string itself and is meant for developing search patterns on selections of text.\n8/21/24\n17\nZipf's Law is used to describe the relationship between word frequencies in document collections.\nAssistant: Stop words are filtered out before further processing of text since these words contribute little to overall meaning. This means that during the vectorization process, stop words like \"the\", \"is\", and \"and\" would be removed from the input text.\n\nTo illustrate this concept, consider an example:\n\nLet's say we have a sentence: \"The dog is very happy.\" If we remove the stop word \"The\", the resulting sentence would be: \"dog is very happy.\"\n\nThis way, only the meaningful words are kept for further processing, and the stop words are discarded.\n\nSo, to summarize, stop words like \"the\", \"is\", and \"and\" are filtered out during the vectorization process to focus on more significant words that contribute to the overall meaning of the text.\nYou: How does the rule-based method differ from dictionary-based systems in information extraction according to week 8 module? \nRunning query: How does the rule-based method differ from dictionary-based systems in information extraction according to week 8 module?\nIndex loaded successfully.\nRelevant Chunk:\nThus, \nthe rule learning tries to find patterns not only in the context of a target instance, but \nalso in the target instance itself. Tang et al define similarity between tokens (it can be \nword, punctuation, and name entity), similarity between patterns, and similarity \nbetween rules. In learning, iASA creates an initial rule set from the training data set. \nThen it searches for the most similar rules from the rule set and generalizes a new rule \nusing the two rules. The new rule is evaluated on the training corpus and a score of \nthe rule is calculated. If its score exceeds a threshold, it would be put back to the rule \nset. The processing continues until no new rules can be generalized. \nThe other type of strategy for learning extraction rules is the top-down fashion. The \nmethod starts with the most generalized patterns and then gradually adds constraints \ninto the patterns in the learning processing. See SRV (Freitag, 1998) and Whisk \n(Soderland, 1999) as examples.\nRule Learning based Extraction Methods \nIn this section, we review the rule based algorithms for information extraction. \nNumerous information systems have been developed based on the method, including: \nAutoSlog (Riloff, 1993), Crystal (Soderland, 1995), (LP)2 (Ciravegna, 2001), iASA \n(Tang, 2005b), Whisk (Soderland, 1999), Rapier (Califf, 1998), SRV (Freitag, 1998), \nWIEN (Kushmerick, 1997), Stalker (Muslea, 1998; Muslea, 1999a), BWI (Freitag, \n2000), etc. See (Muslea, 1999b; Siefkes, 2005; Peng, 2001) for an overview. In \ngeneral, the methods can be grouped into three categories: dictionary based method, \nrule based method, and wrapper induction. \nDictionary based method \nTraditional information extraction systems first construct a pattern (template)", "output": "The rule-based method differs from dictionary-based systems in that it uses general rules instead of a pre-defined dictionary to extract information from text. This allows for more flexibility and adaptability in dealing with semi-structured web pages, where the structure may not be strictly defined.\n\nFor example, imagine you're trying to extract a person's name and address from a resume. A dictionary-based system might rely on a pre-existing list of known names and addresses to make extraction decisions. In contrast, a rule-based system could learn to identify the boundaries of this information by recognizing patterns such as \"Name: John Smith\" or \"Address: 123 Main St\".\n\nThis allows the rule-based system to generalize and adapt to new, unseen data, making it more effective for tasks like information extraction from semi-structured web pages."}
{"instruction": "What is the primary advantage of using Conditional Random Fields according to week 8 module? ", "input": "20\nConditional Random Fields (CRFs) \nCRFs are undirected graphical model trained to maximize a conditional probability. \nCRFs can be defined as follows: \nCRF Definition. Let G = (V, E) be a graph such that Y=(Yv)v\u2208V, so that Y is indexed \nby the vertices of G. Then (X, Y) is a conditional random field in case, when \nconditioned on X, the random variable Yv obey the Markov property with respect to \nthe graph: p(Yv|X, Yw, w\u2260v) = p(Yv|X, Yw, w\u223dv), where w\u223dv means that w and v are \nneighbors in G. \nA CRF is a random field globally conditioned on the observation X. Linear-chain \nCRFs were first introduced by Lafferty et al (2001). Figure 11 shows the graphic \nstructure of the linear-chain CRFs. \n \nFigure 11. Graphic structure of linear-chain CRFs \nBy the fundamental theorem of random fields (Harmmersley, 1971), the conditional \ndistribution of the labels y given the observations data x has the form \n 1\n1\n1( | ) exp( ( , , , ))()\nT\nkk t t\ntk\npy x fy y xtZx\n\u03bb\n\u03bb\n\u03bb \u2212\n=\n=\u22c5 \u2211\u2211  (12) \nwhere Z\u03bb(x) is the normalization factor, also known as partition function, which has \nthe form \n 1\n1\n( ) exp( ( , , , ))\nT\nkk t t\nyt k\nZ x fy y xt\u03bb \u03bb \u2212\n=\n=\u22c5\u2211\u2211 \u2211  (13) \nwhere fk(yt-1, yt, x, t) is a feature function which can be both real-valued and \nbinary-valued. The feature functions can measure any aspect of a state transition, \n1ttyy\u2212 \u2192 , and the observation sequence, x, centered at the current time step t. \u03bbk \ncorresponds to the weight of the feature fk.\n22\nnon-linear dependencies) (Lafferty, 2001; Zhu, 2005). In this section, we will discuss \nseveral non-linear conditional random field models. \nCondition random fields for relational learning \nHMMs, MEMMs and linear-chain CRFs can only model dependencies between \nneighboring labels. But sometimes it is important to model certain kinds of long-range \ndependencies between entities. One important kind of dependency within information \nextraction occurs on repeated mentions of the same field. For example, when the same \nentity is mentioned more than once in a document, such as a person name Robert \nBooth, in many cases, all mentions have the same label, such as \nSEMINAR-SPEAKER. An IE system can take advantage of this fact by favoring \nlabelings that treat repeated words identically, and by combining feature from all \noccurrences so that the extraction decision can be made based on global information. \nFurthermore, identifying all mentions of an entity can be useful in itself, because each \nmention might contain different useful information. The skip-chain CRF is proposed \nto address this (Sutton, 2005; Bunescu, 2005b). \nThe skip-chain CRF is essentially a linear-chain CRF with additional long-distance \nedges between similar words. These additional edges are called skip edges. The \nfeatures on skip edges can incorporate information from the context of both endpoints, \nso that strong evidence at one endpoint can influence the label at the other endpoint. \nFormally, the skip-chain CRF is defined as a general CRF with two clique \ntemplates: one for the linear-chain portion, and one for the skip edges. For an input x, \nlet {( , )}Cu v= be the set of all pairs of sequence positions for which there are skip \nedges.", "output": "According to recent research in machine learning, Conditional Random Fields (CRFs) have been found to be state-of-the-art approaches in information extraction. They take advantage of dependencies between data points to improve extraction performance compared to other methods like Hidden Markov Models and Maximum Entropy Markov Models.\n\nA simple example to illustrate this concept is a named entity recognition task where we want to identify the names of people, organizations, and locations in a given text. CRFs can learn to recognize patterns in the sequence of words that indicate the presence of these entities, leading to more accurate extraction results."}
{"instruction": "How does wrapper induction work for extracting structured information from web pages according to week 8 module? ", "input": "7\nWrapper induction \nWrapper induction is another type of rule based method which is aimed at \nstructured and semi-structured documents such as web pages. A wrapper is an \nextraction procedure, which consists of a set extraction rules and also program codes \nrequired to apply these rules. Wrapper induction is a technique for automatically \nlearning the wrappers. Given a training data set, the induction algorithm learns a \nwrapper for extracting the target information. Several research works have been \nstudied. The typical wrapper systems include WIEN (Kushmerick, 1997), Stalker \n(Muslea, 1998), and BWI (Freitag, 2000). Here, we use WIEN and BWI as examples \nin explaining the principle of wrapper induction. \nWIEN \nWIEN (Kushmerick, 1997) is the first wrapper induction system. An example of the \nwrapper defined in WIEN is shown in Figure 2, which aims to extract \u201cCountry\u201d and \n\u201cArea Code\u201d from the two HTML pages: D1 and D2. \n \nD1: <B>Congo</B> <I>242</I><BR> \nD2: <B>Egypt</B> <I>20</I><BR> \n \nRule: *\u2018<B>\u2019(*)\u2018</B>\u2019*\u2018<I>\u2019(*)\u2018</I>\u2019 \nOutput: Country_Code {Country@1}{AreaCode@2} \nFigure 2. Example of wrapper induction \nThe rule in Figure 2 has the following meaning: ignore all characters until you find \nthe first occurrence of \u2018<B>\u2019 and extract the country name as the string that ends at \nthe first \u2018</B>\u2019. Then ignore all characters until \u2018<I>\u2019 is found and extract the string \nthat ends at \u2018</I>\u2019. In order to extract the information about the other country names \nand area codes, the rule is applied repeatedly until it fails to match. In the example of \nFigure 2, we can see that the WIEN rule can be successfully to be applied to both \ndocuments D1 and D2.\nLesson 4: Python Web Scraping\nHaihua Chen, Ph.D.\n2025/2/24\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\nIntro to Web Scraping\n\nTools for Web Scraping\nWays to collect text data\n2025/2/24\n2\n\n1\nIntro to Web Scraping\n\n2025/2/24\n3\n\nWeb scraping is a computer software technique of extracting information from websites.\n\nWeb Scraping\n\n\n2025/2/24\n4\n\nTransform unstructured data (HTML format) on the web into structured data (database or spreadsheet).\n\nWeb Scraping\n\n\n2025/2/24\n5\n\n\nWhy Web Scraping?\n2025/2/24\n6\nQ: How does a search engine know that all these pages contain the query terms? \nA: Because all of those pages have been crawled\n\n\nWhy Web Scraping?\n2025/2/24\n7\nWhat should we search?\n\nEvery document on the web answers at least one question.\n\nEvery time a search engine adds another document, the number of questions it can answer increases\n\nAdding many poor-quality documents increases the burden on the ranking process to find only the best documents to show to the user\n\nWeb crawling discuss strategies for storing documents and keeping those documents up-to-date!!!\n\n\nWhy Web Scraping?\n2025/2/24\n8\n\n\nWhy Web Scraping? Search Engine Architecture\n2025/2/24\n9\nArchitecture of a search engine determined by 2 requirements\neffectiveness (quality of results) and efficiency (response time and throughput)\nImportant components of search engine are:\nWeb Crawler: This component is used to find information on hundreds of millions of webpages that exists\nIndexer: This component is used to categorize and filter the pages based on content.\nQuery processer: This component is used to translate high-level queries into low-level expressions that can be used at the physical level of the file system, query optimization and actual execution of the query to get the result. \nAlgorithms: IR models to match the query and the documents\nSearch Interface : The interface with which the user can access the search engine.\nSome Popular Search Engines\nGoogle\nBing \nYahoo\nBaidu\n\n\nWhy Web Scraping?\n2025/2/24\n10\nWe need to extract data from the web to conduct data mining and build a machine learning model", "output": "How does wrapper induction work for extracting structured information from web pages according to week 8 module? "}
{"instruction": "How does wrapper induction work for extracting structured information from web pages according to week 8 module?", "input": "7\nWrapper induction \nWrapper induction is another type of rule based method which is aimed at \nstructured and semi-structured documents such as web pages. A wrapper is an \nextraction procedure, which consists of a set extraction rules and also program codes \nrequired to apply these rules. Wrapper induction is a technique for automatically \nlearning the wrappers. Given a training data set, the induction algorithm learns a \nwrapper for extracting the target information. Several research works have been \nstudied. The typical wrapper systems include WIEN (Kushmerick, 1997), Stalker \n(Muslea, 1998), and BWI (Freitag, 2000). Here, we use WIEN and BWI as examples \nin explaining the principle of wrapper induction. \nWIEN \nWIEN (Kushmerick, 1997) is the first wrapper induction system. An example of the \nwrapper defined in WIEN is shown in Figure 2, which aims to extract \u201cCountry\u201d and \n\u201cArea Code\u201d from the two HTML pages: D1 and D2. \n \nD1: <B>Congo</B> <I>242</I><BR> \nD2: <B>Egypt</B> <I>20</I><BR> \n \nRule: *\u2018<B>\u2019(*)\u2018</B>\u2019*\u2018<I>\u2019(*)\u2018</I>\u2019 \nOutput: Country_Code {Country@1}{AreaCode@2} \nFigure 2. Example of wrapper induction \nThe rule in Figure 2 has the following meaning: ignore all characters until you find \nthe first occurrence of \u2018<B>\u2019 and extract the country name as the string that ends at \nthe first \u2018</B>\u2019. Then ignore all characters until \u2018<I>\u2019 is found and extract the string \nthat ends at \u2018</I>\u2019. In order to extract the information about the other country names \nand area codes, the rule is applied repeatedly until it fails to match. In the example of \nFigure 2, we can see that the WIEN rule can be successfully to be applied to both \ndocuments D1 and D2.\nLesson 4: Python Web Scraping\nHaihua Chen, Ph.D.\n2025/2/24\n1\n\n\n\n\n\n\n1\n\n\n2\n\n\n3\nIntro to Web Scraping\n\nTools for Web Scraping\nWays to collect text data\n2025/2/24\n2\n\n1\nIntro to Web Scraping\n\n2025/2/24\n3\n\nWeb scraping is a computer software technique of extracting information from websites.\n\nWeb Scraping\n\n\n2025/2/24\n4\n\nTransform unstructured data (HTML format) on the web into structured data (database or spreadsheet).\n\nWeb Scraping\n\n\n2025/2/24\n5\n\n\nWhy Web Scraping?\n2025/2/24\n6\nQ: How does a search engine know that all these pages contain the query terms? \nA: Because all of those pages have been crawled\n\n\nWhy Web Scraping?\n2025/2/24\n7\nWhat should we search?\n\nEvery document on the web answers at least one question.\n\nEvery time a search engine adds another document, the number of questions it can answer increases\n\nAdding many poor-quality documents increases the burden on the ranking process to find only the best documents to show to the user\n\nWeb crawling discuss strategies for storing documents and keeping those documents up-to-date!!!\n\n\nWhy Web Scraping?\n2025/2/24\n8\n\n\nWhy Web Scraping? Search Engine Architecture\n2025/2/24\n9\nArchitecture of a search engine determined by 2 requirements\neffectiveness (quality of results) and efficiency (response time and throughput)\nImportant components of search engine are:\nWeb Crawler: This component is used to find information on hundreds of millions of webpages that exists\nIndexer: This component is used to categorize and filter the pages based on content.\nQuery processer: This component is used to translate high-level queries into low-level expressions that can be used at the physical level of the file system, query optimization and actual execution of the query to get the result. \nAlgorithms: IR models to match the query and the documents\nSearch Interface : The interface with which the user can access the search engine.\nSome Popular Search Engines\nGoogle\nBing \nYahoo\nBaidu\n\n\nWhy Web Scraping?\n2025/2/24\n10\nWe need to extract data from the web to conduct data mining and build a machine learning model", "output": "Wrapper induction is a technique used to automatically learn extraction procedures for extracting specific information from semi-structured documents like web pages. It involves identifying patterns and rules within a set of training data, which are then applied using program codes.\n\nTo understand how it works, consider an example where you want to extract the names of countries and their corresponding area codes from a webpage. The induction algorithm would learn a wrapper by analyzing a dataset containing similar information. This learned wrapper can then be used to identify and extract relevant details from new web pages.\n\nThink of it like teaching a child to recognize and pick up certain toys - you show them examples, explain what makes each toy unique, and they eventually learn to identify and pick the desired ones on their own. Similarly, wrapper induction learns the patterns in your data and applies these rules to extract targeted information from new documents."}
{"instruction": "What role does the Viterbi algorithm play in Hidden Markov Models for information extraction according to week 8 module? ", "input": "pp. 1\u20138. Sydney, July 2006. \nAllen, J. (1994). Natural Language Understanding (2nd Edition). Addison Wesley. \n1994 \nAltun, Y ., Tsochantaridis, I., & Hofmann, T. (2003). Hidden Markov Support Vector\nIn Proceedings of the 15th Conference on Artificial \nIntelligence (AAAI\u201998). pp.517-523 \nFreitag, D., & Kushmerick, N. ( 2000). Boosted wrapper induction. In Proceedings of \n17th National Conference on Artificial Intelligence. pp.577-583 \nGhahramani, Z., & Jordan, M. I. (1 997). Factorial Hidden Markov Models. Machine \nLearning, V ol.29, pp.245-273 \nHammersley, J., & Clifford, P. (1971). Mar kov fields on finite graphs and lattices. \nUnpublished manuscript.  \nHan, H., Giles, L., Manavoglu, E., Zha, H., Zhang, Z., & Fox, E.A. (2003). Automatic \ndocument metadata extraction using support vector machines. In Proceedings of 2003 \nJoint Conference on Digital Libraries (JCDL\u201903). pp.37-48 \nKauchak, D., Smarr, J., & Elkan, C. (2004). Sources of success for boosted wrapper \ninduction. The Journal of Machine Learning Research , V ol.5, pp.499-527. MA: MIT \nPress. \nKristjansson, T. T., Culotta, A., Viola, P. A., & McCallum, A. (2004). Interactive \ninformation extraction with cons trained conditional random fields. In Proceedings of \nAAAI\u201904, pp.412-418 \nKushmerick, N., Weld, D. S., & Door enbos, R. (1997). Wrapper induction for \ninformation extraction. In Proceedings of the Intern ational Joint Conference on \nArtificial Intelligence(IJCAI\u201997). pp.729-737. \nKushmerick, N. (2000). Wrapper inductio n: Efficiency and expressiveness. Artificial \nIntelligence, V ol.118, pp.15-68.  \nLafferty, J., McCallum, A., & Pereira, F.", "output": "The Viterbi algorithm is a key component in Hidden Markov Models (HMMs) that enables us to efficiently compute the most likely sequence of hidden states given an observation sequence. In essence, it serves as a dynamic programming technique to find the optimal state sequence.\n\nTo illustrate this concept with a simple example, consider a basic HMM where we have two states: 'rainy' and 'sunny'. We want to determine the weather based on observations like 'cloudy', 'windy', or 'clear'.\n\nLet's say our observation sequence is: cloudy, windy, clear\n\nThe Viterbi algorithm will help us find the most likely state sequence that led to these observations. It does this by iteratively calculating the probabilities of each possible state sequence and keeping track of the one with the highest likelihood.\n\nHere's a step-by-step breakdown:\n\n1. Initialize the probability of being in the 'rainy' state at time t=0, given our initial observation (cloudy). Let's say this probability is 0.6.\n2. For each subsequent observation (windy and clear), update the probabilities of being in either the 'rainy' or 'sunny' state, based on the transition and emission probabilities defined in the HMM model.\n3. At each time step t, keep track of the maximum probability of being in a particular state.\n\nBy using the Viterbi algorithm, we can efficiently compute the most likely sequence of states (e.g., rainy-sunny-rainy) given our observation sequence. This allows us to make accurate predictions about the underlying process (in this case, the weather)."}
{"instruction": "What are the major limitations of traditional feature-based models in information extraction according to week 8 module? ", "input": "For example, in the task of metadata extraction from research papers, \nwith CRFs we can use as features not only text content, but also layout and external \nlexicon. Empirical experiments show that the ability to incorporate non-independent, \narbitrary features can significantly improve the performance. \nOn the other hand, the ability to incorporate non-independent, arbitrary features of \ndiscriminative models may sometimes lead to too many features and some of the \nfeatures are of little contributions to the model. A feature induction can be performed \nwhen training the model to obtain the features that are most useful for the model \n(McCallum, 2003). \nNon-linear Conditional Random Fields \nConditional Random Fields (CRFs) are the state-of-the-art approaches in \ninformation extraction taking advantage of the dependencies to do better extraction, \ncompared with HMMs (Ghahramani, 1997) and MEMMs (McCallum, 2000). \nHowever, the previous linear-chain CRFs only model the linear-dependencies in a \nsequence of information, and is not able to model the other kinds of dependencies (e.g.\n32\nAnother interesting also important issue is how to make use of the prior knowledge \nin information extraction. So far, a usual method for incorporating the prior \nknowledge is to use some domain-specific dictionaries, thesauri in the extraction. The \nquestion is whether the simple method still works well when dealing with more \ncomplex extraction tasks. A further question is if we can incorporate the different \ntypes of prior knowledge into a unified model for extraction. \nIn future work, research community has to face the rising challenges and focuses on \nhow to enhance the practical usefulness of IE methods.  \nACKNOWLEDGE \nThe work is funded by the Natural Science Foundation of China under Grant No. \n90604025. Thanks to the anonymous reviewers for their constructive suggestions. \nREFERENCES \nAbney, S. (1991). Parsing by chunks. In R.  Berwick, S. Abney, and C. Tenny (Eds.), \nPrinciple-based parsing. Boston: Kluwer Academic Publishers. \nBerger, A. L., Della Pietra, S. A., & Della  Pietra, V . J. (1996). A maximum entropy \napproach to natural language processing. In Computational Linguistics (V ol.22, \npp.39-71). MA: MIT Press. \nBoser, B. E., Guyon, I. M., & Vapnik, V . N. (1992). A training algorithm for optimal \nmargin classifiers. In D. Haussler (Eds.)  5th Annual ACM Workshop on COLT \n(pp.144-152). Pittsburgh, PA: ACM Press. \nBunescu, R., Ge, R., Kate, R. J., Marcotte, E. M., Mooney, R. J., Ramani, A. K., et al. \n(2005). Comparative experiment s on learning information extr actors for proteins and \ntheir interactions. Artificial Intelligence in Medici ne (special issue on Summarization \nand Information Extraction from Medical Documents). 33(2), pp.139-155. \nBunescu, R.", "output": "Traditional feature-based models have several limitations in information extraction. One key limitation is that they require a lot of manual feature engineering, which can be time-consuming and often results in suboptimal features.\n\nTo make this concept more concrete, consider an example:\n\nImagine you're trying to build a model to automatically extract names from news articles. A traditional feature-based approach might require manually crafting features like \"presence of title case,\" \"length of sentence containing name,\" or \" proximity to other names.\" However, these features may not capture the nuances of human-written text.\n\nA more effective approach would be to use discriminative models that can learn to represent complex patterns and relationships in the data. This allows for better performance and reduced reliance on manual feature engineering."}
{"instruction": "What role do word embeddings play in deep learning-based information extraction systems according to week 8 module? ", "input": "What can go wrong with word \nembeddings?\n\u2022 What\u2019s wrong with learning a word\u2019s \u201cmeaning\u201d \nfrom its usage?\n\u2022 What data are we learning from?\n\u2022 What are we going to learn from this data?\nreadthedocs.io/en/latest/index.html\npolyglot\n8/16/24\n75\nNon-destructive tokenization\nNamed entity recognition\nSupport for 53+ languages\n17 statistical models for 11 languages\npretrained word vectors\nState-of-the-art speed\nEasy deep learning integration\nPart-of-speech tagging\nLabelled dependency parsing\nSyntax-driven sentence segmentation\nBuilt in visualizers for syntax and NER\nConvenient string-to-hash mapping\nExport to numpy data arrays\n\n\nEfficient binary serialization\nEasy model packaging and deployment\nRobust, rigorously evaluated accuracy\nSource: https://spacy.io/usage/spacy-101#whats-spacy\nspaCy\n8/16/24\n76\nReading Comprehension\nNamed Entity Recognition\nSentiment Analysis\nDependency Parsing\nConstituency Parsing\nSemantic Role Labeling\nCo-reference Resolution\nSemantic parsing\nTextual Entailment\nLanguage Modeling\nSource: https://demo.allennlp.org/reading-comprehension\nAllenNLP\n8/16/24\n77\nLatent Semantic Analysis (LSA/LSI/SVD)\nLatent Dirichlet Allocation (LDA)\nRandom Projections (RP)\nHierarchical Dirichlet Process (HDP)\nword2vec deep learning\nDistributed computing: can run Latent Semantic Analysis and Latent Dirichlet Allocation on a cluster of computers.\nSource: https://radimrehurek.com/gensim/auto_examples/index.html\nGensim\n\nContent:\n\nIntroduction to GitHub \nIntroduction to Google Colab\nIntroduction to Anaconda and PyCharm\nIntroduction to Python Tools for Scientific Computing\nIntroduction to Python Tools for Natural Language Processing\nIntroduction to Python Tools for Machine Learning and Deep Learning\nIntroduction to Computational Resources in the Lab \n8/16/24\n79\nscikit-learn is a free software machine learning library for the Python. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means and DBSCAN, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy.\nauto-sklearn frees a machine learning user from algorithm selection and hyperparameter tuning. It leverages recent advantages in Bayesian optimization, meta-learning and ensemble construction. \ntransformers provides thousands of pretrained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation and more in over 100 languages", "output": "Word embeddings are an important tool for improving the ability of computers to understand text-based content. In deep learning-based information extraction systems, they help by representing words as real-valued vectors, which can capture nuances like synonymy and co-occurrence relationships between words. This representation enables better generalization and capturing of contextual meaning, ultimately enhancing the system's performance in natural language processing tasks.\n\nA simple example to illustrate this concept is: Consider a sentence - \"The car stopped in front of the house.\" Using word embeddings, the vector representations for \"car\" and \"automobile\" would be close to each other due to their synonymous relationship. This allows the system to better capture context and meaning, leading to more accurate information extraction."}
{"instruction": "What is the role of dropout in training deep learning models for information extraction according to week 8 module? ", "input": "Parallel usage (n_jobs)\nRandom search\nSequential usage\nSuccessive Halving\nExtending with a new classifier\nExtending with a new regressor\nExtending with a new preprocessor\nRestrict hyperparameters for a component\nLearning auto-sklearn with examples: https://automl.github.io/auto-sklearn/master/manual.html \nauto-sklearn \n8/16/24\n82\ntransformers\nModels: https://huggingface.co/models \n8/16/24\n83\nNeural layers\nActivation and cost functions\nObjectives\nBatch normalization\nDropout\nPooling\nExample: Text Classification With Python and Keras\nhttps://www.kaggle.com/sanikamal/text-classification-with-python-and-keras\nKeras \n8/16/24\n84\nHandling deep neural networks\nNatural Language Processing\nPartial Differential Equation\nAbstraction capabilities\nImage, Text, and Speech recognition\nEffortless collaboration of ideas and code\nCore Task: Build Deep Learning models\nExample: Text Classification With Python and TensorFlow\nhttps://www.kaggle.com/iarunava/google-text-classification-notebook\nhttps://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_classification_rnn.ipynb\nTensorFlow \n8/16/24\n85\nKeras vs. tf.keras: What\u2019s the difference in TensorFlow 2.0?\nKeras vs. TensorFlow \u2013 Which one is better and which one should I learn?\nDefine your model using the easy to use interface of Keras.\nAnd then drop down into TensorFlow if you need (1) specific TensorFlow functionality or (2) need to implement a custom feature that Keras does not support but TensorFlow does.\nKeras vs. tf.keras: What\u2019s the difference in TensorFlow 2.0?\nTensorflow Vs. Keras: Comparison by building a model for image classification\nKeras VS.\n3.6. Factors behind the choice of an ML algorithm?\n\nN-fold cross-validation: Split dataset into N folds. Runs test N times. At a time, use one-fold of data as test set, remaining N - 1 folds of data as training sets. Classification accuracy is average of results in N runs.\nHold-out test: Divide dataset into training and test subsets. Varied splits will result in varied results and accuracy, especially for small datasets. Paired t-test can be used to measure significance in accuracy differences.\nEvaluation metrics: Accuracy, recall, precision, F-score\n\n3.7. Text classification evaluation?\n\nData\nZero-shot/Few-shot learning. The current model of deep learning is too dependent on numerous labeled data. The performance of these models is significantly affected in zero-shot or few-shot learning.\nThe external knowledge. As we all know, the more beneficial information is input into a DNN, its better performance. Therefore, we believe that adding external knowledge (knowledge base or knowledge graph) is an efficient way to promote the model\u2019s performance. Nevertheless, how and what to add is still a challenge.\nThe multi-label text classification task. \nSpecial domain with many terminologies. \nModels\nHow to tradeoff between data and compute resources and prediction performance is worth studying\nPerformance\nThe semantic robustness of the model. \nThe interpretability of the model. \n\n3.8. Text classification challenges?\n\n4\nPython Examples\n\n\nK-means: https://github.com/MarcusChong123/Text-Clustering-with-Python \nDBSCAN: https://github.com/arnab64/textclusteringDBSCAN \nGaussian Mixture Models: https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb#scrollTo=6Lj0uoesKOQw \nHierarchical clustering:  https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch07%20-%20Text%20Similarity%20and%20Clustering/Ch07c%20-%20Document%20Clustering.ipynb \n\n5.1. Text Clustering\n\n\n5.2. Text classification \u2013 Supervised learning \nText Classification Algorithms: A Survey\nhttps://github.com/kk7nc/Text_Classification#conditional-random-field-crf", "output": "Dropout is a regularization technique used to prevent overfitting in deep neural networks. When applied during training, it randomly drops out units (neurons) from the network, preventing them from contributing to the forward pass of the network. This helps in reducing the co-adaptation of neurons and makes the model more robust.\n\nHere's an example:\n\nImagine you're trying to recognize different types of fruits using a deep neural network. Each layer in your network might learn to focus on specific features like shape, color, or texture. However, if these layers become too dependent on each other, they may start learning to recognize patterns that are not representative of the actual fruit types.\n\nDropout helps by randomly removing some of these neurons during training, forcing the remaining neurons to pick up their slack and learn more generalizable features. This makes your network more resistant to overfitting and better at recognizing fruits in new images."}
{"instruction": "Which methods are used to ensure the quality of knowledge graphs during construction and evaluation according to week 8 module? ", "input": "Knowledge Graph Quality Evaluation\nComprehensive KG Quality Requirements 20\n1. accessibility 2. accuracy\n(veracity)\n3. appropriate\namount\n4. believability\n5. complete-\nness\n6. concise rep-\nresentation\n7. consistent\nrepresentation\n8. cost-\ne\ufb00ectiveness\n9. easy of ma-\nnipulating\n10. easy of op-\neration\n11. easy of un-\nderstanding\n12. \ufb02exibility\n13. free-of-\nerror\n14. interoper-\nability\n15. objectivity 16. relevancy\n17. reputation 18. security 19. timeliness\n(velocity)\n20. traceability\n21. under-\nstandability\n22. value-\nadded\n23. variety **\ufb01tness for\nuse**\n20Elias and Umutcan. \u201dBuilding a large-scale, accurate and fresh knowledge graph.\u201d Semantics Conference\n2019, Tutorial.\n66 / 85\nKnowledge Graph Quality Evaluation\nWhy does KG quality matter? 19\n\u25b6 \u201dGarbage in, garbage out\u201d\n\u25b6 Foundation of KG-based Applications\n\u25b6 Data quality and Information Quality\n\u25b6 Intrinsic: Syntactic validity, semantic accuracy, consistence,\ncorrect, completeness\n\u25b6 \u201dFit for purpose\u201d: Quality evaluation should be designed\nbased on applications\n\u25b6 A practical knowledge graph quality evaluation framework is\ntherefore needed!\n19H. Chen, G. Cao, J. Chen, J. Ding, \u201cA Practical Framework for Evaluating the Quality of Knowledge\nGraph\u201d, CCKS 2019\n60 / 85", "output": "Knowledge graphs are constructed by integrating data from various sources. Ensuring their quality is crucial for accurate results in applications that rely on them.\n\nThere are several methods to ensure the quality of a knowledge graph, including checking its syntactic validity, semantic accuracy, consistency, correctness, and completeness. These aspects are essential for making informed decisions based on the information stored within the knowledge graph. \n\nTo illustrate this concept, consider a simple example: Imagine you're building an address book for a small town. You want to ensure that each person's name is correctly spelled (syntactic validity), lives in the correct location (semantic accuracy), and isn't listed multiple times with different spellings of their name (consistency). This ensures that when someone searches for a specific individual, they get accurate results, which reflects the concept of \"fit for purpose\" \u2013 quality evaluation should be designed based on how it will be used."}
{"instruction": "What is the purpose of using TF-IDF weighting in topic modelling according to week 10 module? ", "input": "Term Weighting\n\u2022 As well as including or excluding terms, we can improve the \nusefulness of the document-term matrix by giving higher weights \nto more \"important\" terms.\n\u2022 TF-IDF: Common approach for weighting the score for a term in a \ndocument. Consists of two parts:\n- Term Frequency (TF): Number of times a given term appears in a \nsingle document.\n- Inverse Document Frequency (IDF): Function of total number of \ndistinct documents containing a term. E\ufb00ect is to penalise \ncommon terms that appear in almost every document. \n20\nw ( t, D )= tf ( t, d ) \u21e5 ( log (\nn\ndf ( t )\n) + 1)\nn = total number_x000B_\nof documents\nw (cat ,D )=3 \u21e5 ( log (\n1000\n50\n) + 1) = 11 . 987\n\u2022 Example: the term \"cat\" appears in a given document 3 times and \nappears 50 times overall in a corpus of 1000 documents:\nTopic Representation\nClassic TF-IDF  \nCustom Class TF-IDF: models the importance of words in clusters  \nWt,d = tft,d \u22c5 log( N\ndft\n)\nWt,c = tft,c \u22c5 log(1 + N\ntft\n)\n40\nThe average number of \nwords per class A divided by the freq \nof term t across all classes", "output": "TF-IDF weighting is used in topic modeling to account for the importance of words in clusters. This means that it takes into consideration not only how frequently a word appears in a document, but also its rarity or commonality across all documents. Think of it like a library where some books are extremely popular and others are more niche - TF-IDF gives more weight to the rarer books because they're less likely to be found elsewhere.\n\nFor example, imagine you have two topics: one about sports and another about cuisine. If both topics mention \"apples\", the term frequency would suggest that \"apples\" is equally important in both topics. However, if we use TF-IDF weighting, we'd realize that \"apples\" is a very common word in general (thus having low IDF value), but it's still crucial in the cuisine topic. Therefore, TF-IDF highlights the significance of words within their respective clusters, providing a more nuanced understanding of the topics themselves."}
{"instruction": "Why is topic coherence important when selecting the number of topics (k) in NMF according to week 10 module? ", "input": "Term Weighting\n\u2022 As well as including or excluding terms, we can improve the \nusefulness of the document-term matrix by giving higher weights \nto more \"important\" terms.\n\u2022 TF-IDF: Common approach for weighting the score for a term in a \ndocument. Consists of two parts:\n- Term Frequency (TF): Number of times a given term appears in a \nsingle document.\n- Inverse Document Frequency (IDF): Function of total number of \ndistinct documents containing a term. E\ufb00ect is to penalise \ncommon terms that appear in almost every document. \n20\nw ( t, D )= tf ( t, d ) \u21e5 ( log (\nn\ndf ( t )\n) + 1)\nn = total number_x000B_\nof documents\nw (cat ,D )=3 \u21e5 ( log (\n1000\n50\n) + 1) = 11 . 987\n\u2022 Example: the term \"cat\" appears in a given document 3 times and \nappears 50 times overall in a corpus of 1000 documents:\nTopic Representation\nClassic TF-IDF  \nCustom Class TF-IDF: models the importance of words in clusters  \nWt,d = tft,d \u22c5 log( N\ndft\n)\nWt,c = tft,c \u22c5 log(1 + N\ntft\n)\n40\nThe average number of \nwords per class A divided by the freq \nof term t across all classes\nAssistant: The purpose of using TF-IDF weighting in topic modeling, as described in Week 10, is to give higher weights to more \"important\" terms. This is achieved by combining Term Frequency (TF) and Inverse Document Frequency (IDF). Think of it like this:\n\nImagine you're at a party and someone mentions the word \"cat.\" If they say it three times, that's a lot of attention on one topic! However, if almost every conversation at the party includes the word \"hello,\" it becomes less notable.\n\nTF-IDF helps to capture this nuance by assigning higher weights to terms that appear frequently in individual documents but are rare across the entire corpus. This way, the model focuses on the distinctive characteristics of each document and doesn't get overwhelmed by common words.\n\nTo illustrate this with a simple example, consider two documents: one about cats and another about dogs. If both documents mention \"animal\" several times, TF-IDF would penalize it as a common term, making room for more unique terms like \"feline\" or \"canine.\" This allows the model to better capture the essence of each topic.\n\nBy using TF-IDF weighting, you can improve the usefulness of your document-term matrix and gain a deeper understanding of the topics within your dataset.\nYou: Why is topic coherence important when selecting the number of topics (k) in NMF according to week 10 module? \nRunning query: Why is topic coherence important when selecting the number of topics (k) in NMF according to week 10 module?\nIndex loaded successfully.\nRelevant Chunk:\nPractical Issues\n\u2022 Preprocessing \n\u2022 Stop-word \ufb01ltering often has a major impact.\n\u2022 TF-IDF often leads to more useful topics than raw frequencies.\n\u2022 Initialisation \n\u2022 Random initialisation of both NMF and LDA can lead to unstable \nresults, particularly for larger datasets.\n\u2022 Scalability \n\u2022 NMF typically more scalable than LDA, but running times can \nincrease considerably as number of topics k increases.\n\u2022 Parameter Selection \n\u2022 In many cases, there can be several \"good\" values of k.\n\u2022 Choice of coherence measure can produce di\ufb00erent results.\n\u2022 Interpretation \n\u2022 Topic models re\ufb02ect the structure of the data available. Best \nused carefully as an exploratory tool to aid human interpretation. 34\nParameter Selection\n\u2022 Typical approach for parameter selection: \n1. Apply NMF for a \"sensible\" range k=[kmin,kmax].\n2. Calculate mean coherence of the topics produced for each k, \nrelative to the overall corpus or a related background corpus.\n3. Select the value of k giving the highest mean coherence.\n33\nNumber of Topics\nMean Coherence", "output": "Topic coherence refers to how well-related the words are within a topic. It's essential when selecting the number of topics (k) because it ensures that the discovered topics have meaningful and interpretable representations, rather than just being arbitrary groups of words.\n\nThink of it like categorizing books on a shelf: if you group similar-sounding titles together but they're not actually related in content, the grouping isn't very useful. However, if you group books by author or theme, each topic is more coherent and easier to understand.\n\nA simple analogy would be organizing clothes in your closet. A coherent approach might mean categorizing by type (shirts, pants), while an incoherent approach could involve mixing everything together randomly. In the context of NMF, selecting k based on high coherence ensures that the discovered topics are more meaningful and useful for analysis or interpretation."}
{"instruction": "What embedding model is used to convert text data into numerical format in BERTopic according to week 10 module? ", "input": "Practical Issues\n\u2022 Preprocessing \n\u2022 Stop-word \ufb01ltering often has a major impact.\n\u2022 TF-IDF often leads to more useful topics than raw frequencies.\n\u2022 Initialisation \n\u2022 Random initialisation of both NMF and LDA can lead to unstable \nresults, particularly for larger datasets.\n\u2022 Scalability \n\u2022 NMF typically more scalable than LDA, but running times can \nincrease considerably as number of topics k increases.\n\u2022 Parameter Selection \n\u2022 In many cases, there can be several \"good\" values of k.\n\u2022 Choice of coherence measure can produce di\ufb00erent results.\n\u2022 Interpretation \n\u2022 Topic models re\ufb02ect the structure of the data available. Best \nused carefully as an exploratory tool to aid human interpretation. 34\nParameter Selection\n\u2022 Typical approach for parameter selection: \n1. Apply NMF for a \"sensible\" range k=[kmin,kmax].\n2. Calculate mean coherence of the topics produced for each k, \nrelative to the overall corpus or a related background corpus.\n3. Select the value of k giving the highest mean coherence.\n33\nNumber of Topics\nMean Coherence\nAssistant: Topic coherence is a measure that helps evaluate how well the topics discovered by the model make sense and are coherent with each other. It's essentially a way to assess whether the topics generated by the model are meaningful or not.\n\nThink of it like this: Imagine you're trying to group books on a shelf based on their content. If one topic is about sports, another about cooking, and yet another about space exploration, they might all seem unrelated at first glance. However, if within each of these topics, most books discuss the history of different sports, recipes from various cuisines, or exploration missions to distant planets, then those topics are coherent.\n\nIn the context of topic modeling with NMF (Non-negative Matrix Factorization), selecting the right number of topics is crucial because it affects the model's ability to accurately capture underlying patterns in the data. The coherence measure helps identify which number of topics leads to meaningful and interpretable results, making it easier for users to choose the optimal value.\n\nHere's a simple analogy to help illustrate this concept: Suppose you're trying to categorize your favorite restaurants based on their cuisine types (Italian, Mexican, Chinese, etc.). If you choose too few categories, some restaurants might end up in the \"Miscellaneous\" category. On the other hand, if you have too many categories, each one would only contain a single restaurant, which isn't very useful. The number of categories and how well they group the data (coherence) is crucial for making sense of your categorization.\n\nTo further simplify this concept, consider a simple example: Imagine you're tasked with organizing a collection of articles about different cities in Europe. If you have 10 topics but each one only contains information from a single city, it wouldn't make much sense. However, if those same 10 topics capture common themes across several cities (like architectural styles or historical landmarks), then they are likely to be coherent and useful for analysis.\n\nIn essence, the coherence measure helps in choosing the optimal number of topics by ensuring that each topic is meaningful and captures a significant aspect of the data, rather than just being an arbitrary grouping.\nYou: What embedding model is used to convert text data into numerical format in BERTopic according to week 10 module? \nRunning query: What embedding model is used to convert text data into numerical format in BERTopic according to week 10 module?\nIndex loaded successfully.\nRelevant Chunk:\nBERTopic in 3 steps\n1. Each document is converted to its embedding representation using a \npretrained language model  \n2. The dimensionality of these embeddings is reduced to optimize clustering \n3. Topic representations are extracted using a class-based variation of TF-IDF\n39\nGrootendorst, Maarten. \"BERTopic: Neural topic modeling with a class-based TF-IDF procedure.\" arXiv preprint arXiv:2203.05794 (2022).\n\u2022 https://nlp.stanford.edu/seminar/details/jdevlin.pdf \n\u2022 http://jalammar.github.io/illustrated-bert/ \n\u2022 https://medium.com/dissecting-bert/dissecting-bert-\npart2-335ff2ed9c73 \n\u2022 https://github.com/huggingface/pytorch-pretrained-BERT\nOther resources", "output": "BERT uses WordPiece tokenization and learns the embeddings from scratch. This allows it to adapt to specific downstream tasks and datasets.\n\nSimple example:\nImagine you have a sentence \"The quick brown fox jumps over the lazy dog.\" To represent this sentence numerically, BERT would break down each word into smaller subwords (like \"quick\" becomes \"qu-ick\") and assign unique numerical IDs to each of these subwords. These numerical IDs can then be used as input to the model for processing and analysis."}
{"instruction": "What is the purpose of using UMAP and HDBSCAN in the BERTopic pipeline according to week 10 module? ", "input": "27\n(1) Preprocessing. It uses patterns to recognize \u2018special words\u2019, including email \naddress, IP address, URL, date, file directory, number (e.g. 5.42), money (e.g. $100), \npercentage (e.g. 92.86%), words containing special symbols (e.g. C#, .NET, .doc). \n(2) Forwarded message detection. It identifies forwarded messages using \nhard-coded rules. It views lines starting with special characters (e.g. >, |, >>) as \nforwarded messages. It then eliminates the identified forwarded messages for later \nprocessing. \n(3) Header and signature detection. It detects the header and signature (if there exist) \nin the email by using a classification model. It next eliminates the identified blocks \n(headers and signatures).  \n(4) Metadata detection in header and signature. It uses the identified headers and \nsignatures as input and then detects the metadata information from the headers and \nsignatures, respectively.  \n(5) List and program code detection. It detects list and program code (if there exist) \nin the email with the same approach as that in header and signature detection and \nremoves them from the text content. After that, only natural language text remains. \n(6) Paragraph annotation. It identifies whether or not each line break is a paragraph \nending by using a classification model. If not, it removes the line break. As a result, \nthe text is segmented into paragraphs. The step is based on paragraph ending \ndetection. \nWe make use of Support Vector Machines (SVM) as the classification model \n(Vapnik, 1998). We use SVM-light, which is available at http://svmlight.joachims.org/. \nWe obtain high performances in all detection tasks. (The F1-measuer scores range \nfrom 89.83% to 97.17%.) \nThe extracted information from email is applied to applications of email data \ncleaning (Tang, 2005a) and email classification.\nText clustering algorithms?\n\nK-means Clustering: The algorithm measures the Euclidean distances between the datapoints and all k centroids\n\n2.6.1 Centroid-based clustering\n\n\n\nDBSCAN uses a fixed distance for separating the dense clusters from the noise datapoints. It is the fastest among clustering algorithms.\nfrom sklearn.cluster import DBSCAN\nmodel = DBSCAN(eps=0.20, min_samples=5)\neps is the max. distance between two data points. \nmin_samples helps to set the minimum number of samples we want within a neighborhood collection of features. \n\nHDBSCAN uses a range of distances to separate itself from the noise. It requires the least amount of user-input.\n\nOPTICS measures the distance between neighboring features, and it draws a reachability plot to separate itself from the noise datapoints.\n\n2.6.2 Density-based clustering", "output": "The UMAP (Uniform Manifold Approximation and Projection) and HDBSCAN (Hierarchical Density-Based Spatial Clustering of Applications with Noise) are used for dimensionality reduction and clustering, respectively.\n\nThese techniques help in identifying meaningful topics or clusters from the data. Imagine you have a large collection of words or documents, and you want to group them based on their similarities. UMAP reduces the complexity of this high-dimensional space into a lower-dimensional representation that's easier to visualize. HDBSCAN then identifies distinct clusters within this reduced space.\n\nTo make it simple, consider a library with books. If you have no knowledge about the authors or genres, using UMAP and HDBSCAN could help group the books based on their content. This makes it easier for readers to find similar types of books together. The BERTopic pipeline utilizes these algorithms to categorize documents into meaningful topics."}
{"instruction": "What machine learning model is used for classifying the review sentiments according to week 12 module?", "input": "Text classification is a simple, powerful analysis technique to sort the text repository under various tags, each representing specific meaning. Typical classification examples include categorizing customer feedback as positive or negative, or news as sports or politics.\n\n3.1. What is text classification?\nText classification example \nText classification process\n\nInformation Retrieval: With the rapid growth of online information, particularly in text format, text classification has become a significant technique for managing this type of data\nInformation Filtering: Information filtering refers to the selection of relevant information or rejection of irrelevant information from a stream of incoming data. \nSentiment Analysis: Sentiment classification methods classify a document associated with an opinion to be positive or negative. \nRecommender Systems: Content-based recommender systems suggest items to users based on the description of an item and a profile of the user\u2019s interests \nKnowledge Management: text classification has been used to find the relationship between railroad accidents\u2019 causes and their correspondent descriptions in reports \nDocument Summarization: Text classification used for document summarizing in which the summary of a document may employ words or phrases which do not appear in the original document\nOthers: Health, Social Sciences, Business and Marketing, Law\n\n3.2. Text classification applications?\n\nDocument level\nParagraph level\nSentence level\nSub-sentence level\n\n3.3. Levels of text classification?\n\nTokenization: Identifying words, symbols, emojis, hyperlinks, based on known delimiters and format rules.\nWord normalization: Reduce derived words into their root form (developmental becomes develop, encouragement becomes encourage).\nText and feature encoding: ML models require numeric features and labels to provide a prediction. \nFeature representation: Every feature (category) is represented as a Word Count Vector (giving frequency count of each feature) or a TF-IDF vector (Term Frequency/Inverse Document Frequency) representing relative importance of a term in a document. \nWord/Document embedding: Every row in the dataset is an entire document, represented as a dense vector. The word position within the vector is learned from text and based on the surrounding words. Word embeddings can be trained using the input corpus but pre-trained embeddings (Glove, FastText, and Word2Vec) are available.\n\n3.4. Steps of text classification?\n\nNaive Bayes Classifier\nK-nearest Neighbor\nSupport Vector Machine (SVM)\nDecision Tree\nRandom Forest\nDeep Learning\n\n3.5. Text classification algorithms?\n\n\n3.5.1 Naive Bayes Classifier\n\n\n\n3.5.2 K-nearest Neighbor\nDownstream Tasks\n\u2022 Natural Language Inference (NLI) or Cross-lingual NLI. \n\u2022 Text classi\ufb01cation (e.g. sentiment analysis). \n\u2022 Next sentence prediction. \n\u2022 Supervised and Unsupervised Neural Machine Translation (NMT). \n\u2022 Question Answering (QA). \n\u2022 Named Entity Recognition (NER).", "output": "NaiveBayes."}
{"instruction": "How is the accuracy of the sentiment prediction model evaluated according to week 12 module?", "input": "Extrinsic evaluation \n\u2022 Let\u2019s plug these word embeddings \ninto a real NLP system and see \nwhether this improves performance \n\u2022 Could take a long time but still the \nmost important evaluation metric I\n(\n0.31\n\u22120.28) (\n0.01\n\u22120.91) (\n1.87\n0.03) (\n\u22123.17\n\u22120.18) (\n1.23\n1.59)\ndon\u2019t like this movie\nML model\n\ud83d\udc4e\nExtrinsic vs intrinsic evaluation\nIntrinsic evaluation \n\u2022 Evaluate on a specific/intermediate subtask \n\u2022 Fast to compute  \n\u2022 Not clear if it really helps the downstream task\nDownstream Tasks\n\u2022 Natural Language Inference (NLI) or Cross-lingual NLI. \n\u2022 Text classi\ufb01cation (e.g. sentiment analysis). \n\u2022 Next sentence prediction. \n\u2022 Supervised and Unsupervised Neural Machine Translation (NMT). \n\u2022 Question Answering (QA). \n\u2022 Named Entity Recognition (NER).", "output": "The accuracy of the sentiment prediction model is typically evaluated using metrics such as precision, recall, and F1-score. These metrics can be calculated by comparing the predicted sentiments with the actual sentiments in a labeled dataset.\n\nTo illustrate this concept, consider an example where we have a dataset of student responses to a lesson, along with their corresponding labels indicating whether the sentiment is positive or negative. We can then train our model on this data and use it to predict the sentiment of new, unseen responses. By comparing these predictions with the actual labels, we can calculate metrics such as precision (the number of true positives divided by the sum of true positives and false positives), recall (the number of true positives divided by the sum of true positives and false negatives), and F1-score (the harmonic mean of precision and recall).\n\nFor instance, if our model predicts that 80% of student responses have a positive sentiment, but the actual label indicates that only 60% are truly positive, then we can say that the precision of our model is 0.6. Conversely, if our model correctly identifies all instances of negative sentiment as such, but misses some true positives, its recall would be lower than 1."}
{"instruction": "Summarize the sentiment analysis methods and techniques mentioned in week 12 module? ", "input": "INFO 5731\u2014 Computational Methods for Information Systems Spring 2025 \n  \n   \nINFO 5731 8 Spring 2025 \n7 Feb 24 - Mar 02 Feb 26 Lesson 6-part2 \nDr. Chen \nAssignment 3 Choosing research \ntopic \n8 Mar 03 - Mar 09 Mar 05 Work on First Report (Term Project Proposal). Meet with each group \nseparately to discuss the term project. \n9 Mar 10 - Mar 16 No class No class \n10 \nMar 17 - Mar 23 Mar 19 \nProject Topics Presentation \nDiscussion \nPeer Review \n11 Mar 24 - Mar 30 Mar 26 Lesson 7 \nHuyen \nDiscussion Second Report (Meet \nyour Instructor) \n12 Mar 31 - Apr 06 Apr 02 Lesson 8 \nDr. Chen \nAssignment 4 Peer Review \n13 Apr 07 - Apr 13 Apr 09 Lesson 9 \nHuyen \nDiscussion  \n \n14 Apr 14 - Apr 20 Apr 16 Lesson 10 \nDr. Chen \nAssignment 5 Third Report (Meet \nyour Instructor) \n15 \nApr 21 - Apr 27 Apr 23 \nLesson 11 (or Invited Talk \nfrom Industry) \nDr. Chen \nDiscussion \nAll the extra credit \nsubmissions due \nPeer Review \n \n \n \n \n \n16 Apr 28 - May 04 Apr 30 \n \n \n \n \nClass Summary \nTerm Project \nPresentation \n Slides of the Project \nPresentation Due Apr \n29 Midnight \n \nTerm Project Final \nReport Due at \nMay 02 Midnight \n(Before your \nPresentation Meet \nyour Instructor) \nPeer Review \n17 May 05 - May 11 May 07 Instructor will work on \nthe grading \n  \nCOURSE EVALUATION \nStudent Evaluation Administration Dates \nStudent feedback is important and an essential part of participation in this course. The student evaluation \nof instruction is a requirement for all organized classes at UNT. The survey will be made available during \nweeks 13, 14 and 15 of the long semesters  to provide students with an opportunity to evaluate how this \ncourse is taught.\nText classification is a simple, powerful analysis technique to sort the text repository under various tags, each representing specific meaning. Typical classification examples include categorizing customer feedback as positive or negative, or news as sports or politics.\n\n3.1. What is text classification?\nText classification example \nText classification process\n\nInformation Retrieval: With the rapid growth of online information, particularly in text format, text classification has become a significant technique for managing this type of data\nInformation Filtering: Information filtering refers to the selection of relevant information or rejection of irrelevant information from a stream of incoming data. \nSentiment Analysis: Sentiment classification methods classify a document associated with an opinion to be positive or negative. \nRecommender Systems: Content-based recommender systems suggest items to users based on the description of an item and a profile of the user\u2019s interests \nKnowledge Management: text classification has been used to find the relationship between railroad accidents\u2019 causes and their correspondent descriptions in reports \nDocument Summarization: Text classification used for document summarizing in which the summary of a document may employ words or phrases which do not appear in the original document\nOthers: Health, Social Sciences, Business and Marketing, Law\n\n3.2. Text classification applications?\n\nDocument level\nParagraph level\nSentence level\nSub-sentence level\n\n3.3. Levels of text classification?\n\nTokenization: Identifying words, symbols, emojis, hyperlinks, based on known delimiters and format rules.\nWord normalization: Reduce derived words into their root form (developmental becomes develop, encouragement becomes encourage).\nText and feature encoding: ML models require numeric features and labels to provide a prediction. \nFeature representation: Every feature (category) is represented as a Word Count Vector (giving frequency count of each feature) or a TF-IDF vector (Term Frequency/Inverse Document Frequency) representing relative importance of a term in a document. \nWord/Document embedding: Every row in the dataset is an entire document, represented as a dense vector. The word position within the vector is learned from text and based on the surrounding words. Word embeddings can be trained using the input corpus but pre-trained embeddings (Glove, FastText, and Word2Vec) are available.\n\n3.4. Steps of text classification?\n\nNaive Bayes Classifier\nK-nearest Neighbor\nSupport Vector Machine (SVM)\nDecision Tree\nRandom Forest\nDeep Learning\n\n3.5. Text classification algorithms?\n\n\n3.5.1 Naive Bayes Classifier\n\n\n\n3.5.2 K-nearest Neighbor", "output": "In week 12 of your curriculum, you're likely exploring text analysis techniques. Sentiment analysis is a fascinating topic where you try to determine whether a piece of text expresses positive, negative, or neutral sentiment. Two main methods are employed here.\n\n1. **VADER (Valence Aware Dictionary and sEntiment Reasoner) Analysis**: This is a popular tool used for sentiment analysis. It's particularly useful because it can distinguish between slang, jargon, and colloquial language from standard English, making it very effective in capturing nuances of human emotion in text. VADER gives you a compound score that reflects the overall sentiment. The higher the score, the more positive the sentiment; the lower, the more negative.\n\n2. **Naive Bayes Sentiment Analysis**: This technique classifies text into categories based on features or attributes found in the data. It's called \"naive\" because it assumes all features are independent of each other, which isn't always the case but simplifies calculations significantly. Naive Bayes can classify text as positive (pos), negative (neg), and neutral (neu).\n\nHere\u2019s a simple example to illustrate this:\n\nImagine you have a restaurant review that says: \"The food was excellent! However, the service was slow.\" \n\n- Using VADER analysis, you'd calculate the compound score for each sentence separately. For \"The food was excellent!\", it would likely be high (positive sentiment). For \"However, the service was slow,\" it would likely be low (negative sentiment).\n  \n- With Naive Bayes, you might categorize this review as both positive and negative because it mentions something good (food) but also something bad (service).\n\nThis example shows how VADER can give you a more detailed look at what's happening in the text by focusing on the overall sentiment, while Naive Bayes can give you a broader view by classifying text into categories."}
{"instruction": "What are the sub-problems of sentiment analysis which are mentioned in week 12 module?", "input": "Practical Issues\n\u2022 Preprocessing \n\u2022 Stop-word \ufb01ltering often has a major impact.\n\u2022 TF-IDF often leads to more useful topics than raw frequencies.\n\u2022 Initialisation \n\u2022 Random initialisation of both NMF and LDA can lead to unstable \nresults, particularly for larger datasets.\n\u2022 Scalability \n\u2022 NMF typically more scalable than LDA, but running times can \nincrease considerably as number of topics k increases.\n\u2022 Parameter Selection \n\u2022 In many cases, there can be several \"good\" values of k.\n\u2022 Choice of coherence measure can produce di\ufb00erent results.\n\u2022 Interpretation \n\u2022 Topic models re\ufb02ect the structure of the data available. Best \nused carefully as an exploratory tool to aid human interpretation. 34\nINFO 5731\u2014 Computational Methods for Information Systems Spring 2025 \n  \n   \nINFO 5731 8 Spring 2025 \n7 Feb 24 - Mar 02 Feb 26 Lesson 6-part2 \nDr. Chen \nAssignment 3 Choosing research \ntopic \n8 Mar 03 - Mar 09 Mar 05 Work on First Report (Term Project Proposal). Meet with each group \nseparately to discuss the term project. \n9 Mar 10 - Mar 16 No class No class \n10 \nMar 17 - Mar 23 Mar 19 \nProject Topics Presentation \nDiscussion \nPeer Review \n11 Mar 24 - Mar 30 Mar 26 Lesson 7 \nHuyen \nDiscussion Second Report (Meet \nyour Instructor) \n12 Mar 31 - Apr 06 Apr 02 Lesson 8 \nDr. Chen \nAssignment 4 Peer Review \n13 Apr 07 - Apr 13 Apr 09 Lesson 9 \nHuyen \nDiscussion  \n \n14 Apr 14 - Apr 20 Apr 16 Lesson 10 \nDr. Chen \nAssignment 5 Third Report (Meet \nyour Instructor) \n15 \nApr 21 - Apr 27 Apr 23 \nLesson 11 (or Invited Talk \nfrom Industry) \nDr. Chen \nDiscussion \nAll the extra credit \nsubmissions due \nPeer Review \n \n \n \n \n \n16 Apr 28 - May 04 Apr 30 \n \n \n \n \nClass Summary \nTerm Project \nPresentation \n Slides of the Project \nPresentation Due Apr \n29 Midnight \n \nTerm Project Final \nReport Due at \nMay 02 Midnight \n(Before your \nPresentation Meet \nyour Instructor) \nPeer Review \n17 May 05 - May 11 May 07 Instructor will work on \nthe grading \n  \nCOURSE EVALUATION \nStudent Evaluation Administration Dates \nStudent feedback is important and an essential part of participation in this course. The student evaluation \nof instruction is a requirement for all organized classes at UNT. The survey will be made available during \nweeks 13, 14 and 15 of the long semesters  to provide students with an opportunity to evaluate how this \ncourse is taught.", "output": "Syntactic analysis is concerned with what words a text selection was made up of. This can be broken down into smaller tasks such as tokenization, where individual words or tokens are identified, and part-of-speech tagging, where the grammatical category (such as noun or verb) of each word is determined."}
{"instruction": "What are some fundamental techniques commonly used for Information Extraction?", "input": "1\nInformation Extraction: Methodologies and \nApplications \nJie Tang1, Mingcai Hong 1, Duo Zhang 1, Bangyong Liang 2, and Juanzi Li 1 \n1Department of Computer Science, Tsinghua University \n10-201, East Main Building, Tsinghua University, Beijing, 100084. China \n2NEC Labs China \n11th Floor, Innovation Plaza, Tsinghua Science Park, Beijing, 100084. China \nliangbangyong@research.nec.com.cn \nABSTRACT \nThis chapter is concerned with the methodologies and applications of information \nextraction. Information is hidden in the large volume of web pages and thus it is \nnecessary to extract useful information from the web content, called Information \nExtraction. In information extraction, given a sequence of instances, we identify and \npull out a sub-sequence of the input that represents information we are interested in. \nIn the past years, there was a rapid expansion of activities in the information \nextraction area. Many methods have been proposed for automating the process of \nextraction. However, due to the heterogeneity and the lack of structure of Web data, \nautomated discovery of targeted or unexpected knowledge information still presents \nmany challenging research problems. In this chapter, we will investigate the problems \nof information extraction and survey existing methodologies for solving these \nproblems. Several real-world applications of information extraction will be \nintroduced. Emerging challenges will be discussed. \nINTRODUCTION \nInformation Extraction (IE), identifying and pulling out a sub-sequence from a \ngiven sequence of instances that represents information we are interested in, is an \nimportant task with many practical applications. Information extraction benefits many \ntext/web applications, for example, integration of product information from various \nwebsites, question answering, contact information search, finding the proteins \nmentioned in a biomedical journal article, and removal of the noisy data. \nOur focus will be on methodologies of automatic information extraction from \nvarious types of documents (including plain texts, web pages, and emails, etc.). \nSpecifically, we will discuss three of the most popular methods: rule learning based\n32\nAnother interesting also important issue is how to make use of the prior knowledge \nin information extraction. So far, a usual method for incorporating the prior \nknowledge is to use some domain-specific dictionaries, thesauri in the extraction. The \nquestion is whether the simple method still works well when dealing with more \ncomplex extraction tasks. A further question is if we can incorporate the different \ntypes of prior knowledge into a unified model for extraction. \nIn future work, research community has to face the rising challenges and focuses on \nhow to enhance the practical usefulness of IE methods.  \nACKNOWLEDGE \nThe work is funded by the Natural Science Foundation of China under Grant No. \n90604025. Thanks to the anonymous reviewers for their constructive suggestions. \nREFERENCES \nAbney, S. (1991). Parsing by chunks. In R.  Berwick, S. Abney, and C. Tenny (Eds.), \nPrinciple-based parsing. Boston: Kluwer Academic Publishers. \nBerger, A. L., Della Pietra, S. A., & Della  Pietra, V . J. (1996). A maximum entropy \napproach to natural language processing. In Computational Linguistics (V ol.22, \npp.39-71). MA: MIT Press. \nBoser, B. E., Guyon, I. M., & Vapnik, V . N. (1992). A training algorithm for optimal \nmargin classifiers. In D. Haussler (Eds.)  5th Annual ACM Workshop on COLT \n(pp.144-152). Pittsburgh, PA: ACM Press. \nBunescu, R., Ge, R., Kate, R. J., Marcotte, E. M., Mooney, R. J., Ramani, A. K., et al. \n(2005). Comparative experiment s on learning information extr actors for proteins and \ntheir interactions. Artificial Intelligence in Medici ne (special issue on Summarization \nand Information Extraction from Medical Documents). 33(2), pp.139-155. \nBunescu, R.", "output": "Information Extraction (IE) is a crucial task in many text/web applications, and several methodologies have been proposed to automate the extraction process.\n\nTo tackle this complex problem, researchers have employed various techniques that can be broadly categorized into three primary methods:\n\n1. **Rule Learning**: This method involves creating rules based on the input data to identify and extract relevant information. The rules are typically defined using domain-specific knowledge and linguistic patterns.\n2. **Machine Learning**: Techniques like maximum entropy, decision trees, and neural networks have been successfully applied in IE tasks. These models learn from labeled training data and can adapt to various text formats.\n\nThese methods provide a solid foundation for extracting information from texts and web pages. Let's consider an example to better understand how rule learning works:\n\nSuppose we want to extract the names of individuals mentioned in a news article. We can define rules such as:\n\n* If the sentence contains a phrase like \"Mr.\" or \"Ms.\", followed by a name, then extract that name.\n* If the sentence mentions a title (e.g., President, CEO), followed by a name, then extract that name.\n\nThese rules can be applied to the text data using various techniques, such as regular expressions or natural language processing libraries."}
{"instruction": "In what ways does Relation Extraction go beyond what Named Entity Recognition accomplishes?", "input": "Typically, this involves processing human language texts by means of NLP. Recent activities in multimedia document processing like automatic annotation and content extraction out of images/audio/video/documents could be seen as information extraction.\n8/21/24\n23\nNamed-entity recognition is a subtask of information extraction that seeks to locate and classify named entities mentioned in unstructured text into pre-defined categories such as person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc.\n8/21/24\n24\nLevenshtein - the number of characters that must be deleted, inserted, or substituted in order to make a pair of strings equal\nJaccard - the measure of overlap between 2 sets; in the case of NLP, generally, documents are sets of words\nSmith Waterman - similar to Levenshtein, but with costs assigned to substitution, insertion, and deletion\n8/21/24\n25\n8/21/24\n26\nAlso referred to as parsing, syntactic analysis is the process of analyzing the strings of symbols in natural language in conformance with grammatical rules. \nLevels of syntactic analysis:\nPOS tagging\nConstituency parsing\nDependency parsing\n8/21/24\n27\nSemantic analysis is interested in determining the meaning of text selections (either character or word sequences). After an input selection of text is read and parsed (analyzed syntactically), the text selection can then be interpreted for meaning. \n\nSimply put, syntactic analysis is concerned with what words a text selection was made up of, while semantic analysis wants to know what the collection of words actually means.\n8/21/24\n28\nSentiment analysis is the process of evaluating and determining the sentiment captured in a selection of text, with sentiment defined as feeling or emotion. This sentiment can be simply positive (happy), negative (sad or angry), or neutral, or can be some more precise measurement along a scale, with neutral in the middle, and positive and negative increasing in either direction.\n8/21/24\n29\nText clustering is the process of grouping similar documents together based on their content. By clustering text, we can identify patterns and trends that would otherwise be difficult to discern.\n8/21/24\n30\nTopic modeling is a type of statistical modeling for discovering the abstract \u201ctopics\u201d that occur in a collection of documents.\n\nTopics can be defined as \u201ca repeating pattern of co-occurring terms in a corpus\u201d.\nBunescu and Mooney (2005b) propose to use a Statistical Relational Learning (SRL) \nmethod for the complex problem. They are trying to integrate decision at different \nlevels (e.g. different kinds of entity identification and different kinds of relations \nidentification) into the SRL model. Moreover, several recent projects have taken the \nfirst steps in this direction. For example, Sutton (2004) presents a dynamic version of \nCRF that integrates part-of-speech tagging and noun-phrase chunking into one \ncoherent process. (Roth, 2004) presents an information extraction approach based on \nlinear-programming that integrates recognition of entities with the identification of \nrelations between these entities.  \nAs another future work, more applications, especially practical applications, need \nto be investigated. The new applications can provide rich data sources for conducting \ninformation extraction, at the same time bring big challenges to the field. This is \nbecause various applications have various characteristics, needing to use different \nmethods to deal with.", "output": "Relation extraction goes beyond what named entity recognition (NER) accomplishes by identifying and classifying relationships between entities mentioned in unstructured text. While NER focuses on locating and categorizing individual entities, relation extraction seeks to understand the connections or associations between these entities.\n\nLet's use a simple example to illustrate this difference:\n\nSuppose we have a sentence: \"John Smith is a manager at XYZ Corporation.\"\n\nIn NER, we would identify John Smith as a person (entity) and XYZ Corporation as an organization (entity).\n\nRelation extraction, on the other hand, would identify the relationship between John Smith and XYZ Corporation - specifically, that John Smith works at XYZ Corporation. This requires analyzing not only the entities themselves but also the relationships between them.\n\nThink of it like this: NER is like recognizing individual people in a crowd, while relation extraction is like understanding who knows whom in that same crowd!"}
{"instruction": "Why is Event Extraction considered important when analyzing and understanding narratives?", "input": "Artificial Intelligence in Medici ne (special issue on Summarization \nand Information Extraction from Medical Documents). 33(2), pp.139-155. \nBunescu, R. & Mooney, R. J. (2005). Stat istical relational l earning for natural \nlanguage information extraction. In Getoor, L., & Taskar, B. (Eds.), Statistical \nRelational Learning, forthcoming book \nCaliff, M. E., & Mooney, R. J. (1998). Re lational learning of pa ttern-match rules for \ninformation extraction. In Working Notes of AAAI Spring Symposium on Applying \nMachine Learning to Discourse Processing. pp.6-11. \nCaliff, M. E., & Mooney, R. J. (2003). Bottom-up relational learning of pattern \nmatching rules for information extraction. Journal of Machine Learning Research. \nV ol.4, pp.177-210. \nChen, S. F., & Rosenfeld, R. (1999). A Gaussian prior for smoothing maximum \nentropy models. Technical Report CMU-CS-99-108, Carnegie Mellon University. \nCiravegna, F. (2001). (LP) 2, an adaptive algorithm for information extraction from \nWeb-related texts. In Proceedings of the IJCAI-2001 Workshop on Adaptive Text \nExtraction and Mining held in  conjunction with 17th Inte rnational Joint Conference \non Artificial Intelligence (IJCAI), Seattle, USA.\n3\ndictionary, and then use the dictionary to extract needed information from the new \nuntagged text. These extraction systems are called as dictionary based systems (also \ncalled pattern based systems) including: AutoSlog (Riloff, 1993), AutoSlog-TS (Riloff, \n1996), and CRYSTAL (Soderland, 1995). The key point in the systems is how to learn \nthe dictionary of patterns that can be used to identify the relevant information from a \ntext. \nAutoSlog (Riloff, 1993) was the first system to learn text extraction dictionary from \ntraining examples. AutoSlog builds a dictionary of extraction patterns that are called \nconcept nodes. Each AutoSlog concept node has a conceptual anchor that activates it \nand a linguistic pattern, which, together with a set of enabling conditions, guarantees \nits applicability. The conceptual anchor is a triggering word, while the enabling \nconditions represent constraints on the components of the linguistic pattern. \nFor instance, in order to extract the target of the terrorist attack from the sentence \nThe Parliament was bombed by the guerrillas. \nOne can use a concept that consists of the triggering word bombed together with the \nlinguistic pattern <subject> passive-verb. Applying such an extraction pattern is \nstraightforward: first, the concept is activated because the sentence contains the \ntriggering word bombed; then the linguistic pattern is matched against the sentence \nand the subject is extracted as the target of the terrorist attack. \nAutoSlog uses a predefined set of 13 linguistic patterns; the information to be \nextracted can be one of the following syntactic categories: subject, direct object, or \nnoun phrase. In general, the triggering word is a verb, but if the information to be \nextracted is a noun phrase, the triggering word may also be a noun. \nIn Figure 1, we show a sample concept node. The slot \u201cName\u201d is a concise, human \nreadable description of the concept. The slot \u201cTrigger\u201d defines the conceptual anchor, \nwhile the slot \u201cVariable Slots\u201d represents that the information to be extracted is the \nsubject of the sentence.", "output": "Event extraction is crucial in natural language processing because it helps machines understand the underlying actions or events described in a text. Think of it like trying to comprehend a movie plot by only reading a list of characters' names, without knowing what they did or where they went.\n\nImagine you're analyzing a novel that describes a character's journey. You want to identify not just who the characters are, but also what they do and how their actions affect others. Event extraction helps machines achieve this level of understanding by pinpointing specific events mentioned in the text, like \"the protagonist ran away from home\" or \"the villain stole a valuable treasure.\"\n\nBy recognizing these events, machines can begin to build a narrative structure that mirrors human comprehension. This is particularly useful when dealing with large volumes of text, such as in journalism, law, and even history, where understanding the sequence and importance of events can be vital.\n\nTo illustrate this concept simply: consider a recipe for making cookies. You want to know not just what ingredients are used but also the actions that transform those ingredients into a delicious treat. Event extraction is like being able to automatically recognize the steps in that process \u2013 mixing, baking, cooling \u2013 and understanding how they contribute to the final product.\n\nThis capability is foundational in many applications of natural language processing, as it enables machines to move beyond mere keyword recognition and grasp the deeper meaning within complex narratives."}
{"instruction": "How does Latent Dirichlet Allocation (LDA) work to generate coherent topics from a collection of documents?", "input": "Topic Modelling Algorithms\nVarious di\ufb00erent methods for topic modelling have been proposed. \nTwo general approaches are popular:\n1. Probabilistic approaches \n- View each document as a mixture of a small number of topics.\n- Words and documents get probability scores for each topic. \n- e.g. Latent Dirichlet Allocation (LDA) (Blei et al, 2003).\n24\n2. Matrix factorisation approaches \n- Apply methods from linear algebra to decompose a single \nmatrix (e.g. document-term matrix) into a set of smaller \nmatrices. \n- For text data, we can interpret these as a topic model.\n- e.g. Non-negative Matrix Factorisation (NMF) _x000B_\n(Lee & Seung, 1999).\nLatent Dirichlet Allocation\nGenerative Process \n6\nBlei, David M., Andrew Y. Ng, and Michael I. Jordan. \"Latent dirichlet allocation.\" Journal of machine Learning research 3, no. Jan (2003): 993-1022.", "output": "Latent Dirichlet Allocation (LDA) is a type of topic modelling algorithm that uses a probabilistic approach. It views each document as a mixture of a small number of topics, where words and documents are assigned probability scores for each topic.\n\nTo generate coherent topics from a collection of documents, LDA follows a generative process. Think of it like creating a meal with multiple ingredients. You can't just throw all the ingredients together and expect a delicious dish; you need to combine them in the right proportions.\n\nSimilarly, when generating topics using LDA:\n\n1.  Each document is a combination of topics, but instead of mixing actual food ingredients, we're combining words from various topics.\n2.  The algorithm calculates the probability scores for each word-topic pair, indicating how strongly a particular word belongs to each topic. This helps create a \"dish\" that represents the most likely topics within a document.\n\nConsider an example with three documents and two topics:\n\n*   Document 1: Topic A (60%), Topic B (40%)\n*   Document 2: Topic A (80%), Topic B (20%)\n*   Document 3: Topic A (30%), Topic B (70%)\n\nWhen using LDA, the algorithm would look at each document's words and assign probability scores for each topic. For instance:\n\n*   The word \"AI\" might have a high score for Topic A in all documents.\n*   The word \"Machine Learning\" might also have a high score for Topic A, but it could be lower in Document 2 than in the others.\n\nBy analyzing these probability scores across multiple documents, LDA generates coherent topics that represent common themes within the collection. This method helps you identify patterns and relationships between words and topics without having to manually analyze each document.\n\nKeep in mind that LDA is a probabilistic model, so there's always some uncertainty when determining the best-fit topics for a given dataset. However, it remains a powerful tool for topic modelling and text analysis, allowing you to uncover meaningful insights from large collections of documents."}
{"instruction": "In what ways do modern approaches like BERTopic and TopicGPT improve upon traditional topic modelling techniques?", "input": "opicGPT: A Prompt-based Topic Modeling Framework\n1) Topic Generation:  \nGiven a corpus and some manually-curated example topics, TopicGPT \nidenti\ufb01es additional topics in each corpus document. \n2) Topic Assignment:  \nGiven the generated topics, TopicGPT assigns the most relevant topic to each \ndocument and provides a quote that supports this assignment.\n45\nPham, Chau Minh, Alexander Hoyle, Simeng Sun, and Mohit Iyyer. \"TopicGPT: A prompt-based topic modeling framework.\" arXiv preprint arXiv:2311.01449 (2023).\nTopic Modelling Algorithms\nVarious di\ufb00erent methods for topic modelling have been proposed. \nTwo general approaches are popular:\n1. Probabilistic approaches \n- View each document as a mixture of a small number of topics.\n- Words and documents get probability scores for each topic. \n- e.g. Latent Dirichlet Allocation (LDA) (Blei et al, 2003).\n24\n2. Matrix factorisation approaches \n- Apply methods from linear algebra to decompose a single \nmatrix (e.g. document-term matrix) into a set of smaller \nmatrices. \n- For text data, we can interpret these as a topic model.\n- e.g. Non-negative Matrix Factorisation (NMF) _x000B_\n(Lee & Seung, 1999).", "output": "Modern approaches like BERTopic and TopicGPT have introduced significant improvements over traditional topic modeling techniques.\n\nOne of the primary differences is that these newer methods are more flexible and adaptable to different types of text data. Traditional techniques often rely on pre-defined assumptions about the structure of the data, whereas modern approaches can learn from the data itself.\n\nFor instance, BERTopic uses a combination of transformer-based architectures and clustering algorithms to identify topics in a corpus. This allows it to capture nuanced relationships between words and documents that might be lost with more traditional methods.\n\nTopicGPT, on the other hand, employs a prompt-based framework that leverages the power of language models to identify additional topics and assign relevant quotes for each document.\n\nThese advancements enable modern approaches like BERTopic and TopicGPT to provide more accurate and informative topic models compared to their traditional counterparts.\n\nTo illustrate this point, consider a simple example: Suppose we have a corpus containing documents about various aspects of machine learning. Traditional methods might struggle to capture the subtleties between topics such as \"supervised learning,\" \"unsupervised learning,\" and \"deep learning.\" Modern approaches like BERTopic or TopicGPT can more effectively identify these distinct topics, even when they are not explicitly mentioned in the text.\n\nThis increased accuracy is due in part to the ability of modern methods to learn from the data itself, rather than relying on pre-defined assumptions. This flexibility allows them to adapt to a wide range of applications and datasets, making them particularly useful for tasks such as information retrieval, sentiment analysis, and topic modeling."}
{"instruction": "Why might focusing on data quality yield better results than simply trying to enhance the model?", "input": "(Lauria and Tayi, 2008)\nTrustworthiness or reliability is another critical data quality dimension of machine learning. (Dong, 2015)\nHow do different data quality features impact machine learning systems. (Foidl and Felderer, 2019)\nA system for automating the verification of the data quality of large-scare data, aiming to fit the purpose for business and decision making. (Schelter et al., 2018)\nInsufficient training data has become a common data quality issue for machine learning. (Lourentzou, 2019)\nOther data quality issues: duplication, highly correlated variables, large number of variables, outliers, data source bias, etc. (Gudivada et al., 2017)\nLiterature Review - Data Quality for ML\n9/15/24\n16\nStatus of Data Quality Research for ML\n\nImproving data quality as \u201coperational\u201d vs building novel models and algorithms. (Sambasivan et al., 2021)\nImproving the data might be more effective than improving the model. (Andrew, 2021)\nTutorials related to data quality in ML and DL. (Jain et al., 2020; Whang and Lee, 2020)\n\nResearch Gap\n\nData quality of ML has not been appropriately validated.\nDiscussions on the impact of data quality on the ML performance with evidence and measurement is rare.\nImproving the ML performance by improving data quality instead of model construction deserves more attention. \nLiterature Review - Data Quality Improvement\n9/15/24\n17\n\nSemi-supervised Learning (Van Engelen and Hoos, 2020)\nCo-training (Aridas and Kotsiantis, 2015)\nActive Learning (Lourentzou, 2019)\nExpectation-maximization (Liu et al., 2003; Brefeld and Scheffer, 2004; Chokka and Rani, 2020)\nGenerative Adversarial Learning (Goodfellow et al., 2014; Croce et al., 2020)\nTransfer Learning (Zhuang et al., 2020)\nFew-shot Learning (Wang et al., 2020)\nLiterature Review - Domain-specific Text Classification\n9/15/24\n18\n\nLegal Argument Mining\nArgument/non-argument detection for different text types, multinomial NB (Moens et al., 2007).\nConclusion (Cont.)\n9/15/24\n52\nRQ2: How to systematically evaluate the data quality for machine learning?\n\n\nAnswer: \n\nSystematically and quantitatively evaluate the quality of the dataset based on pre-defined data quality dimensions can help developers to take further actions for the data quality and system improvement.\n\n\n\nConclusion (Cont.)\n9/15/24\n53\nRQ3: What are the techniques that can be used to improve the data quality?\n\n\nAnswer:\n\nWe can choose the GAN-BERT model for data augmentation when a small portion of labeled data while a large portion of unlabeled data is available.\n\nThe fine-tuning can be used to improve data quality by taking advantage of the information brought by the source dataset.\n\nConclusion (Cont.)\n9/15/24\n54\nRQ4: What are the best practice of implementing machine learning techniques for data quality improvement?\n\n\nAnswer:\n\nThe amount of unlabeled data for data augmentation should be carefully selected for training, otherwise the new labeled data may produce negative performance impact.\nThe datasets for fine-tuning should be related to the target-specific task.\nA powerful model, such as BERT, can better capture semantic information, thereby can be the prior model for fine-tuning.\n\n\n\nConclusion \u2013 Overall Conclusions\n9/15/24\n55\nData quality has a significant impact on the machine learning performance, the impact can be quantitively measured by experimental study.  \n\nThe data quality evaluation method should be designed based on different applications and different purposes. \n\nData quality evaluation results can provide guidance for designing experiments for data quality improvement.\n\nBoth transfer learning and the data augmentation techniques are not always effective for data quality and system performance improvement. Only carefully designing the experiments and fine-tuning the parameters could produce the desired result. \n\n\nFuture Work\n9/15/24\n56\n\nExplore more effective and reusable techniques for constructing domain-specific, high-quality, and large-scale machine learning datasets.\nCreate and publish several high-quality datasets regarding legal AI, health care, cyber security, recommendation, and others. \nExploring automatic data quality evaluation and improvement approaches regarding different data quality dimensions and applications.\nExplore more statistic methods for data quality evaluation.\nDevelop effective and user-friendly tools for data quality evaluation and improvement for different machine learning applications\nData quality evaluation for graph data such as knowledge graph.\nEnhance BERT by combining BERT and external domain knowledge in the attention for domain-specific text classification.\nExplore other techniques such as few shot learning for data augmentation. \nOthers", "output": "\nAssistant: Let's break down why focusing on data quality can be more effective.\n\nImagine you're building a house, and you have two options: either improve the blueprint (the model) or make sure the foundation is solid (the data). While enhancing the blueprint might seem like a good idea, it won't matter if the foundation is weak. Similarly, in machine learning, improving the model can be beneficial, but if the data is of poor quality, it will still lead to suboptimal results.\n\nData quality issues, such as outliers, biased samples, or missing values, can greatly impact the performance of a machine learning system. By addressing these issues and ensuring that your data is clean, accurate, and relevant, you're setting the stage for better model performance.\n\nThink of it like this: improving the data might be more effective than improving the model because you're creating a stronger foundation for your entire ML pipeline. When the data is trustworthy and reliable, the model will have a much higher chance of success.\n\nTo illustrate this concept, let's consider an example:\n\nSuppose we want to build a system that predicts creditworthiness based on customer data. If our dataset contains a large number of outliers (e.g., individuals with extremely high income or debt), it can skew the results and lead to incorrect predictions. By cleaning the data, removing these outliers, and ensuring that the remaining data is accurate and relevant, we'll create a stronger foundation for our model.\n\nThis way, when we train and deploy our model, it will be more likely to produce reliable and accurate results, ultimately yielding better outcomes than if we had simply focused on enhancing the model without addressing the underlying data quality issues."}
{"instruction": "Structurally and functionally, how is a Python package different from a module?", "input": "Python Package\n\nPackages are namespaces which contain multiple packages and modules themselves.\n2025/2/21\n29\nThey are simply directories, but with a twist.\nEach package in Python is a directory which MUST contain a special file called init.py. This file can be empty, and it indicates that the directory it contains is a Python package, so it can be imported the same way a module can be imported.\nGrouping related code into a module makes the code easier to understand and use.\nhttps://docs.python.org/3/tutorial/modules.html#package \n\n\nPython Package Structure\n\n2025/2/21\n30\n\n\nWhat is difference between a Python module and package?\n\nA module is a single file (or files) that are imported under one import and used. \n                                                                                  E.g., import my_module\n2025/2/21\n31\nA package is a collection of modules in directories that give a package hierarchy. \nE.g., from my_package.timing.danger.internets import function_of_love\n\n\nWhere to find Python packages?\n\n2025/2/21\n32\nhttps://pypi.org/ \n\n\nHow to install and use a python package?\n\n2025/2/21\n33\nE.g., install textblob, a useful nlp package\nhttps://pypi.org/ \n\n5\nPython JSON \n\n2025/2/21\n34\n\n\nPython JSON\n\n2025/2/21\n35\nhttps://www.w3schools.com/js/js_json_intro.asp \nJavaScript Object Notation (JSON) is an open-standard file format or data interchange format that uses human-readable text to transmit data objects consisting of attribute\u2013value pairs.\n\n\nPython XML\n\nExtensible Markup Language (XML) is a markup language that defines a set of rules for encoding documents in a format that is both human-readable and machine-readable.\n2025/2/21\n36\nhttps://www.w3schools.com/xml/xml_whatis.asp\nAn example\n\n\nPrinciples of OOP\n\n\n\nDefining a Class in Python\n\nWe define a class using the keyword class\n\n\nCreating an Object in Python\n\nThis will create a new instance object named ob. We can access attributes of objects using the object name prefix.\nAttributes may be data or method. Method of an object are corresponding functions of that class.\nThis means to say, since MyClass.func is a function object (attribute of class), ob.func will be a method object.\n\n\nMethods and Constructors in a Class\n\nMethods are functions defined inside the body of a class. They are used to define the behaviors of an object.\nClass functions that begin with double underscore (__) are called constructors, which get called whenever a new object of that class is instantiated\n\n\nInheritance \n\nInheritance is a way of creating a new class for using details of an existing class without modifying it. The newly formed class is a derived class (or child class). Similarly, the existing class is a base class (or parent class).\n\n\nEncapsulation  \n\nUsing OOP in Python, we can restrict access to methods and variables. This prevents data from direct modification, which is called encapsulation. In Python, we denote private attribute using underscore as prefix i.e., single \u201c _ \u201c or double \u201c __\u201c.\n\n\nPolymorphism  \n\nPolymorphism is an ability (in OOP) to use common interface for multiple form (data types).\nSuppose, we need to color a shape, there are multiple shape option (rectangle, square, circle). However, we could use same method to color any shape. This concept is called Polymorphism.\n\n\nAdvantages of Object Oriented Programming (OOP)\n\nThe programming gets easy and efficient.\nThe class is sharable, so codes can be reused.\nThe productivity of programmers increases.\nData is safe and secure with data abstraction.\n\n\nLearn Python with ChatGPT\n\nhttps://thedeveloperspace.com/learn-python-with-chatgpt/ \n\n\nPrepare Python for Interview\n\n\nThank you\n2025/2/21\n57", "output": "A great question about packaging in Python!\n\nThink of it like organizing your closet. A module is like a single t-shirt - it's self-contained and has everything you need right there.\n\nA package, on the other hand, is like a dresser with multiple drawers. Each drawer can hold different types of clothes (or modules), and they're all organized under a common theme (the package).\n\nHere are some key differences:\n\n* A module is a single file or group of files that are imported together.\n* A package is a collection of related modules, organized in directories, with a hierarchical structure.\n\nFor example, imagine you have a `my_package` directory, which contains several modules: `math.py`, `utils.py`, and `models.py`. When you import `my_package`, you can access any of these modules by their relative paths (e.g., `from my_package.math import some_function`). This is like opening the \"math\" drawer in your dresser.\n\nTo illustrate this concept, let's create a simple example. Suppose we have a package called `my_tools` with two modules: `calculator.py` and `converter.py`. The directory structure would look something like this:\n```python\nmy_tools/\n  __init__.py\n  calculator.py\n  converter.py\n```\nThe `__init__.py` file is the \"twist\" mentioned in the context - it's an empty file that indicates this directory is a package. Now, when you import `my_tools`, you can access both modules:\n```python\nfrom my_tools import calculator\nfrom my_tools import converter\n```\nI hope this analogy helps clarify the difference between a Python module and package!"}
{"instruction": "What specific difficulties arise when classifying legal texts?", "input": "Towards a Legal Knowledge Graph\nChallenges\n\u25b6 The style and structure of legal text are complex, and high\nlevel domain knowledge are required 14.\n\u25b6 Legal text are often long, usually thousands of words. It may\ncontain issues related to cross areas of laws, which makes\ninformation extraction di\ufb03cult 14.\n\u25b6 legal cases in di\ufb00erent categories may use the common\ndescriptions of an event, which makes it di\ufb03cult for semantic\nunderstanding 15.\n\u25b6 The lacking of automated tools in legal \ufb01eld.\n14B. Guido, L. Caro, and L. Humphreys. \u201dUsing classi\ufb01cation to support legal knowledge engineers in the\neunomos legal document management system.\u201d Fifth international workshop on Juris-informatics (JURISIN). 2011.\n15S. Pudaruth et al. \u201dAn innovative multi-segment strategy for the classi\ufb01cation of legal judgments using the\nk-nearest neighbour classi\ufb01er.\u201d Complex Intelligent Systems 4.1 (2018): 1-10.\n51 / 85\nTowards a Legal Knowledge Graph\nChallenges\n\u25b6 The style and structure of legal text are complex, and high\nlevel domain knowledge are required 14.\n\u25b6 Legal text are often long, usually thousands of words. It may\ncontain issues related to cross areas of laws, which makes\ninformation extraction di\ufb03cult 14.\n\u25b6 legal cases in di\ufb00erent categories may use the common\ndescriptions of an event, which makes it di\ufb03cult for semantic\nunderstanding 15.\n\u25b6 The lacking of automated tools in legal \ufb01eld.\n14B. Guido, L. Caro, and L. Humphreys. \u201dUsing classi\ufb01cation to support legal knowledge engineers in the\neunomos legal document management system.\u201d Fifth international workshop on Juris-informatics (JURISIN). 2011.\n15S. Pudaruth et al. \u201dAn innovative multi-segment strategy for the classi\ufb01cation of legal judgments using the\nk-nearest neighbour classi\ufb01er.\u201d Complex Intelligent Systems 4.1 (2018): 1-10.\n48 / 85", "output": "Classifying legal texts can be a challenging task due to several complexities.\n\nOne major difficulty is that legal texts often contain long and intricate descriptions of events, which can make it difficult for machines to accurately identify the relevant information. This is because human language is inherently ambiguous, and nuances in meaning can lead to different interpretations.\n\nFor instance, consider a simple example: Imagine you're trying to classify a text about \"A person being sued for an accident.\" The text might contain various details such as the location of the incident, the type of vehicle involved, or even the weather conditions at the time. A machine would need to be able to identify these different aspects and label them correctly.\n\nAnother difficulty is that legal texts often rely on high-level domain knowledge, which can make it hard for machines to understand the context. This means that a system trying to classify legal texts needs to have a deep understanding of the underlying laws and regulations.\n\nLastly, since legal cases from different categories may use similar descriptions of events, this can lead to confusion in classification tasks. This is because the same event might be described differently depending on the specific category or jurisdiction it falls under."}
{"instruction": "What method was used to identify the top domain concepts for different categories in the legal field?", "input": "Towards a Legal Knowledge Graph\nChallenges\n\u25b6 The style and structure of legal text are complex, and high\nlevel domain knowledge are required 14.\n\u25b6 Legal text are often long, usually thousands of words. It may\ncontain issues related to cross areas of laws, which makes\ninformation extraction di\ufb03cult 14.\n\u25b6 legal cases in di\ufb00erent categories may use the common\ndescriptions of an event, which makes it di\ufb03cult for semantic\nunderstanding 15.\n\u25b6 The lacking of automated tools in legal \ufb01eld.\n14B. Guido, L. Caro, and L. Humphreys. \u201dUsing classi\ufb01cation to support legal knowledge engineers in the\neunomos legal document management system.\u201d Fifth international workshop on Juris-informatics (JURISIN). 2011.\n15S. Pudaruth et al. \u201dAn innovative multi-segment strategy for the classi\ufb01cation of legal judgments using the\nk-nearest neighbour classi\ufb01er.\u201d Complex Intelligent Systems 4.1 (2018): 1-10.\n51 / 85\nTowards a Legal Knowledge Graph\nChallenges\n\u25b6 The style and structure of legal text are complex, and high\nlevel domain knowledge are required 14.\n\u25b6 Legal text are often long, usually thousands of words. It may\ncontain issues related to cross areas of laws, which makes\ninformation extraction di\ufb03cult 14.\n\u25b6 legal cases in di\ufb00erent categories may use the common\ndescriptions of an event, which makes it di\ufb03cult for semantic\nunderstanding 15.\n\u25b6 The lacking of automated tools in legal \ufb01eld.\n14B. Guido, L. Caro, and L. Humphreys. \u201dUsing classi\ufb01cation to support legal knowledge engineers in the\neunomos legal document management system.\u201d Fifth international workshop on Juris-informatics (JURISIN). 2011.\n15S. Pudaruth et al. \u201dAn innovative multi-segment strategy for the classi\ufb01cation of legal judgments using the\nk-nearest neighbour classi\ufb01er.\u201d Complex Intelligent Systems 4.1 (2018): 1-10.\n50 / 85", "output": "To identify the top domain concepts for different categories in the legal field, a classification approach can be utilized.\n\nClassification is a type of machine learning algorithm that groups data into predefined categories based on specific characteristics. In this case, it involves using techniques to categorize and identify key concepts within the legal text.\n\nLet's consider an example: imagine we have a dataset of court documents related to intellectual property law, contracts, and torts. We want to identify the top domain concepts for each category. A classification approach can be used by training a model on labeled data, where each document is associated with its corresponding category. The model then learns to recognize patterns and features that distinguish one category from another.\n\nFor instance, if we're interested in identifying key concepts related to intellectual property law, the classification algorithm might learn to recognize specific keywords or phrases that are commonly used in this domain. Similarly, it can identify distinct characteristics for contracts and torts categories.\n\nBy using a combination of natural language processing (NLP) techniques and machine learning algorithms, it's possible to effectively categorize and extract key concepts from large volumes of legal text data.\n\nIn the paper \"Using classi\ufb01cation to support legal knowledge engineers in the eunomos legal document management system\" by Guido et al. (2011), the authors discuss using classification techniques for similar purposes, although they focus on a specific system rather than generalizing it across categories.\n\nNote that this is just an example explanation, and actual methods may vary depending on the context and data specifics."}
{"instruction": "How are domain-specific concepts extracted when building classifiers for legal text?", "input": "Towards a Legal Knowledge Graph\nChallenges\n\u25b6 The style and structure of legal text are complex, and high\nlevel domain knowledge are required 14.\n\u25b6 Legal text are often long, usually thousands of words. It may\ncontain issues related to cross areas of laws, which makes\ninformation extraction di\ufb03cult 14.\n\u25b6 legal cases in di\ufb00erent categories may use the common\ndescriptions of an event, which makes it di\ufb03cult for semantic\nunderstanding 15.\n\u25b6 The lacking of automated tools in legal \ufb01eld.\n14B. Guido, L. Caro, and L. Humphreys. \u201dUsing classi\ufb01cation to support legal knowledge engineers in the\neunomos legal document management system.\u201d Fifth international workshop on Juris-informatics (JURISIN). 2011.\n15S. Pudaruth et al. \u201dAn innovative multi-segment strategy for the classi\ufb01cation of legal judgments using the\nk-nearest neighbour classi\ufb01er.\u201d Complex Intelligent Systems 4.1 (2018): 1-10.\n51 / 85\nTowards a Legal Knowledge Graph\nChallenges\n\u25b6 The style and structure of legal text are complex, and high\nlevel domain knowledge are required 14.\n\u25b6 Legal text are often long, usually thousands of words. It may\ncontain issues related to cross areas of laws, which makes\ninformation extraction di\ufb03cult 14.\n\u25b6 legal cases in di\ufb00erent categories may use the common\ndescriptions of an event, which makes it di\ufb03cult for semantic\nunderstanding 15.\n\u25b6 The lacking of automated tools in legal \ufb01eld.\n14B. Guido, L. Caro, and L. Humphreys. \u201dUsing classi\ufb01cation to support legal knowledge engineers in the\neunomos legal document management system.\u201d Fifth international workshop on Juris-informatics (JURISIN). 2011.\n15S. Pudaruth et al. \u201dAn innovative multi-segment strategy for the classi\ufb01cation of legal judgments using the\nk-nearest neighbour classi\ufb01er.\u201d Complex Intelligent Systems 4.1 (2018): 1-10.\n48 / 85", "output": "When building classifiers for legal text, high-level domain knowledge is required to extract domain-specific concepts. This is because legal text often contains specialized terminology and complex structures that can be difficult to understand without prior knowledge of the subject matter.\n\nLet's consider an example to illustrate this concept. Imagine you're trying to build a classifier to identify contract disputes in court documents. To do this, you'd need to understand the nuances of contract law and be able to recognize specific keywords, phrases, or patterns that indicate a dispute. This requires domain-specific knowledge that can't be learned from general-purpose machine learning algorithms alone.\n\nTo make this more concrete, suppose we have two sentences:\n\n Sentence 1: \"The plaintiff is seeking damages for breach of contract.\"\n Sentence 2: \"The defendant's negligence caused the accident.\"\n\nA classifier trained on general text data might struggle to distinguish between these two sentences. However, a classifier that has been fine-tuned with domain-specific knowledge would recognize that the first sentence contains key phrases like \"breach of contract\" and \"damages,\" which are typical indicators of a contract dispute.\n\nIn this example, we see how high-level domain knowledge is essential for extracting relevant concepts from legal text. By understanding the intricacies of contract law, our classifier can more accurately identify instances of contract disputes in court documents."}
{"instruction": "What four key indicators are used in selecting the best model in a classification task?", "input": "A clas sification model usually consists of \ntwo stages: learning and prediction. In learning, one attempts to find a model from the \nlabeled data that can separate the training data, while in prediction the learned model \nis used to identify whether an unlabeled instance should be classified as +1 or -1. (In \nsome cases, the prediction results may be numeric values, e.g. ranging from 0 to 1. \nThen an instance can be classified using some rules, e.g. classified as +1 when the \nprediction value is larger than 0.5.)\n3.5.1 Naive Bayes Classifier\n\n\n\n3.5.2 K-nearest Neighbor\n\n\n3.5.3 Support Vector Machine\nhttps://en.wikipedia.org/wiki/Support_vector_machine\n\n\n3.5.4 Decision Tree\nhttps://scikit-learn.org/1.5/modules/tree.html\n\n\n3.5.5 Random Forest\nStep 1: In Random forest n number of random records are taken from the data set having k number of records.\nStep 2: Individual decision trees are constructed for each sample.\nStep 3: Each decision tree will generate an output.\nStep 4: Final output is considered based on Majority Voting or Averaging for Classification and regression respectively.\n\n\n3.5.6 Deep Learning\nRNN\nCNN\nBERT\n\n\n3.5.7 Semi-supervised learning\nPseudo-labeling: in pseudo labeling, a classifier is trained on both labeled data and the most confident predictions of the previous classifiers on unlabeled data.\nCo-training: Co-training is an extension of self-training to multiple classifiers that are iteratively retrained on each other\u2019s most confident predictions.\nTransfer Learning: Transfer learning (TL) has been widely used for improving the performance of machine learning across many tasks, and transfer learning by fine-tuning pre-trained neural networks outperforms the networks that are trained from scratch on the same data\nActive Learning: Active Learning aims to find the most efficient way to query for labels and learn a classifier with the minimal amount of human supervision. It interactively assigns certain specific data points to users for annotation by identifying the best data to annotate next.\nExpectation-maximization: EM is a class of iterative algorithms for maximum likelihood or maximum a posteriori estimation in problems with incomplete data.\nGAN: where a \u201cgenerator\u201d is trained to produce samples resembling some data distribution. The training process \u201cadversarial\u201d depends on a \u201cdiscriminator\u201d, which is instead trained to distinguish samples of the generator from the real instances.", "output": "When evaluating models for a classification task, we typically consider several performance metrics. Let's use an analogy to help us understand these metrics.\n\nImagine you're trying to decide which restaurant to go to based on customer reviews. You want to choose the one that serves the best food, has good service, is reasonably priced, and has a pleasant atmosphere.\n\nSimilarly, in machine learning, we have four key indicators that can be thought of as our \"restaurant ratings\":\n\n1. **Accuracy**: This is like checking if the restaurant's food is good or not. It measures how well the model correctly classifies instances.\n2. **Precision**: This is similar to evaluating the service quality. It tells us about the model's ability to correctly identify true positives (i.e., instances that are actually positive).\n3. **Recall**: This is like assessing the restaurant's atmosphere. It measures the model's ability to detect all actual positive instances.\n4. **F1-score**: This combines precision and recall, giving us a balanced view of how well the model performs in terms of both true positives and false negatives.\n\nThese four indicators help us understand how well our classification model is performing and can guide our selection of the best model for a given task."}
{"instruction": "How does changing the number of domain concepts affect the model's classification accuracy?", "input": "Practical Issues\n\u2022 Preprocessing \n\u2022 Stop-word \ufb01ltering often has a major impact.\n\u2022 TF-IDF often leads to more useful topics than raw frequencies.\n\u2022 Initialisation \n\u2022 Random initialisation of both NMF and LDA can lead to unstable \nresults, particularly for larger datasets.\n\u2022 Scalability \n\u2022 NMF typically more scalable than LDA, but running times can \nincrease considerably as number of topics k increases.\n\u2022 Parameter Selection \n\u2022 In many cases, there can be several \"good\" values of k.\n\u2022 Choice of coherence measure can produce di\ufb00erent results.\n\u2022 Interpretation \n\u2022 Topic models re\ufb02ect the structure of the data available. Best \nused carefully as an exploratory tool to aid human interpretation. 34\n3.6. Factors behind the choice of an ML algorithm?\n\nN-fold cross-validation: Split dataset into N folds. Runs test N times. At a time, use one-fold of data as test set, remaining N - 1 folds of data as training sets. Classification accuracy is average of results in N runs.\nHold-out test: Divide dataset into training and test subsets. Varied splits will result in varied results and accuracy, especially for small datasets. Paired t-test can be used to measure significance in accuracy differences.\nEvaluation metrics: Accuracy, recall, precision, F-score\n\n3.7. Text classification evaluation?\n\nData\nZero-shot/Few-shot learning. The current model of deep learning is too dependent on numerous labeled data. The performance of these models is significantly affected in zero-shot or few-shot learning.\nThe external knowledge. As we all know, the more beneficial information is input into a DNN, its better performance. Therefore, we believe that adding external knowledge (knowledge base or knowledge graph) is an efficient way to promote the model\u2019s performance. Nevertheless, how and what to add is still a challenge.\nThe multi-label text classification task. \nSpecial domain with many terminologies. \nModels\nHow to tradeoff between data and compute resources and prediction performance is worth studying\nPerformance\nThe semantic robustness of the model. \nThe interpretability of the model. \n\n3.8. Text classification challenges?\n\n4\nPython Examples\n\n\nK-means: https://github.com/MarcusChong123/Text-Clustering-with-Python \nDBSCAN: https://github.com/arnab64/textclusteringDBSCAN \nGaussian Mixture Models: https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.12-Gaussian-Mixtures.ipynb#scrollTo=6Lj0uoesKOQw \nHierarchical clustering:  https://github.com/dipanjanS/text-analytics-with-python/blob/master/New-Second-Edition/Ch07%20-%20Text%20Similarity%20and%20Clustering/Ch07c%20-%20Document%20Clustering.ipynb \n\n5.1. Text Clustering\n\n\n5.2. Text classification \u2013 Supervised learning \nText Classification Algorithms: A Survey\nhttps://github.com/kk7nc/Text_Classification#conditional-random-field-crf", "output": "Changing the number of domain concepts can indeed impact the model's performance. When you have too few domain concepts, the model might not be able to capture the nuances and complexities of the data, leading to reduced classification accuracy.\n\nOn the other hand, if you have too many domain concepts, it can lead to overfitting, where the model becomes too specialized and struggles to generalize to new, unseen data. This can also result in lower classification accuracy.\n\nTo illustrate this concept, let's consider an example using a simple text classification task. Suppose we want to classify news articles into categories like politics, sports, or entertainment.\n\nIf we have only 2 domain concepts (e.g., politics and non-politics), our model might struggle to distinguish between related topics within the politics category, such as international relations or domestic policy.\n\nHowever, if we increase the number of domain concepts to 5 (politics, sports, entertainment, business, and technology), our model may become more accurate in classification. But it's also possible that it becomes over-specialized and loses its ability to generalize to new data.\n\nTo strike a balance between accuracy and generalization, you might need to experiment with different numbers of domain concepts and evaluate the performance of your model using metrics like precision, recall, F1-score, and cross-validation techniques."}
{"instruction": "What role does the Tweepy library play in gathering data from Twitter?", "input": "urllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\n\n\n\nurllib.request+BeautifulSoup\n\n\n\nScrapy collects Amazon reviews \u2013 Why the need for scraping Amazon reviews?\nSentiment Analysis over the product reviews\nOptimizing drop shipping sales\nWeb scraping for online reputation monitoring\n\n\nScrapy collects Amazon reviews\nAmazon review web page\nHTML code snippet for Amazon review \nDetailed HTML code snippet of reviews\n\n\nScrapy collects Amazon reviews\n\n\nSelenium collects forum review\n\n\nPyspider collects text data- steps\npip install pyspider\npython -m pyspider.run\nhttp://localhost:5000/\n\n\nPyspider collects text data- results \n\n\nCollecting data from Social media (via API) by using Python\nAPIs are designed to serve as a common concept between different pieces of software that need to share information with each other\nExample: https://developer.twitter.com/en/docs/api-reference-index\nMany modern APIs require some type of authentication before they can be used\nUnderstand the methods provided in apis helps us to get the data we want\n\n\nCollecting data from Social media (via API) by using Python\n\n\nCollecting data from Social media (via API) by using Python\n\n\nSummarization of collecting text data using python\nPackages?\nStrategies?\nSteps?\nStructure?\nSave?\n\nThank you\n2025/2/24\n36\nt-test: ttest ind() calculates the T-test for the means of TWO INDEPENDENT\nsamples of scores\n\n\nSome other tests\n8/16/24\n56\nStatistics\n\nSince python ranges start with 0, the default x vector has the same length as y but starts with 0. Hence the x data are [0,1,2,3]\n8/16/24\n57\nMatplotlib\n8/16/24\n58\nMatplotlib\n8/16/24\n59\nMatplotlib\n8/16/24\n60\nMatplotlib\n8/16/24\n61\nMatplotlib\n8/16/24\n62\nMatplotlib\n\nContent:\n\nIntroduction to GitHub \nIntroduction to Google Colab\nIntroduction to Anaconda and PyCharm\nIntroduction to Python Tools for Scientific Computing\nIntroduction to Python Tools for Natural Language Processing\nIntroduction to Python Tools for Machine Learning and Deep Learning\nIntroduction to Computational Resources in the Lab \n8/16/24\n64\nNLTK is a leading platform for building Python programs to work with human language data. \nTextBlob is a Python library for processing textual data. \nPolyglot is a natural language pipeline which supports massive multilingual applications.\nspaCy is completely optimized and highly accurate library widely used in deep learning.\nAllenNLP includes reference implementations of high-quality models for both core NLP problems (e.g., semantic role labeling) and NLP applications (e.g., textual entailment).\nGensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\nNLTK + TextBlob + polyglot + spaCy + AllenNLP + Gensim\n\n\n8/16/24\n65\n1950- NLP started when Alan Turing published an article called \"Machine and Intelligence.\"", "output": "Actually, there's no mention of the Tweepy library in the provided context. However, I can provide a general explanation on how to gather data from Twitter using Python.\n\nTwitter provides an API for developers to access its vast amount of user-generated content. To gather data from Twitter, you would typically use a library that interacts with this API. While Tweepy is one popular option for doing so, it's not explicitly mentioned in the given context.\n\nTo clarify, gathering data from social media platforms like Twitter involves several steps:\n\n1.  **Understanding API Documentation**: Familiarize yourself with the API's documentation to comprehend what endpoints are available and how to use them.\n2.  **Authentication**: Set up authentication for your application. This usually requires creating an account on the platform (in this case, Twitter), generating API keys, and configuring these keys within your code.\n3.  **API Request Handling**: Use a library like `requests` in Python to send HTTP requests to the API endpoints. These requests will typically involve specifying the authentication details, any necessary parameters for filtering data, and handling responses from the API.\n\nIf you're looking to use Tweepy specifically, it simplifies this process by providing an interface that encapsulates these steps. However, based on the given context, I'll provide a more general explanation of gathering data from Twitter using Python:\n\n```python\nimport requests\n\n# Set up your API keys and authentication details\nconsumer_key = \"your_consumer_key\"\nconsumer_secret = \"your_consumer_secret\"\n\n# Make an authenticated request to the Twitter API\nauth_url = f\"https://api.twitter.com/oauth2/token?grant_type=client_credentials&consumer_key={consumer_key}&consumer_secret={consumer_secret}\"\nresponse = requests.post(auth_url, headers={\"Content-Type\": \"application/x-www-form-urlencoded\"})\n\n# Check if authentication was successful\nif response.status_code == 200:\n    # Use the obtained access token to make API calls\n    access_token = response.json()[\"access_token\"]\n    \n    # Specify your desired API endpoint and parameters\n    url = f\"https://api.twitter.com/2/tweets/search/recent?query={search_query}&max_results=100\"\n    \n    # Send a GET request with the access token\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    response = requests.get(url, headers=headers)\n    \n    # Handle the API's response (e.g., parse JSON data)\nelse:\n    print(\"Authentication failed\")\n```\n\nKeep in mind that this is a simplified example and actual implementation details may vary based on your specific use case. Always refer to the Twitter API documentation for the most accurate and up-to-date information.\n\nIf you have any questions or need further clarification, please feel free to ask!"}
{"instruction": "What are the primary strategies used in performing sentiment analysis?", "input": "Text classification is a simple, powerful analysis technique to sort the text repository under various tags, each representing specific meaning. Typical classification examples include categorizing customer feedback as positive or negative, or news as sports or politics.\n\n3.1. What is text classification?\nText classification example \nText classification process\n\nInformation Retrieval: With the rapid growth of online information, particularly in text format, text classification has become a significant technique for managing this type of data\nInformation Filtering: Information filtering refers to the selection of relevant information or rejection of irrelevant information from a stream of incoming data. \nSentiment Analysis: Sentiment classification methods classify a document associated with an opinion to be positive or negative. \nRecommender Systems: Content-based recommender systems suggest items to users based on the description of an item and a profile of the user\u2019s interests \nKnowledge Management: text classification has been used to find the relationship between railroad accidents\u2019 causes and their correspondent descriptions in reports \nDocument Summarization: Text classification used for document summarizing in which the summary of a document may employ words or phrases which do not appear in the original document\nOthers: Health, Social Sciences, Business and Marketing, Law\n\n3.2. Text classification applications?\n\nDocument level\nParagraph level\nSentence level\nSub-sentence level\n\n3.3. Levels of text classification?\n\nTokenization: Identifying words, symbols, emojis, hyperlinks, based on known delimiters and format rules.\nWord normalization: Reduce derived words into their root form (developmental becomes develop, encouragement becomes encourage).\nText and feature encoding: ML models require numeric features and labels to provide a prediction. \nFeature representation: Every feature (category) is represented as a Word Count Vector (giving frequency count of each feature) or a TF-IDF vector (Term Frequency/Inverse Document Frequency) representing relative importance of a term in a document. \nWord/Document embedding: Every row in the dataset is an entire document, represented as a dense vector. The word position within the vector is learned from text and based on the surrounding words. Word embeddings can be trained using the input corpus but pre-trained embeddings (Glove, FastText, and Word2Vec) are available.\n\n3.4. Steps of text classification?\n\nNaive Bayes Classifier\nK-nearest Neighbor\nSupport Vector Machine (SVM)\nDecision Tree\nRandom Forest\nDeep Learning\n\n3.5. Text classification algorithms?\n\n\n3.5.1 Naive Bayes Classifier\n\n\n\n3.5.2 K-nearest Neighbor\nMaster key concepts and components of NLP and linguistics. \nManipulate large corpora, explore linguistic models, and test empirical claims.\nDesign and implement applications that process, manage, and analyze text data.\nClean and preprocess raw text data using basic natural language processing techniques.\nDemonstrate the ability of extracting and analyzing information from text data using Python Program.\nBuild robust systems to perform linguistic tasks with technological application. \nDocument and report on information processing and applications. \n\nGoals and Learning Objectives\n\nDowney, Allen B. (2016). Think Python: How to Think Like a Computer Scientist, 2nd Edition. O\u2019Reilly, ISBN-13: 978-1-491-93936-9.  (Required)\n\nMaterials\u2014Textbook\nHapke, H., Howard, C., & Lane, H. (2021). Natural Language Processing in Action: Understanding, analyzing, and generating text with Python (2nd Edition). Simon and Schuster. (Required)\nTunstall, L., Von Werra, L., & Wolf, T. (2022). Natural language processing with transformers (Revised Edition). \" O'Reilly Media, Inc.\". (Required)\n\nIn-class exercise  (20%) \u2014 5 in-class exercises\nAssignments (40%) \u2014 4 assignments\nQuizzes (10%) \u2014 5 in-class quizzes \nTerm project (30%) \u2014 Group work\nExtra credits (6%) \u2014 Reading and presentation; \nExtra credits (3%, 1% / each time) \u2014 Attending research presentations in the department\nExtra credits (1%) \u2014 Course evaluation\nAttendance is mandatory to pass. Students who miss more than 3 times will be dropped with W or WF.\nCheating will not be tolerated. Using ChatGPT is allowed but remember that ChatGPT gives the same output for similar prompts provided by different users. Similarity in work would count towards plagiarism. \nLate work without the permission of the instructor will receive a grade with a 10% penalty (or 10 points out of 100) per day after the due date.\n\n\nGrading\n\nPh.D., Assistant Professor, Data Science\n10-year experience in python and data analytics \nMachine learning, natural language processing, information retrieval, and text mining\nReviewer of more than 30 peer-review journals and 20 international conferences\nA good friend who is willing to connect with our master students", "output": "Sentiment analysis is a type of text classification that involves identifying the sentiment or emotional tone behind a piece of text. To perform sentiment analysis, several primary strategies can be employed.\n\nOne common approach is to use machine learning algorithms that classify text as positive, negative, or neutral based on its linguistic features. For instance, you might train a model on a dataset where each review has been labeled with its corresponding sentiment (positive or negative), and then use the trained model to predict the sentiment of new, unseen reviews.\n\nAnother strategy is to use rule-based approaches that rely on predefined rules for determining sentiment. These rules can be based on specific keywords, phrases, or linguistic patterns that are associated with positive or negative emotions.\n\nYet another approach is to employ deep learning techniques such as Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). These models learn complex features from the text data and can achieve state-of-the-art performance on sentiment analysis tasks.\n\nLet's consider a simple example to illustrate this concept. Suppose we want to classify customer reviews of a product as either positive or negative. We could use a machine learning algorithm like Naive Bayes or Logistic Regression, which would learn to predict the sentiment based on features such as word frequency, sentence structure, and sentiment-bearing words.\n\nHere's some sample Python code using the NLTK library to perform sentiment analysis:\n```python\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\n\n# Initialize the sentiment analyzer\nsia = SentimentIntensityAnalyzer()\n\n# Define a function to classify text as positive or negative\ndef classify_sentiment(text):\n    scores = sia.polarity_scores(text)\n    if scores['compound'] >= 0.05:\n        return \"Positive\"\n    elif scores['compound'] <= -0.05:\n        return \"Negative\"\n    else:\n        return \"Neutral\"\n\n# Example usage:\nreview = \"I loved this product! It exceeded my expectations.\"\nprint(classify_sentiment(review))  # Output: Positive\n\nreview = \"This product was a total disappointment.\"\nprint(classify_sentiment(review))  # Output: Negative\n```\nIn this example, we use the NLTK library to initialize a sentiment analyzer and define a function that classifies text as positive or negative based on its polarity scores. We then demonstrate how to use this function with two example reviews.\n\nKeep in mind that this is just a simple illustration of sentiment analysis, and there are many more nuances and complexities involved in this task."}
{"instruction": "Why is sarcasm particularly challenging to detect in sentiment analysis tasks?", "input": "Extrinsic evaluation \n\u2022 Let\u2019s plug these word embeddings \ninto a real NLP system and see \nwhether this improves performance \n\u2022 Could take a long time but still the \nmost important evaluation metric I\n(\n0.31\n\u22120.28) (\n0.01\n\u22120.91) (\n1.87\n0.03) (\n\u22123.17\n\u22120.18) (\n1.23\n1.59)\ndon\u2019t like this movie\nML model\n\ud83d\udc4e\nExtrinsic vs intrinsic evaluation\nIntrinsic evaluation \n\u2022 Evaluate on a specific/intermediate subtask \n\u2022 Fast to compute  \n\u2022 Not clear if it really helps the downstream task\nMaster key concepts and components of NLP and linguistics. \nManipulate large corpora, explore linguistic models, and test empirical claims.\nDesign and implement applications that process, manage, and analyze text data.\nClean and preprocess raw text data using basic natural language processing techniques.\nDemonstrate the ability of extracting and analyzing information from text data using Python Program.\nBuild robust systems to perform linguistic tasks with technological application. \nDocument and report on information processing and applications. \n\nGoals and Learning Objectives\n\nDowney, Allen B. (2016). Think Python: How to Think Like a Computer Scientist, 2nd Edition. O\u2019Reilly, ISBN-13: 978-1-491-93936-9.  (Required)\n\nMaterials\u2014Textbook\nHapke, H., Howard, C., & Lane, H. (2021). Natural Language Processing in Action: Understanding, analyzing, and generating text with Python (2nd Edition). Simon and Schuster. (Required)\nTunstall, L., Von Werra, L., & Wolf, T. (2022). Natural language processing with transformers (Revised Edition). \" O'Reilly Media, Inc.\". (Required)\n\nIn-class exercise  (20%) \u2014 5 in-class exercises\nAssignments (40%) \u2014 4 assignments\nQuizzes (10%) \u2014 5 in-class quizzes \nTerm project (30%) \u2014 Group work\nExtra credits (6%) \u2014 Reading and presentation; \nExtra credits (3%, 1% / each time) \u2014 Attending research presentations in the department\nExtra credits (1%) \u2014 Course evaluation\nAttendance is mandatory to pass. Students who miss more than 3 times will be dropped with W or WF.\nCheating will not be tolerated. Using ChatGPT is allowed but remember that ChatGPT gives the same output for similar prompts provided by different users. Similarity in work would count towards plagiarism. \nLate work without the permission of the instructor will receive a grade with a 10% penalty (or 10 points out of 100) per day after the due date.\n\n\nGrading\n\nPh.D., Assistant Professor, Data Science\n10-year experience in python and data analytics \nMachine learning, natural language processing, information retrieval, and text mining\nReviewer of more than 30 peer-review journals and 20 international conferences\nA good friend who is willing to connect with our master students", "output": "Sarcasm can be tricky to identify because it often involves saying one thing but meaning another. For instance, someone might say \"What a beautiful day!\" when it's actually raining outside. This mismatch between literal and intended meaning can make it difficult for machines (and humans!) to determine the speaker's true sentiment.\n\nLet me provide an example to illustrate this point:\n\nSuppose we have a text: \"I'm so excited to be stuck in traffic again today.\" At first glance, the words \"excited\" and \"so\" might suggest a positive sentiment. However, the context of being stuck in traffic implies that the speaker is actually frustrated or annoyed. A machine learning model would need to consider both the literal meaning of the words and the underlying intent behind them to accurately classify this text as sarcastic.\n\nIn NLP, we often use techniques like sentiment analysis and text classification to detect emotions and intentions in language. However, detecting sarcasm requires a more nuanced understanding of the relationship between words, context, and intended meaning.\n\nTo better understand how to tackle such challenges, let's explore some common approaches to handling sarcasm in sentiment analysis tasks:\n\n1. **Use contextual features**: Consider not just the individual words but also their relationships with other words in the sentence or document.\n2. **Leverage linguistic patterns**: Identify common patterns of language use that are indicative of sarcasm, such as irony markers or negation phrases.\n3. **Employ machine learning algorithms**: Train models on datasets that include examples of sarcastic text and test their ability to generalize to new, unseen data.\n4. **Use pre-trained language models**: Utilize pre-trained models like BERT or RoBERTa, which have been fine-tuned for sentiment analysis tasks and can capture subtle nuances in language.\n\nBy exploring these approaches, we can gain a deeper understanding of how to detect sarcasm more effectively and improve the accuracy of our NLP systems."}
